{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 12975714,
          "sourceType": "datasetVersion",
          "datasetId": 8212713
        }
      ],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e837379ed6064caeaa6a9a83a10a878f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_efbfd056152d4673a5176a7361c55db6",
              "IPY_MODEL_38a479512c274d338e0303331c2e962c",
              "IPY_MODEL_01c07454131649798e6f85c25d9df9b7"
            ],
            "layout": "IPY_MODEL_a6f42124686d491aa88a398a49999c7f"
          }
        },
        "efbfd056152d4673a5176a7361c55db6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03c2e9e748a245fd8e499ec3957fa3a8",
            "placeholder": "​",
            "style": "IPY_MODEL_a99a38b21c724315a97a9d9beb8d3b0d",
            "value": "yolox_l0.05.onnx: 100%"
          }
        },
        "38a479512c274d338e0303331c2e962c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a62e69e69214daa99947d9b80667e18",
            "max": 216625723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a45e0d6ab314bfcb95a64b20d594eb5",
            "value": 216625723
          }
        },
        "01c07454131649798e6f85c25d9df9b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52a14e55e65b4b758d1282b07e332736",
            "placeholder": "​",
            "style": "IPY_MODEL_6966012f38cc4fa08bcb1f99d7d0eb53",
            "value": " 217M/217M [00:06&lt;00:00, 33.9MB/s]"
          }
        },
        "a6f42124686d491aa88a398a49999c7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03c2e9e748a245fd8e499ec3957fa3a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a99a38b21c724315a97a9d9beb8d3b0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a62e69e69214daa99947d9b80667e18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a45e0d6ab314bfcb95a64b20d594eb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52a14e55e65b4b758d1282b07e332736": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6966012f38cc4fa08bcb1f99d7d0eb53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mucherlaananthalakshmi-web/Agentic-Project/blob/main/claims_management.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced RAG-Based Expense Claims Processing with Filename IDs and Token Tracking\n",
        "# Complete workflow with detailed progress tracking and usage monitoring\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from dataclasses import dataclass, asdict\n",
        "import warnings\n",
        "import uuid\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"🎯 Enhanced RAG-Based Expense Claims Processing System\")\n",
        "print(\"🔍 Features: Filename-based IDs + Token Usage Tracking\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ================================\n",
        "# STEP 1: SYSTEM SETUP & INSTALLATION\n",
        "# ================================\n",
        "\n",
        "print(\"📦 Installing dependencies...\")\n",
        "\n",
        "# Core RAG dependencies\n",
        "subprocess.run([\n",
        "    \"pip\", \"install\", \"-q\",\n",
        "    \"ollama\",\n",
        "    \"langchain-ollama\",\n",
        "    \"langchain-core\",\n",
        "    \"langchain-community\",\n",
        "    \"chromadb>=0.4.0\",\n",
        "    \"unstructured[pdf]>=0.10.0\",\n",
        "    \"sentence-transformers\",\n",
        "    \"pandas\",\n",
        "    \"pillow\",\n",
        "    \"python-dateutil\",\n",
        "    \"pydantic\",\n",
        "    \"langgraph\"\n",
        "], check=True)\n",
        "\n",
        "print(\"✅ Dependencies installed\")\n",
        "\n",
        "# ================================\n",
        "# STEP 1.5: PRE-DOWNLOAD UNSTRUCTURED MODELS\n",
        "# ================================\n",
        "\n",
        "print(\"\\n📥 Pre-downloading UnstructuredIO models...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "def predownload_unstructured_models():\n",
        "    \"\"\"Pre-download all necessary UnstructuredIO models\"\"\"\n",
        "\n",
        "    try:\n",
        "\n",
        "        # Import the layout model which triggers YOLO download\n",
        "        from unstructured.partition.pdf import partition_pdf\n",
        "        from unstructured.partition.auto import partition\n",
        "\n",
        "        # Try to initialize the model by importing layout detection\n",
        "        try:\n",
        "            from unstructured.models import detectron2_onnx\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            from unstructured.partition.utils.constants import Source\n",
        "            from unstructured.documents.elements import Element\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Force download by attempting to process a dummy image\n",
        "        # This triggers YOLO model download\n",
        "\n",
        "        # Create a minimal dummy image to trigger model download\n",
        "        from PIL import Image\n",
        "        import io\n",
        "\n",
        "        # Create a simple white image\n",
        "        img = Image.new('RGB', (100, 100), color='white')\n",
        "        img_bytes = io.BytesIO()\n",
        "        img.save(img_bytes, format='PNG')\n",
        "        img_bytes.seek(0)\n",
        "\n",
        "        # Save temporarily\n",
        "        temp_path = \"/tmp/dummy_image.png\"\n",
        "        with open(temp_path, \"wb\") as f:\n",
        "            f.write(img_bytes.getvalue())\n",
        "\n",
        "        # Try to partition it to trigger model downloads\n",
        "        try:\n",
        "            from unstructured.partition.auto import partition\n",
        "            elements = partition(filename=temp_path)\n",
        "            print(\"✅ YOLO model (yolox_l0.05.onnx) downloaded successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Model download triggered, may complete in background: {e}\")\n",
        "\n",
        "        # Clean up\n",
        "        if os.path.exists(temp_path):\n",
        "            os.remove(temp_path)\n",
        "\n",
        "        # Download models for Excel/CSV if available\n",
        "        try:\n",
        "            from unstructured.partition.xlsx import partition_xlsx\n",
        "            from unstructured.partition.csv import partition_csv\n",
        "        except ImportError:\n",
        "            print(\"ℹ️ Excel/CSV models not required or already available\")\n",
        "\n",
        "        # Download models for table extraction if available\n",
        "\n",
        "        try:\n",
        "            from unstructured.partition.pdf import partition_pdf_with_table_extraction\n",
        "\n",
        "        except:\n",
        "            print(\"ℹ️ Table extraction models not required\")\n",
        "\n",
        "        print(\"\\n✅ All UnstructuredIO models pre-downloaded successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Some models may download on first use: {e}\")\n",
        "\n",
        "\n",
        "# Run the pre-download function\n",
        "predownload_unstructured_models()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "# Install and start Ollama\n",
        "print(\"📦 Installing Ollama...\")\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        [\"curl\", \"-fsSL\", \"https://ollama.com/install.sh\"],\n",
        "        capture_output=True, text=True, check=True\n",
        "    )\n",
        "    subprocess.run([\"sh\"], input=result.stdout, text=True, check=True)\n",
        "    print(\"✅ Ollama installed successfully\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"❌ Error installing Ollama: {e}\")\n",
        "\n",
        "# Start Ollama server\n",
        "print(\"🔧 Starting Ollama server...\")\n",
        "os.environ['OLLAMA_HOST'] = '127.0.0.1:11434'\n",
        "\n",
        "try:\n",
        "    ollama_process = subprocess.Popen(\n",
        "        [\"ollama\", \"serve\"],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        preexec_fn=os.setsid\n",
        "    )\n",
        "    print(f\"✅ Ollama server started (PID: {ollama_process.pid})\")\n",
        "    time.sleep(10)\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error starting server: {e}\")\n",
        "\n",
        "# Download models\n",
        "print(\"📥 Downloading Ollama models...\")\n",
        "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
        "TEXT_MODEL = \"gemma3:1b\"\n",
        "\n",
        "models = [EMBEDDING_MODEL, TEXT_MODEL]\n",
        "for model in models:\n",
        "    print(f\"📥 Downloading {model}...\")\n",
        "    try:\n",
        "        result = subprocess.run([\"ollama\", \"pull\", model],\n",
        "                              capture_output=True, text=True, timeout=600)\n",
        "        if result.returncode == 0:\n",
        "            print(f\"✅ {model} downloaded successfully\")\n",
        "        else:\n",
        "            print(f\"❌ Error downloading {model}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error downloading {model}: {e}\")\n",
        "\n",
        "# Test connection\n",
        "import ollama\n",
        "try:\n",
        "    response = ollama.chat(model=TEXT_MODEL, messages=[{'role': 'user', 'content': 'test'}])\n",
        "    print(\"✅ Ollama connection working\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Connection error: {e}\")\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - All models pre-downloaded!\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-06T06:56:22.907297Z",
          "iopub.execute_input": "2025-09-06T06:56:22.907494Z",
          "iopub.status.idle": "2025-09-06T06:59:45.928293Z",
          "shell.execute_reply.started": "2025-09-06T06:56:22.907475Z",
          "shell.execute_reply": "2025-09-06T06:59:45.927367Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552,
          "referenced_widgets": [
            "e837379ed6064caeaa6a9a83a10a878f",
            "efbfd056152d4673a5176a7361c55db6",
            "38a479512c274d338e0303331c2e962c",
            "01c07454131649798e6f85c25d9df9b7",
            "a6f42124686d491aa88a398a49999c7f",
            "03c2e9e748a245fd8e499ec3957fa3a8",
            "a99a38b21c724315a97a9d9beb8d3b0d",
            "8a62e69e69214daa99947d9b80667e18",
            "8a45e0d6ab314bfcb95a64b20d594eb5",
            "52a14e55e65b4b758d1282b07e332736",
            "6966012f38cc4fa08bcb1f99d7d0eb53"
          ]
        },
        "id": "YOvhd-iLV9rD",
        "outputId": "c768daca-7c60-4505-8598-b228fc989767"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Enhanced RAG-Based Expense Claims Processing System\n",
            "🔍 Features: Filename-based IDs + Token Usage Tracking\n",
            "======================================================================\n",
            "📦 Installing dependencies...\n",
            "✅ Dependencies installed\n",
            "\n",
            "📥 Pre-downloading UnstructuredIO models...\n",
            "======================================================================\n",
            "Warning: No languages specified, defaulting to English.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "yolox_l0.05.onnx:   0%|          | 0.00/217M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e837379ed6064caeaa6a9a83a10a878f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ YOLO model (yolox_l0.05.onnx) downloaded successfully\n",
            "ℹ️ Excel/CSV models not required or already available\n",
            "ℹ️ Table extraction models not required\n",
            "\n",
            "✅ All UnstructuredIO models pre-downloaded successfully!\n",
            "\n",
            "======================================================================\n",
            "📦 Installing Ollama...\n",
            "✅ Ollama installed successfully\n",
            "🔧 Starting Ollama server...\n",
            "✅ Ollama server started (PID: 6803)\n",
            "📥 Downloading Ollama models...\n",
            "📥 Downloading nomic-embed-text...\n",
            "✅ nomic-embed-text downloaded successfully\n",
            "📥 Downloading gemma3:1b...\n",
            "✅ gemma3:1b downloaded successfully\n",
            "✅ Ollama connection working\n",
            "\n",
            "✅ SETUP COMPLETE - All models pre-downloaded!\n",
            "======================================================================\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# STEP 2: TOKEN USAGE TRACKING\n",
        "# ================================\n",
        "\n",
        "class TokenUsageTracker:\n",
        "    \"\"\"Track token usage across all LLM calls\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.call_history = []\n",
        "        self.total_input_tokens = 0\n",
        "        self.total_output_tokens = 0\n",
        "        self.total_tokens = 0\n",
        "        self.call_count = 0\n",
        "\n",
        "    def track_call(self, operation: str, filename: str, task: str, response) -> Dict[str, Any]:\n",
        "        \"\"\"Track a single LLM call and extract usage info\"\"\"\n",
        "\n",
        "        usage_info = {\n",
        "            \"operation\": operation,\n",
        "            \"filename\": filename,\n",
        "            \"task\": task,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"input_tokens\": 0,\n",
        "            \"output_tokens\": 0,\n",
        "            \"total_tokens\": 0,\n",
        "            \"duration_ms\": 0\n",
        "        }\n",
        "\n",
        "        # Extract token usage from response\n",
        "        try:\n",
        "            if hasattr(response, 'usage_metadata') and response.usage_metadata:\n",
        "                usage_info[\"input_tokens\"] = response.usage_metadata.get('input_tokens', 0)\n",
        "                usage_info[\"output_tokens\"] = response.usage_metadata.get('output_tokens', 0)\n",
        "                usage_info[\"total_tokens\"] = response.usage_metadata.get('total_tokens', 0)\n",
        "\n",
        "            # Fallback: try response_metadata\n",
        "            elif hasattr(response, 'response_metadata') and response.response_metadata:\n",
        "                metadata = response.response_metadata\n",
        "                usage_info[\"input_tokens\"] = metadata.get('prompt_eval_count', 0)\n",
        "                usage_info[\"output_tokens\"] = metadata.get('eval_count', 0)\n",
        "                usage_info[\"total_tokens\"] = usage_info[\"input_tokens\"] + usage_info[\"output_tokens\"]\n",
        "                usage_info[\"duration_ms\"] = metadata.get('total_duration', 0) // 1000000  # Convert to ms\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not extract token usage: {e}\")\n",
        "\n",
        "        # Update totals\n",
        "        self.total_input_tokens += usage_info[\"input_tokens\"]\n",
        "        self.total_output_tokens += usage_info[\"output_tokens\"]\n",
        "        self.total_tokens += usage_info[\"total_tokens\"]\n",
        "        self.call_count += 1\n",
        "\n",
        "        # Store call history\n",
        "        self.call_history.append(usage_info)\n",
        "\n",
        "        # Print usage info\n",
        "        self.print_usage_info(usage_info)\n",
        "\n",
        "        return usage_info\n",
        "\n",
        "\n",
        "    def print_usage_info(self, usage_info: Dict[str, Any]):\n",
        "        \"\"\"Print formatted usage information\"\"\"\n",
        "        print(f\"📊 TOKEN USAGE - {usage_info['operation']} | {usage_info['filename']} | {usage_info['task']}\")\n",
        "        print(f\"   📥 Input: {usage_info['input_tokens']} tokens\")\n",
        "        print(f\"   📤 Output: {usage_info['output_tokens']} tokens\")\n",
        "        print(f\"   🔢 Total: {usage_info['total_tokens']} tokens\")\n",
        "        if usage_info['duration_ms'] > 0:\n",
        "            print(f\"   ⏱️ Duration: {usage_info['duration_ms']}ms\")\n",
        "        print()\n",
        "\n",
        "    def print_summary(self):\n",
        "        \"\"\"Print overall token usage summary\"\"\"\n",
        "        print(\"=\" * 60)\n",
        "        print(\"📊 TOTAL TOKEN USAGE SUMMARY\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"🔢 Total LLM Calls: {self.call_count}\")\n",
        "        print(f\"📥 Total Input Tokens: {self.total_input_tokens:,}\")\n",
        "        print(f\"📤 Total Output Tokens: {self.total_output_tokens:,}\")\n",
        "        print(f\"🎯 Grand Total Tokens: {self.total_tokens:,}\")\n",
        "\n",
        "        if self.call_count > 0:\n",
        "            print(f\"📊 Average per call: {self.total_tokens/self.call_count:.1f} tokens\")\n",
        "        print()\n",
        "\n",
        "# Global token tracker\n",
        "token_tracker = TokenUsageTracker()\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - TokenUsageTracker!\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-06T07:00:08.883313Z",
          "iopub.execute_input": "2025-09-06T07:00:08.883900Z",
          "iopub.status.idle": "2025-09-06T07:00:08.894785Z",
          "shell.execute_reply.started": "2025-09-06T07:00:08.883871Z",
          "shell.execute_reply": "2025-09-06T07:00:08.894231Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S6W71TnV9rJ",
        "outputId": "48f33292-14d7-48d0-efd3-5094fc5bbbdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ SETUP COMPLETE - TokenUsageTracker!\n",
            "======================================================================\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# STEP 3: FILENAME-BASED DOCUMENT MANAGEMENT\n",
        "# ================================\n",
        "\n",
        "@dataclass\n",
        "class ClaimDocument:\n",
        "    \"\"\"Document with filename-based identification\"\"\"\n",
        "    filename: str  # Primary identifier (no more UUIDs!)\n",
        "    file_path: str\n",
        "    raw_text: str\n",
        "    chunks: List[str]\n",
        "    metadata: Dict[str, Any]\n",
        "    processed_timestamp: datetime\n",
        "\n",
        "class FilenameBasedDocumentManager:\n",
        "    \"\"\"Manages documents using filenames as primary identifiers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.documents_registry = {}  # filename -> ClaimDocument\n",
        "        self.chunk_to_file_map = {}  # chunk_id -> filename\n",
        "\n",
        "    def register_document(self, file_path: str, raw_text: str) -> str:\n",
        "        \"\"\"Register document using filename as ID\"\"\"\n",
        "\n",
        "        filename = Path(file_path).stem  # Get filename without extension\n",
        "\n",
        "        print(f\"📋 REGISTERING DOCUMENT: {filename}\")\n",
        "        print(f\"   📁 Source: {Path(file_path).name}\")\n",
        "        print(f\"   📄 Text length: {len(raw_text)} characters\")\n",
        "\n",
        "        # Create isolated chunks for this document\n",
        "        chunks = self.create_document_chunks(raw_text, filename)\n",
        "\n",
        "        claim_doc = ClaimDocument(\n",
        "            filename=filename,\n",
        "            file_path=file_path,\n",
        "            raw_text=raw_text,\n",
        "            chunks=chunks,\n",
        "            metadata={\n",
        "                \"file_name\": Path(file_path).name,\n",
        "                \"file_extension\": Path(file_path).suffix,\n",
        "                \"chunk_count\": len(chunks),\n",
        "                \"source\": \"ocr_extraction\"\n",
        "            },\n",
        "            processed_timestamp=datetime.now()\n",
        "        )\n",
        "\n",
        "        self.documents_registry[filename] = claim_doc\n",
        "\n",
        "        # Update chunk mapping\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk_id = f\"{filename}_chunk_{i}\"\n",
        "            self.chunk_to_file_map[chunk_id] = filename\n",
        "\n",
        "        print(f\"✅ Document registered: {filename} with {len(chunks)} chunks\")\n",
        "        return filename\n",
        "\n",
        "    def create_document_chunks(self, text: str, filename: str) -> List[str]:\n",
        "        \"\"\"Create chunks with filename-specific context isolation\"\"\"\n",
        "\n",
        "        print(f\"🔪 CHUNKING DOCUMENT: {filename}\")\n",
        "\n",
        "        lines = text.split('\\n')\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "        max_chunk_size = 500\n",
        "\n",
        "        # Expense document section markers\n",
        "        section_markers = [\n",
        "            'total', 'amount', 'date', 'vendor', 'receipt', 'invoice',\n",
        "            'item', 'quantity', 'price', 'tax', 'subtotal'\n",
        "        ]\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            line_length = len(line)\n",
        "            is_section_start = any(marker in line.lower() for marker in section_markers)\n",
        "\n",
        "            if (current_length + line_length > max_chunk_size) or \\\n",
        "               (is_section_start and current_chunk and current_length > 200):\n",
        "\n",
        "                chunk_text = '\\n'.join(current_chunk)\n",
        "                if chunk_text.strip():\n",
        "                    # Add filename isolation metadata to chunk\n",
        "                    isolated_chunk = f\"[DOCUMENT: {filename}]\\n{chunk_text}\"\n",
        "                    chunks.append(isolated_chunk)\n",
        "\n",
        "                current_chunk = [line]\n",
        "                current_length = line_length\n",
        "            else:\n",
        "                current_chunk.append(line)\n",
        "                current_length += line_length + 1\n",
        "\n",
        "        # Add final chunk\n",
        "        if current_chunk:\n",
        "            chunk_text = '\\n'.join(current_chunk)\n",
        "            if chunk_text.strip():\n",
        "                isolated_chunk = f\"[DOCUMENT: {filename}]\\n{chunk_text}\"\n",
        "                chunks.append(isolated_chunk)\n",
        "\n",
        "        print(f\"   🔪 Created {len(chunks)} chunks (avg {len(text)//len(chunks) if chunks else 0} chars each)\")\n",
        "        return chunks\n",
        "\n",
        "\n",
        "\n",
        "    def get_document_context(self, filename: str) -> Optional[ClaimDocument]:\n",
        "        \"\"\"Get complete context for a specific document\"\"\"\n",
        "        return self.documents_registry.get(filename)\n",
        "\n",
        "    def list_all_documents(self) -> List[str]:\n",
        "        \"\"\"List all registered filenames\"\"\"\n",
        "        return list(self.documents_registry.keys())\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - FILENAME-BASED DOCUMENT MANAGEMENT!\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-06T07:00:13.315565Z",
          "iopub.execute_input": "2025-09-06T07:00:13.316275Z",
          "iopub.status.idle": "2025-09-06T07:00:13.327920Z",
          "shell.execute_reply.started": "2025-09-06T07:00:13.316248Z",
          "shell.execute_reply": "2025-09-06T07:00:13.327201Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61RJN3xaV9rK",
        "outputId": "9a8a5808-46b4-42dd-c740-a5ce4279ece1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ SETUP COMPLETE - FILENAME-BASED DOCUMENT MANAGEMENT!\n",
            "======================================================================\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# STEP 4: ENHANCED OCR PROCESSOR\n",
        "# ================================\n",
        "\n",
        "class EnhancedOCRProcessor:\n",
        "    \"\"\"OCR processing with detailed progress tracking\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.supported_formats = ['.pdf', '.jpg', '.jpeg', '.png', '.tiff']\n",
        "\n",
        "    def extract_text_from_document(self, file_path: str) -> str:\n",
        "        \"\"\"Extract text with detailed progress tracking\"\"\"\n",
        "\n",
        "        filename = Path(file_path).name\n",
        "        print(f\"🔍 EXTRACTING TEXT FROM: {filename}\")\n",
        "        print(f\"   📁 Full path: {file_path}\")\n",
        "        print(f\"   📊 File size: {Path(file_path).stat().st_size / 1024:.1f} KB\")\n",
        "\n",
        "        try:\n",
        "            from unstructured.partition.auto import partition\n",
        "\n",
        "            print(f\"   🔄 Processing with UnstructuredIO...\")\n",
        "\n",
        "            # Process document with UnstructuredIO\n",
        "            elements = partition(filename=file_path)\n",
        "\n",
        "            print(f\"   📋 Found {len(elements)} document elements\")\n",
        "\n",
        "            # Extract text from all elements\n",
        "            full_text = \"\"\n",
        "            for i, element in enumerate(elements):\n",
        "                if hasattr(element, 'text') and element.text:\n",
        "                    full_text += element.text + \"\\n\"\n",
        "                    if i < 5:  # Show first few elements\n",
        "                        print(f\"     Element {i+1}: {element.text[:50]}...\")\n",
        "\n",
        "            # Clean and normalize text\n",
        "            full_text = self.clean_extracted_text(full_text)\n",
        "\n",
        "            print(f\"   ✅ Extracted {len(full_text)} characters\")\n",
        "            print(f\"   📝 Text preview: {full_text[:100]}...\")\n",
        "            return full_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ OCR extraction failed: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def clean_extracted_text(self, text: str) -> str:\n",
        "        \"\"\"Clean extracted text with progress info\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        original_length = len(text)\n",
        "        lines = text.split('\\n')\n",
        "        cleaned_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line and len(line) > 2:\n",
        "                cleaned_lines.append(line)\n",
        "\n",
        "        cleaned_text = '\\n'.join(cleaned_lines)\n",
        "        print(f\"   🧹 Cleaned: {original_length} → {len(cleaned_text)} chars ({len(cleaned_lines)} lines)\")\n",
        "\n",
        "        return cleaned_text\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - FILENAME-BASED DOCUMENT MANAGEMENT!\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-06T07:00:49.825454Z",
          "iopub.execute_input": "2025-09-06T07:00:49.825722Z",
          "iopub.status.idle": "2025-09-06T07:00:49.834392Z",
          "shell.execute_reply.started": "2025-09-06T07:00:49.825704Z",
          "shell.execute_reply": "2025-09-06T07:00:49.833611Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojCZS0dJV9rL",
        "outputId": "ad015d19-15ad-4fa2-88f4-a5d1efaff894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ SETUP COMPLETE - FILENAME-BASED DOCUMENT MANAGEMENT!\n",
            "======================================================================\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# STEP 5: ENHANCED VECTOR STORE\n",
        "# ================================\n",
        "\n",
        "class EnhancedIsolatedVectorStore:\n",
        "    \"\"\"ChromaDB with enhanced tracking and filename-based isolation\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model: str = EMBEDDING_MODEL):\n",
        "        import chromadb\n",
        "\n",
        "        self.embedding_model = embedding_model\n",
        "\n",
        "        print(f\"🗄️ INITIALIZING VECTOR STORE\")\n",
        "        print(f\"   🤖 Embedding Model: {embedding_model}\")\n",
        "\n",
        "        # Initialize ChromaDB client using new API\n",
        "        self.client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "        # Create collection\n",
        "        self.collection = self.client.get_or_create_collection(\n",
        "            name=\"filename_based_expense_claims\",\n",
        "            metadata={\"hnsw:space\": \"cosine\"}\n",
        "        )\n",
        "\n",
        "        print(f\"   ✅ ChromaDB initialized\")\n",
        "\n",
        "    def embed_text(self, text: str, filename: str = \"unknown\") -> List[float]:\n",
        "        \"\"\"Generate embeddings with progress tracking\"\"\"\n",
        "\n",
        "        print(f\"🔢 GENERATING EMBEDDING: {filename}\")\n",
        "        print(f\"   📝 Text length: {len(text)} chars\")\n",
        "\n",
        "        try:\n",
        "            response = ollama.embeddings(model=self.embedding_model, prompt=text)\n",
        "            embedding = response['embedding']\n",
        "            print(f\"   ✅ Generated {len(embedding)}-dimensional embedding\")\n",
        "            return embedding\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Embedding error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def add_document_chunks(self, filename: str, chunks: List[str], metadata: Dict[str, Any]):\n",
        "        \"\"\"Add chunks for a specific document with detailed tracking\"\"\"\n",
        "\n",
        "        print(f\"📚 ADDING CHUNKS TO VECTOR STORE: {filename}\")\n",
        "        print(f\"   📊 Number of chunks: {len(chunks)}\")\n",
        "\n",
        "        embeddings = []\n",
        "        chunk_ids = []\n",
        "        metadatas = []\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            print(f\"   🔄 Processing chunk {i+1}/{len(chunks)}\")\n",
        "\n",
        "            # Generate embedding\n",
        "            embedding = self.embed_text(chunk, f\"{filename}_chunk_{i}\")\n",
        "            if not embedding:\n",
        "                print(f\"   ⚠️ Skipping chunk {i+1} - no embedding generated\")\n",
        "                continue\n",
        "\n",
        "            chunk_id = f\"{filename}_chunk_{i}\"\n",
        "            chunk_metadata = {\n",
        "                **metadata,\n",
        "                \"filename\": filename,\n",
        "                \"chunk_index\": i,\n",
        "                \"chunk_id\": chunk_id,\n",
        "                \"isolated\": True\n",
        "            }\n",
        "\n",
        "            embeddings.append(embedding)\n",
        "            chunk_ids.append(chunk_id)\n",
        "            metadatas.append(chunk_metadata)\n",
        "\n",
        "        # Add to ChromaDB\n",
        "        if embeddings:\n",
        "            self.collection.add(\n",
        "                embeddings=embeddings,\n",
        "                documents=chunks,\n",
        "                metadatas=metadatas,\n",
        "                ids=chunk_ids\n",
        "            )\n",
        "\n",
        "            print(f\"   ✅ Added {len(embeddings)} chunks to vector store\")\n",
        "        else:\n",
        "            print(f\"   ❌ No chunks added - all embeddings failed\")\n",
        "\n",
        "    def query_document_specific(self, query: str, filename: str, n_results: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"Query specific document only - prevents cross-contamination\"\"\"\n",
        "\n",
        "        print(f\"🔍 QUERYING VECTOR STORE: {filename}\")\n",
        "        print(f\"   ❓ Query: {query}\")\n",
        "        print(f\"   📊 Requesting {n_results} results\")\n",
        "\n",
        "        query_embedding = self.embed_text(query, f\"query_{filename}\")\n",
        "        if not query_embedding:\n",
        "            return {\"error\": \"Failed to generate query embedding\"}\n",
        "\n",
        "        # Query with filename filter to ensure isolation\n",
        "        results = self.collection.query(\n",
        "            query_embeddings=[query_embedding],\n",
        "            n_results=n_results,\n",
        "            where={\"filename\": filename},  # CRITICAL: Isolates to specific document\n",
        "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
        "        )\n",
        "\n",
        "        print(f\"   ✅ Found {len(results['documents'][0]) if results['documents'] else 0} relevant chunks\")\n",
        "\n",
        "        return {\n",
        "            \"documents\": results['documents'][0] if results['documents'] else [],\n",
        "            \"metadatas\": results['metadatas'][0] if results['metadatas'] else [],\n",
        "            \"distances\": results['distances'][0] if results['distances'] else [],\n",
        "            \"filename\": filename\n",
        "        }\n",
        "\n",
        "    def get_collection_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get detailed statistics about stored documents\"\"\"\n",
        "\n",
        "        print(\"📊 GENERATING COLLECTION STATISTICS\")\n",
        "\n",
        "        count = self.collection.count()\n",
        "\n",
        "        # Get unique filenames\n",
        "        all_metadata = self.collection.get(include=[\"metadatas\"])\n",
        "        filenames = set()\n",
        "        if all_metadata['metadatas']:\n",
        "            for meta in all_metadata['metadatas']:\n",
        "                if 'filename' in meta:\n",
        "                    filenames.add(meta['filename'])\n",
        "\n",
        "        stats = {\n",
        "            \"total_chunks\": count,\n",
        "            \"unique_documents\": len(filenames),\n",
        "            \"filenames\": list(filenames)\n",
        "        }\n",
        "\n",
        "        print(f\"   📚 Total chunks: {stats['total_chunks']}\")\n",
        "        print(f\"   📄 Unique documents: {stats['unique_documents']}\")\n",
        "        print(f\"   📝 Documents: {', '.join(stats['filenames'])}\")\n",
        "\n",
        "        return stats\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - ENHANCED OCR PROCESSOR!\")\n",
        "print(\"=\" * 70)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-06T07:00:57.701457Z",
          "iopub.execute_input": "2025-09-06T07:00:57.702197Z",
          "iopub.status.idle": "2025-09-06T07:00:57.716681Z",
          "shell.execute_reply.started": "2025-09-06T07:00:57.702170Z",
          "shell.execute_reply": "2025-09-06T07:00:57.715888Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_8X7GMzV9rM",
        "outputId": "284dbba9-ec35-49f3-efa5-2a80c5318537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ SETUP COMPLETE - ENHANCED OCR PROCESSOR!\n",
            "======================================================================\n"
          ]
        }
      ],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# STEP 6: ENHANCED EXPENSE TASK MANAGER\n",
        "# ================================\n",
        "\n",
        "class EnhancedExpenseTaskManager:\n",
        "    \"\"\"Manages predefined expense extraction tasks with better tracking\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.predefined_tasks = {\n",
        "            \"extract_amount\": {\n",
        "                \"query\": \"total amount due payment cost price sum money dollar\",\n",
        "                \"description\": \"Extract the total amount from this expense document\",\n",
        "                \"expected_format\": \"numeric value with currency\"\n",
        "            },\n",
        "            \"extract_date\": {\n",
        "                \"query\": \"date transaction purchase invoice receipt timestamp when\",\n",
        "                \"description\": \"Extract the date from this expense document\",\n",
        "                \"expected_format\": \"date in YYYY-MM-DD format\"\n",
        "            },\n",
        "            \"extract_vendor\": {\n",
        "                \"query\": \"vendor merchant company business supplier store restaurant hotel\",\n",
        "                \"description\": \"Extract vendor/merchant name from this expense document\",\n",
        "                \"expected_format\": \"company or business name\"\n",
        "            },\n",
        "            \"extract_category\": {\n",
        "                \"query\": \"category type classification expense kind service product item\",\n",
        "                \"description\": \"Determine expense category from this document\",\n",
        "                \"expected_format\": \"expense category classification\"\n",
        "            },\n",
        "            \"extract_items\": {\n",
        "                \"query\": \"items products services line items purchases description details\",\n",
        "                \"description\": \"Extract itemized details from this expense document\",\n",
        "                \"expected_format\": \"list of items or services\"\n",
        "            },\n",
        "            \"extract_tax\": {\n",
        "                \"query\": \"tax VAT GST sales tax tax rate percentage\",\n",
        "                \"description\": \"Extract tax information from this expense document\",\n",
        "                \"expected_format\": \"tax amount and rate\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def get_task_info(self, task_name: str) -> Dict[str, str]:\n",
        "        \"\"\"Get complete task information\"\"\"\n",
        "        return self.predefined_tasks.get(task_name, {})\n",
        "\n",
        "    def list_available_tasks(self) -> List[str]:\n",
        "        \"\"\"List all available extraction tasks\"\"\"\n",
        "        return list(self.predefined_tasks.keys())\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - ENHANCED EXPENSE TASK MANAGER!\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-06T07:01:04.583578Z",
          "iopub.execute_input": "2025-09-06T07:01:04.583827Z",
          "iopub.status.idle": "2025-09-06T07:01:04.590497Z",
          "shell.execute_reply.started": "2025-09-06T07:01:04.583808Z",
          "shell.execute_reply": "2025-09-06T07:01:04.589670Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNW0qSa7V9rN",
        "outputId": "48ed6957-a22e-45e6-d99a-2a86f34de2ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ SETUP COMPLETE - ENHANCED EXPENSE TASK MANAGER!\n",
            "======================================================================\n"
          ]
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# STEP 7: ENHANCED RAG PROCESSOR\n",
        "# ================================\n",
        "\n",
        "class EnhancedRAGExpenseProcessor:\n",
        "    \"\"\"RAG-based expense processor with comprehensive tracking\"\"\"\n",
        "\n",
        "    def __init__(self, text_model: str = TEXT_MODEL):\n",
        "        from langchain_ollama import ChatOllama\n",
        "\n",
        "        print(f\"🚀 INITIALIZING RAG EXPENSE PROCESSOR\")\n",
        "        print(f\"   🤖 Text Model: {text_model}\")\n",
        "\n",
        "        self.llm = ChatOllama(\n",
        "            model=text_model,\n",
        "            temperature=0.1,\n",
        "            base_url=\"http://127.0.0.1:11434\"\n",
        "        )\n",
        "\n",
        "        self.vector_store = EnhancedIsolatedVectorStore()\n",
        "        self.task_manager = EnhancedExpenseTaskManager()\n",
        "        self.document_manager = FilenameBasedDocumentManager()\n",
        "        self.ocr_processor = EnhancedOCRProcessor()\n",
        "\n",
        "        print(\"   ✅ All components initialized\")\n",
        "\n",
        "    def ingest_document(self, file_path: str) -> str:\n",
        "        \"\"\"INGESTION PHASE: Process document and store in vector DB\"\"\"\n",
        "\n",
        "        filename = Path(file_path).name\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"🔄 INGESTION PHASE STARTING\")\n",
        "        print(f\"📄 FILE: {filename}\")\n",
        "        print(f\"📁 PATH: {file_path}\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Step 1: OCR extraction\n",
        "        raw_text = self.ocr_processor.extract_text_from_document(file_path)\n",
        "        if not raw_text:\n",
        "            print(\"❌ INGESTION FAILED: No text extracted\")\n",
        "            return None\n",
        "\n",
        "        # Step 2: Register document with filename-based system\n",
        "        filename_id = self.document_manager.register_document(file_path, raw_text)\n",
        "\n",
        "        # Step 3: Get document context\n",
        "        document = self.document_manager.get_document_context(filename_id)\n",
        "\n",
        "        # Step 4: Store in vector database\n",
        "        metadata = {\n",
        "            **document.metadata,\n",
        "            \"ingestion_timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        self.vector_store.add_document_chunks(\n",
        "            filename=filename_id,\n",
        "            chunks=document.chunks,\n",
        "            metadata=metadata\n",
        "        )\n",
        "\n",
        "        print(f\"✅ INGESTION COMPLETED: {filename_id}\")\n",
        "        print(\"=\"*70)\n",
        "        return filename_id\n",
        "\n",
        "    def process_expense_task(self, filename: str, task_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"INFERENCE PHASE: Process specific task for document\"\"\"\n",
        "\n",
        "        print(f\"\\n🎯 INFERENCE PHASE STARTING\")\n",
        "        print(f\"📄 DOCUMENT: {filename}\")\n",
        "        print(f\"🎯 TASK: {task_name}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Step 1: Get task information\n",
        "        task_info = self.task_manager.get_task_info(task_name)\n",
        "        if not task_info:\n",
        "            return {\"error\": f\"Unknown task: {task_name}\"}\n",
        "\n",
        "        task_query = task_info.get(\"query\", \"\")\n",
        "        task_description = task_info.get(\"description\", \"\")\n",
        "\n",
        "        print(f\"📋 Task Description: {task_description}\")\n",
        "        print(f\"🔍 Search Query: {task_query}\")\n",
        "\n",
        "        # Step 2: Retrieve relevant chunks (ISOLATED to this document)\n",
        "        retrieval_results = self.vector_store.query_document_specific(\n",
        "            query=task_query,\n",
        "            filename=filename,\n",
        "            n_results=3\n",
        "        )\n",
        "\n",
        "        if retrieval_results.get(\"error\"):\n",
        "            return retrieval_results\n",
        "\n",
        "        # Step 3: Prepare optimized context\n",
        "        context = self.optimize_context(retrieval_results, task_name)\n",
        "\n",
        "        # Step 4: Generate response with LLM (WITH TOKEN TRACKING)\n",
        "        response_text, token_usage = self.generate_task_response_with_tracking(\n",
        "            context, task_name, task_description, filename\n",
        "        )\n",
        "\n",
        "        result = {\n",
        "            \"task\": task_name,\n",
        "            \"filename\": filename,\n",
        "            \"response\": response_text,\n",
        "            \"context_chunks_used\": len(retrieval_results[\"documents\"]),\n",
        "            # \"confidence\": self.calculate_confidence(retrieval_results),\n",
        "            \"token_usage\": token_usage\n",
        "        }\n",
        "\n",
        "        print(f\"✅ INFERENCE COMPLETED: {task_name} for {filename}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def optimize_context(self, retrieval_results: Dict[str, Any], task_name: str) -> str:\n",
        "        \"\"\"CONTEXT OPTIMIZATION: Reduce context overloading\"\"\"\n",
        "\n",
        "        documents = retrieval_results.get(\"documents\", [])\n",
        "        distances = retrieval_results.get(\"distances\", [])\n",
        "        filename = retrieval_results.get(\"filename\", \"unknown\")\n",
        "\n",
        "        print(f\"🔧 OPTIMIZING CONTEXT: {filename}\")\n",
        "        print(f\"   📊 Raw chunks: {len(documents)}\")\n",
        "\n",
        "        if not documents:\n",
        "            return \"No relevant context found\"\n",
        "\n",
        "        # Rank documents by relevance\n",
        "        doc_scores = list(zip(documents, distances))\n",
        "        doc_scores.sort(key=lambda x: x[1])\n",
        "\n",
        "        optimized_chunks = []\n",
        "        total_length = 0\n",
        "        max_context_length = 1500\n",
        "\n",
        "        for i, (doc, score) in enumerate(doc_scores):\n",
        "            # Remove document prefix from chunks\n",
        "            clean_doc = doc.replace(f\"[DOCUMENT: {filename}]\\n\", \"\")\n",
        "\n",
        "            if total_length + len(clean_doc) <= max_context_length:\n",
        "                optimized_chunks.append(clean_doc)\n",
        "                total_length += len(clean_doc)\n",
        "                print(f\"   ✅ Chunk {i+1}: {len(clean_doc)} chars (relevance: {score:.3f})\")\n",
        "            else:\n",
        "                remaining_space = max_context_length - total_length\n",
        "                if remaining_space > 100:\n",
        "                    truncated = clean_doc[:remaining_space] + \"...\"\n",
        "                    optimized_chunks.append(truncated)\n",
        "                    print(f\"   ✂️ Chunk {i+1}: truncated to {len(truncated)} chars\")\n",
        "                break\n",
        "\n",
        "        context = \"\\n\\n---\\n\\n\".join(optimized_chunks)\n",
        "        print(f\"   🎯 Final context: {len(context)} chars from {len(optimized_chunks)} chunks\")\n",
        "\n",
        "        return context\n",
        "\n",
        "    def generate_task_response_with_tracking(self, context: str, task_name: str, task_description: str, filename: str) -> Tuple[str, Dict[str, Any]]:\n",
        "        \"\"\"Generate LLM response with token usage tracking\"\"\"\n",
        "\n",
        "        print(f\"🤖 GENERATING LLM RESPONSE: {task_name} | {filename}\")\n",
        "\n",
        "        prompt = f\"\"\"You are an expert expense analyst. {task_description}\n",
        "\n",
        "CONTEXT FROM EXPENSE DOCUMENT ({filename}):\n",
        "{context}\n",
        "\n",
        "TASK: {task_name}\n",
        "INSTRUCTION: {task_description}\n",
        "\n",
        "Based ONLY on the context provided above, extract the requested information. Be precise and factual. If the information is not clearly present in the context, state \"Information not found in provided context.\"\n",
        "\n",
        "Response:\"\"\"\n",
        "\n",
        "        print(f\"   📝 Prompt length: {len(prompt)} characters\")\n",
        "\n",
        "        try:\n",
        "            response = self.llm.invoke(prompt)\n",
        "\n",
        "            # Track token usage\n",
        "            token_usage = token_tracker.track_call(\"llm_inference\", filename, task_name, response)\n",
        "\n",
        "            return response.content.strip(), token_usage\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error generating response: {e}\"\n",
        "            print(f\"   ❌ {error_msg}\")\n",
        "            return error_msg, {}\n",
        "\n",
        "    # def calculate_confidence(self, retrieval_results: Dict[str, Any]) -> float:\n",
        "    #     \"\"\"Calculate confidence based on retrieval quality\"\"\"\n",
        "    #     distances = retrieval_results.get(\"distances\", [])\n",
        "    #     if not distances:\n",
        "    #         return 0.0\n",
        "\n",
        "    #     avg_distance = sum(distances) / len(distances)\n",
        "    #     confidence = max(0.0, 1.0 - avg_distance)\n",
        "    #     return round(confidence, 3)\n",
        "\n",
        "    def process_all_tasks_for_document(self, filename: str) -> Dict[str, Any]:\n",
        "        \"\"\"Process all predefined tasks for a document\"\"\"\n",
        "\n",
        "        print(f\"\\n📊 PROCESSING ALL TASKS FOR: {filename}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        tasks = self.task_manager.list_available_tasks()\n",
        "        results = {}\n",
        "\n",
        "        for i, task in enumerate(tasks, 1):\n",
        "            print(f\"\\n[{i}/{len(tasks)}] Starting task: {task}\")\n",
        "            result = self.process_expense_task(filename, task)\n",
        "            results[task] = result\n",
        "\n",
        "        print(f\"\\n✅ ALL TASKS COMPLETED FOR: {filename}\")\n",
        "        return results\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - ENHANCED RAG PROCESSOR!\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-06T07:01:11.237985Z",
          "iopub.execute_input": "2025-09-06T07:01:11.238259Z",
          "iopub.status.idle": "2025-09-06T07:01:11.255967Z",
          "shell.execute_reply.started": "2025-09-06T07:01:11.238239Z",
          "shell.execute_reply": "2025-09-06T07:01:11.255261Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3nRw7oIV9rO",
        "outputId": "b9445f04-fa0b-4c01-a8d8-47286cfe364e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ SETUP COMPLETE - ENHANCED RAG PROCESSOR!\n",
            "======================================================================\n"
          ]
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# STEP 8: ENHANCED WORKFLOW\n",
        "# ================================\n",
        "\n",
        "from langgraph.graph import StateGraph\n",
        "from typing import TypedDict\n",
        "\n",
        "class EnhancedRAGWorkflowState(TypedDict):\n",
        "    \"\"\"Enhanced state for RAG workflow\"\"\"\n",
        "    file_paths: List[str]\n",
        "    current_file_index: int\n",
        "    processed_filenames: List[str]\n",
        "    current_filename: str\n",
        "    task_results: Dict[str, Dict[str, Any]]\n",
        "    workflow_status: str\n",
        "    error: Optional[str]\n",
        "\n",
        "def create_enhanced_rag_workflow() -> StateGraph:\n",
        "    \"\"\"Create enhanced LangGraph workflow for RAG processing\"\"\"\n",
        "\n",
        "    processor = EnhancedRAGExpenseProcessor()\n",
        "\n",
        "    def enhanced_ingestion_node(state: EnhancedRAGWorkflowState) -> EnhancedRAGWorkflowState:\n",
        "        \"\"\"Enhanced ingestion with detailed tracking\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"🔄 WORKFLOW: ENHANCED INGESTION PHASE STARTING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        file_paths = state.get(\"file_paths\", [])\n",
        "        processed_filenames = []\n",
        "\n",
        "        for i, file_path in enumerate(file_paths, 1):\n",
        "            print(f\"\\n[{i}/{len(file_paths)}] Processing file: {Path(file_path).name}\")\n",
        "\n",
        "            try:\n",
        "                filename = processor.ingest_document(file_path)\n",
        "                if filename:\n",
        "                    processed_filenames.append(filename)\n",
        "                    print(f\"✅ Successfully ingested: {filename}\")\n",
        "                else:\n",
        "                    print(f\"❌ Failed to ingest: {Path(file_path).name}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error ingesting {Path(file_path).name}: {e}\")\n",
        "\n",
        "        state[\"processed_filenames\"] = processed_filenames\n",
        "        state[\"workflow_status\"] = \"ingestion_complete\" if processed_filenames else \"ingestion_failed\"\n",
        "\n",
        "        print(f\"\\n📊 INGESTION PHASE COMPLETED\")\n",
        "        print(f\"   ✅ Successfully processed: {len(processed_filenames)} files\")\n",
        "        print(f\"   ❌ Failed: {len(file_paths) - len(processed_filenames)} files\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    def enhanced_task_processing_node(state: EnhancedRAGWorkflowState) -> EnhancedRAGWorkflowState:\n",
        "        \"\"\"Enhanced task processing with detailed tracking\"\"\"\n",
        "\n",
        "        print(\"\\n🎯 WORKFLOW: ENHANCED TASK PROCESSING PHASE STARTING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        processed_filenames = state.get(\"processed_filenames\", [])\n",
        "        task_results = {}\n",
        "\n",
        "        for i, filename in enumerate(processed_filenames, 1):\n",
        "            print(f\"\\n[{i}/{len(processed_filenames)}] Processing tasks for: {filename}\")\n",
        "\n",
        "            try:\n",
        "                results = processor.process_all_tasks_for_document(filename)\n",
        "                task_results[filename] = results\n",
        "                print(f\"✅ Completed all tasks for: {filename}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing tasks for {filename}: {e}\")\n",
        "                task_results[filename] = {\"error\": str(e)}\n",
        "\n",
        "        state[\"task_results\"] = task_results\n",
        "        state[\"workflow_status\"] = \"processing_complete\"\n",
        "\n",
        "        print(f\"\\n📊 TASK PROCESSING PHASE COMPLETED\")\n",
        "        print(f\"   📄 Documents processed: {len(task_results)}\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    def enhanced_results_compilation_node(state: EnhancedRAGWorkflowState) -> EnhancedRAGWorkflowState:\n",
        "        \"\"\"Enhanced results compilation with detailed stats and CSV export\"\"\"\n",
        "\n",
        "        print(\"\\n📊 WORKFLOW: ENHANCED RESULTS COMPILATION STARTING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        task_results = state.get(\"task_results\", {})\n",
        "\n",
        "        # Compile detailed statistics\n",
        "        total_documents = len(task_results)\n",
        "        successful_documents = sum(1 for r in task_results.values() if \"error\" not in r)\n",
        "\n",
        "        # Save results with timestamp\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        results_file = f\"enhanced_rag_expense_results_{timestamp}.json\"\n",
        "        csv_file = f\"enhanced_rag_expense_results_{timestamp}.csv\"\n",
        "\n",
        "        # Create comprehensive results package\n",
        "        comprehensive_results = {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"summary\": {\n",
        "                \"total_documents\": total_documents,\n",
        "                \"successful_documents\": successful_documents,\n",
        "                \"failed_documents\": total_documents - successful_documents\n",
        "            },\n",
        "            \"token_usage_summary\": {\n",
        "                \"total_calls\": token_tracker.call_count,\n",
        "                \"total_input_tokens\": token_tracker.total_input_tokens,\n",
        "                \"total_output_tokens\": token_tracker.total_output_tokens,\n",
        "                \"total_tokens\": token_tracker.total_tokens\n",
        "            },\n",
        "            \"document_results\": task_results,\n",
        "            \"token_call_history\": token_tracker.call_history\n",
        "        }\n",
        "\n",
        "        # Save JSON results\n",
        "        with open(results_file, 'w') as f:\n",
        "            json.dump(comprehensive_results, f, indent=2, default=str)\n",
        "\n",
        "        print(f\"💾 JSON RESULTS SAVED TO: {results_file}\")\n",
        "\n",
        "        # Create CSV from results\n",
        "        csv_rows = []\n",
        "\n",
        "        for filename, doc_results in task_results.items():\n",
        "            if \"error\" in doc_results:\n",
        "                # Add error row\n",
        "                csv_rows.append({\n",
        "                    \"filename\": filename,\n",
        "                    \"task\": \"error\",\n",
        "                    \"response\": doc_results[\"error\"],\n",
        "                    \"context_chunks_used\": 0,\n",
        "                    \"input_tokens\": 0,\n",
        "                    \"output_tokens\": 0,\n",
        "                    \"total_tokens\": 0\n",
        "                })\n",
        "            else:\n",
        "                # Process each task for this document\n",
        "                for task_name, task_result in doc_results.items():\n",
        "                    if isinstance(task_result, dict):\n",
        "                        token_usage = task_result.get(\"token_usage\", {})\n",
        "                        csv_rows.append({\n",
        "                            \"filename\": filename,\n",
        "                            \"task\": task_name,\n",
        "                            \"response\": task_result.get(\"response\", \"\"),\n",
        "                            \"context_chunks_used\": task_result.get(\"context_chunks_used\", 0),\n",
        "                            \"input_tokens\": token_usage.get(\"input_tokens\", 0),\n",
        "                            \"output_tokens\": token_usage.get(\"output_tokens\", 0),\n",
        "                            \"total_tokens\": token_usage.get(\"total_tokens\", 0)\n",
        "                        })\n",
        "\n",
        "        # Save CSV\n",
        "        if csv_rows:\n",
        "            df = pd.DataFrame(csv_rows)\n",
        "\n",
        "            # Reorder columns for better readability\n",
        "            column_order = [\n",
        "                \"filename\", \"task\", \"response\",\n",
        "                \"context_chunks_used\", \"input_tokens\",\n",
        "                \"output_tokens\", \"total_tokens\"\n",
        "            ]\n",
        "            df = df[column_order]\n",
        "\n",
        "            # Save to CSV\n",
        "            df.to_csv(csv_file, index=False, encoding='utf-8')\n",
        "            print(f\"💾 CSV RESULTS SAVED TO: {csv_file}\")\n",
        "\n",
        "            # Display summary statistics\n",
        "            print(f\"\\n📊 CSV Summary:\")\n",
        "            print(f\"   📄 Total rows: {len(df)}\")\n",
        "            print(f\"   📁 Documents: {df['filename'].nunique()}\")\n",
        "            print(f\"   🎯 Tasks per document: {df.groupby('filename').size().mean():.1f}\")\n",
        "            print(f\"   🔢 Total tokens used: {df['total_tokens'].sum():,}\")\n",
        "\n",
        "        # Also save a summary CSV with aggregated data per document\n",
        "        summary_csv_file = f\"enhanced_rag_expense_summary_{timestamp}.csv\"\n",
        "        summary_rows = []\n",
        "\n",
        "        for filename, doc_results in task_results.items():\n",
        "            if \"error\" not in doc_results:\n",
        "                row = {\"filename\": filename}\n",
        "\n",
        "                # Extract key information from each task\n",
        "                for task_name in [\"extract_amount\", \"extract_date\", \"extract_vendor\",\n",
        "                                \"extract_category\", \"extract_tax\"]:\n",
        "                    if task_name in doc_results:\n",
        "                        response = doc_results[task_name].get(\"response\", \"\")\n",
        "                        # Clean the response (take first line or first 100 chars)\n",
        "                        cleaned = response.split('\\n')[0][:100] if response else \"\"\n",
        "                        row[task_name] = cleaned\n",
        "\n",
        "                # Add token totals\n",
        "                total_tokens = sum(\n",
        "                    doc_results.get(task, {}).get(\"token_usage\", {}).get(\"total_tokens\", 0)\n",
        "                    for task in doc_results if isinstance(doc_results.get(task), dict)\n",
        "                )\n",
        "                row[\"total_tokens_used\"] = total_tokens\n",
        "\n",
        "                summary_rows.append(row)\n",
        "\n",
        "        if summary_rows:\n",
        "            summary_df = pd.DataFrame(summary_rows)\n",
        "            summary_df.to_csv(summary_csv_file, index=False, encoding='utf-8')\n",
        "            print(f\"💾 SUMMARY CSV SAVED TO: {summary_csv_file}\")\n",
        "\n",
        "        print(f\"\\n📊 FILES SAVED:\")\n",
        "        print(f\"   📄 Detailed JSON: {results_file}\")\n",
        "        print(f\"   📄 Detailed CSV: {csv_file}\")\n",
        "        print(f\"   📄 Summary CSV: {summary_csv_file}\")\n",
        "        print(f\"   📄 Documents processed: {successful_documents}/{total_documents}\")\n",
        "\n",
        "        # Print token usage summary\n",
        "        token_tracker.print_summary()\n",
        "\n",
        "        state[\"workflow_status\"] = \"complete\"\n",
        "        return state\n",
        "\n",
        "    # Build enhanced workflow\n",
        "    workflow = StateGraph(EnhancedRAGWorkflowState)\n",
        "\n",
        "    workflow.add_node(\"enhanced_ingestion\", enhanced_ingestion_node)\n",
        "    workflow.add_node(\"enhanced_task_processing\", enhanced_task_processing_node)\n",
        "    workflow.add_node(\"enhanced_results_compilation\", enhanced_results_compilation_node)\n",
        "\n",
        "    workflow.set_entry_point(\"enhanced_ingestion\")\n",
        "    workflow.add_edge(\"enhanced_ingestion\", \"enhanced_task_processing\")\n",
        "    workflow.add_edge(\"enhanced_task_processing\", \"enhanced_results_compilation\")\n",
        "    workflow.set_finish_point(\"enhanced_results_compilation\")\n",
        "\n",
        "    return workflow.compile()\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - ENHANCED WORKFLOW!\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-06T07:01:23.069382Z",
          "iopub.execute_input": "2025-09-06T07:01:23.070155Z",
          "iopub.status.idle": "2025-09-06T07:01:23.605579Z",
          "shell.execute_reply.started": "2025-09-06T07:01:23.070128Z",
          "shell.execute_reply": "2025-09-06T07:01:23.604989Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV3VxY2UV9rP",
        "outputId": "efa23f68-a306-4dd0-f0c7-9d98fc805de4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ SETUP COMPLETE - ENHANCED WORKFLOW!\n",
            "======================================================================\n"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# STEP 9: DEMONSTRATION WITH ENHANCED TRACKING\n",
        "# ================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🚀 ENHANCED RAG DEMONSTRATION STARTING\")\n",
        "print(\"📊 Features: Filename IDs + Token Tracking + Detailed Progress\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Initialize enhanced system\n",
        "processor = EnhancedRAGExpenseProcessor()\n",
        "\n",
        "# Check for sample documents\n",
        "samples_dir = Path(\"/kaggle/input/hotel-bills\")\n",
        "if samples_dir.exists():\n",
        "    sample_files = []\n",
        "    for ext in ['.pdf', '.jpg', '.jpeg', '.png']:\n",
        "        sample_files.extend(list(samples_dir.rglob(f\"*{ext}\")))\n",
        "\n",
        "    # Use first few files for demo\n",
        "    # demo_files = sample_files[:min(3, len(sample_files))]\n",
        "    demo_files = sample_files\n",
        "\n",
        "    if demo_files:\n",
        "        print(f\"📄 FOUND {len(demo_files)} SAMPLE DOCUMENTS:\")\n",
        "        for i, file in enumerate(demo_files, 1):\n",
        "            print(f\"   {i}. {file.name} ({file.stat().st_size/1024:.1f} KB)\")\n",
        "\n",
        "        # Create enhanced workflow\n",
        "        workflow = create_enhanced_rag_workflow()\n",
        "\n",
        "        # Execute enhanced workflow\n",
        "        initial_state = {\n",
        "            \"file_paths\": [str(f) for f in demo_files],\n",
        "            \"current_file_index\": 0,\n",
        "            \"processed_filenames\": [],\n",
        "            \"current_filename\": \"\",\n",
        "            \"task_results\": {},\n",
        "            \"workflow_status\": \"initialized\",\n",
        "            \"error\": None\n",
        "        }\n",
        "\n",
        "        print(\"\\n🔄 EXECUTING ENHANCED RAG WORKFLOW...\")\n",
        "        final_state = workflow.invoke(initial_state)\n",
        "\n",
        "        # Display enhanced results\n",
        "        print(\"\\n📊 ENHANCED FINAL RESULTS:\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        task_results = final_state.get(\"task_results\", {})\n",
        "\n",
        "        for filename, results in task_results.items():\n",
        "            print(f\"\\n📄 DOCUMENT: {filename}\")\n",
        "            print(\"─\" * 50)\n",
        "\n",
        "            if \"error\" in results:\n",
        "                print(f\"❌ Error: {results['error']}\")\n",
        "                continue\n",
        "\n",
        "            for task_name, task_result in results.items():\n",
        "                if isinstance(task_result, dict):\n",
        "                    response = task_result.get(\"response\", \"No response\")\n",
        "                    # confidence = task_result.get(\"confidence\", 0)\n",
        "                    chunks_used = task_result.get(\"context_chunks_used\", 0)\n",
        "                    token_usage = task_result.get(\"token_usage\", {})\n",
        "\n",
        "                    print(f\"\\n🎯 {task_name.upper()}:\")\n",
        "                    print(f\"   📝 Response: {response[:150]}...\")\n",
        "                    # print(f\"   🎯 Confidence: {confidence:.3f}\")\n",
        "                    print(f\"   📚 Chunks used: {chunks_used}\")\n",
        "                    if token_usage:\n",
        "                        print(f\"   🔢 Tokens: {token_usage.get('total_tokens', 0)}\")\n",
        "\n",
        "        # Display vector store statistics\n",
        "        print(\"\\n📊 ENHANCED VECTOR STORE STATISTICS:\")\n",
        "        print(\"=\"*50)\n",
        "        stats = processor.vector_store.get_collection_stats()\n",
        "\n",
        "        # Final token usage summary\n",
        "        token_tracker.print_summary()\n",
        "\n",
        "    else:\n",
        "        print(\"📂 No sample documents found for demonstration\")\n",
        "\n",
        "else:\n",
        "    print(\"📂 No input directory found\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-06T07:08:00.999606Z",
          "iopub.execute_input": "2025-09-06T07:08:00.999895Z",
          "iopub.status.idle": "2025-09-06T07:11:12.025273Z",
          "shell.execute_reply.started": "2025-09-06T07:08:00.999876Z",
          "shell.execute_reply": "2025-09-06T07:11:12.024399Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "an6qteRtV9rP",
        "outputId": "dc2eccd9-b87a-4e37-933c-e1552bde316b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "🚀 ENHANCED RAG DEMONSTRATION STARTING\n",
            "📊 Features: Filename IDs + Token Tracking + Detailed Progress\n",
            "======================================================================\n",
            "🚀 INITIALIZING RAG EXPENSE PROCESSOR\n",
            "   🤖 Text Model: gemma3:1b\n",
            "🗄️ INITIALIZING VECTOR STORE\n",
            "   🤖 Embedding Model: nomic-embed-text\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2965149239.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Initialize enhanced system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnhancedRAGExpenseProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Check for sample documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2992683130.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, text_model)\u001b[0m\n\u001b[1;32m     18\u001b[0m         )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnhancedIsolatedVectorStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnhancedExpenseTaskManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFilenameBasedDocumentManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1421296717.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embedding_model)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Initialize ChromaDB client using new API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPersistentClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./chroma_db\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Create collection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/__init__.py\u001b[0m in \u001b[0;36mPersistentClient\u001b[0;34m(path, settings, tenant, database)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mdatabase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mClientCreator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtenant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtenant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tenant, database, settings)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0msettings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSettings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSettings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     ) -> None:\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtenant\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtenant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtenant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/shared_system_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, settings)\u001b[0m\n\u001b[1;32m     17\u001b[0m     ) -> None:\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_identifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSharedSystemClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_identifier_from_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mSharedSystemClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_system_if_not_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_identifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/shared_system_client.py\u001b[0m in \u001b[0;36m_create_system_if_not_exists\u001b[0;34m(cls, identifier, settings)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mnew_system\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mServerAPI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mnew_system\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mprevious_system\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_identifier_to_system\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/config.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcomponent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m             \u001b[0mcomponent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/rust.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         )\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         self.bindings = chromadb_rust_bindings.Bindings(\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mallow_reset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allow_reset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0msqlite_db_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqlite_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# STEP 10: ENHANCED INTERACTIVE FUNCTIONS\n",
        "# ================================\n",
        "\n",
        "def demo_document_isolation():\n",
        "    \"\"\"Enhanced demonstration of document isolation\"\"\"\n",
        "    print(\"\\n🧪 ENHANCED DOCUMENT ISOLATION DEMONSTRATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    stats = processor.vector_store.get_collection_stats()\n",
        "    filenames = stats['filenames']\n",
        "\n",
        "    if len(filenames) >= 2:\n",
        "        filename_1, filename_2 = filenames[0], filenames[1]\n",
        "\n",
        "        test_query = \"total amount\"\n",
        "        print(f\"🔍 Testing query '{test_query}' across isolated documents:\")\n",
        "\n",
        "        # Query document 1\n",
        "        print(f\"\\n📄 RESULTS FOR: {filename_1}\")\n",
        "        result_1 = processor.vector_store.query_document_specific(test_query, filename_1)\n",
        "\n",
        "        # Query document 2\n",
        "        print(f\"\\n📄 RESULTS FOR: {filename_2}\")\n",
        "        result_2 = processor.vector_store.query_document_specific(test_query, filename_2)\n",
        "\n",
        "        print(f\"\\n✅ ISOLATION VERIFIED: Each query only returns chunks from its specific document\")\n",
        "\n",
        "    else:\n",
        "        print(\"⚠️ Need at least 2 documents to demonstrate isolation\")\n",
        "\n",
        "def query_specific_document(filename: str, task: str):\n",
        "    \"\"\"Query a specific document with enhanced tracking\"\"\"\n",
        "\n",
        "    print(f\"\\n❓ QUERYING DOCUMENT: {filename}\")\n",
        "    print(f\"🎯 TASK: {task}\")\n",
        "    print(\"─\" * 50)\n",
        "\n",
        "    result = processor.process_expense_task(filename, task)\n",
        "\n",
        "    print(f\"\\n📝 RESPONSE: {result.get('response', 'No response')}\")\n",
        "    # print(f\"🎯 CONFIDENCE: {result.get('confidence', 0):.3f}\")\n",
        "    print(f\"📚 CHUNKS USED: {result.get('context_chunks_used', 0)}\")\n",
        "\n",
        "    token_usage = result.get('token_usage', {})\n",
        "    if token_usage:\n",
        "        print(f\"🔢 TOKEN USAGE:\")\n",
        "        print(f\"   📥 Input: {token_usage.get('input_tokens', 0)}\")\n",
        "        print(f\"   📤 Output: {token_usage.get('output_tokens', 0)}\")\n",
        "        print(f\"   🎯 Total: {token_usage.get('total_tokens', 0)}\")\n",
        "\n",
        "print(\"\\n🎯 ENHANCED RAG SYSTEM READY!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "✅ ENHANCED SYSTEM COMPONENTS:\n",
        "- 📄 Filename-based Document Management\n",
        "- 🔍 Enhanced OCR Processor (detailed progress tracking)\n",
        "- 🗄️ Enhanced Vector Store (ChromaDB + isolation)\n",
        "- 🤖 Token Usage Tracker (comprehensive monitoring)\n",
        "- 🎯 Enhanced RAG Processor (detailed inference tracking)\n",
        "- 📊 Enhanced LangGraph Workflow (step-by-step progress)\n",
        "\n",
        "📋 DEMONSTRATION COMPLETED:\n",
        "- ✅ Document ingestion with filename-based IDs\n",
        "- ✅ Task-based information extraction with progress tracking\n",
        "- ✅ Context optimization with detailed metrics\n",
        "- ✅ Cross-document contamination prevention verified\n",
        "- ✅ Token usage monitoring for all LLM calls\n",
        "\n",
        "🎯 CONTEXT OPTIMIZATION FEATURES:\n",
        "- ✅ Chunk size optimization (500 chars max per chunk)\n",
        "- ✅ Relevant chunk filtering (top 3 per query with relevance scores)\n",
        "- ✅ Context length limits (1500 chars max to prevent overload)\n",
        "- ✅ Task-specific query optimization (tailored search terms)\n",
        "- ✅ Token usage tracking (input/output/total for every LLM call)\n",
        "\"\"\")\n",
        "\n",
        "# Show available documents for interaction\n",
        "stats = processor.vector_store.get_collection_stats()\n",
        "if stats['filenames']:\n",
        "    print(f\"\\n📄 AVAILABLE DOCUMENTS FOR QUERYING:\")\n",
        "    for i, filename in enumerate(stats['filenames'], 1):\n",
        "        print(f\"   {i}. {filename}\")\n",
        "\n",
        "    # Demo enhanced isolation\n",
        "    demo_document_isolation()\n",
        "\n",
        "    # Show available tasks\n",
        "    tasks = processor.task_manager.list_available_tasks()\n",
        "    print(f\"\\n🎯 AVAILABLE TASKS:\")\n",
        "    for i, task in enumerate(tasks, 1):\n",
        "        print(f\"   {i}. {task}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n⚠️ No documents available. Process some documents first.\")\n",
        "\n",
        "print(f\"\\n🔢 FINAL TOKEN USAGE SUMMARY:\")\n",
        "token_tracker.print_summary()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-06T07:11:45.650729Z",
          "iopub.execute_input": "2025-09-06T07:11:45.651070Z",
          "iopub.status.idle": "2025-09-06T07:11:45.718031Z",
          "shell.execute_reply.started": "2025-09-06T07:11:45.651049Z",
          "shell.execute_reply": "2025-09-06T07:11:45.717423Z"
        },
        "id": "62lea24wV9rQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index"
      ],
      "metadata": {
        "trusted": true,
        "id": "Vhsuupw8V9rR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "xqGMLp2wV9rR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a97975b"
      },
      "source": [
        "# Task\n",
        "Explain the provided Python code and add a front end using a web framework (Flask, Streamlit, or Gradio) to accept expense claim documents as file uploads, process them using the existing RAG workflow, and display the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da06a515"
      },
      "source": [
        "## Choose a web framework\n",
        "\n",
        "### Subtask:\n",
        "Choose a suitable Python web framework (e.g., Flask, Streamlit, or Gradio) to build the front end.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c9b62c5"
      },
      "source": [
        "## Design the user interface\n",
        "\n",
        "### Subtask:\n",
        "Create a simple web page with a file upload form for users to submit their expense claim documents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7edbc873"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a Gradio interface with file upload and output components as requested by the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96792237"
      },
      "source": [
        "import gradio as gr\n",
        "\n",
        "def process_document_ui(file):\n",
        "    \"\"\"Placeholder function for processing the uploaded file.\"\"\"\n",
        "    if file is None:\n",
        "        return \"Please upload a file.\", \"\", \"\"\n",
        "\n",
        "    # Simulate processing\n",
        "    filename = file.name\n",
        "    status = f\"Processing file: {filename}\"\n",
        "    extracted_details = f\"Simulated details for {filename}\"\n",
        "\n",
        "    return status, extracted_details, \"\"\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Enhanced RAG-Based Expense Claims Processing\")\n",
        "    gr.Markdown(\"Upload your expense claim document to extract details.\")\n",
        "\n",
        "    file_input = gr.File(label=\"Upload Expense Document\")\n",
        "    process_button = gr.Button(\"Process Document\")\n",
        "\n",
        "    status_output = gr.Textbox(label=\"Processing Status\")\n",
        "    details_output = gr.Textbox(label=\"Extracted Details\")\n",
        "    # Add a placeholder for potential future output like a dataframe or summary\n",
        "    summary_output = gr.Dataframe(label=\"Summary\", visible=False)\n",
        "\n",
        "    process_button.click(\n",
        "        process_document_ui,\n",
        "        inputs=file_input,\n",
        "        outputs=[status_output, details_output, summary_output]\n",
        "    )\n",
        "\n",
        "demo.launch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "941f20e6"
      },
      "source": [
        "## Implement file upload handling\n",
        "\n",
        "### Subtask:\n",
        "Write code to receive the uploaded files on the server side and save them temporarily.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "266535f1"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the `process_document_ui` function to access the temporary file path provided by Gradio when a file is uploaded and return a status confirming receipt of the file path.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b97f26f9"
      },
      "source": [
        "import gradio as gr\n",
        "\n",
        "def process_document_ui(file):\n",
        "    \"\"\"Function to receive the uploaded file and get its temporary path.\"\"\"\n",
        "    if file is None:\n",
        "        return \"Please upload a file.\", \"\", \"\"\n",
        "\n",
        "    # Access the temporary file path provided by Gradio\n",
        "    file_path = file.name\n",
        "\n",
        "    # Confirm receipt of the file path\n",
        "    status = f\"Received file: {file_path}. Ready for processing.\"\n",
        "    extracted_details = \"\" # Keep as placeholder for now\n",
        "    summary = \"\" # Keep as placeholder for now\n",
        "\n",
        "    return status, extracted_details, summary\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Enhanced RAG-Based Expense Claims Processing\")\n",
        "    gr.Markdown(\"Upload your expense claim document to extract details.\")\n",
        "\n",
        "    file_input = gr.File(label=\"Upload Expense Document\")\n",
        "    process_button = gr.Button(\"Process Document\")\n",
        "\n",
        "    status_output = gr.Textbox(label=\"Processing Status\")\n",
        "    details_output = gr.Textbox(label=\"Extracted Details\")\n",
        "    # Add a placeholder for potential future output like a dataframe or summary\n",
        "    summary_output = gr.Dataframe(label=\"Summary\", visible=False)\n",
        "\n",
        "    process_button.click(\n",
        "        process_document_ui,\n",
        "        inputs=file_input,\n",
        "        outputs=[status_output, details_output, summary_output]\n",
        "    )\n",
        "\n",
        "demo.launch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea591ad7"
      },
      "source": [
        "## Integrate with the rag workflow\n",
        "\n",
        "### Subtask:\n",
        "Modify the existing RAG workflow to accept file paths from the front end as input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9de600e9"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to modify the existing RAG workflow to accept file paths from the front end. This involves updating the `create_enhanced_rag_workflow` function and specifically the `enhanced_ingestion_node` to take a list of file paths as input, rather than relying on a hardcoded directory. I will also update the `process_document_ui` function in the Gradio app to pass the temporary file path to the workflow.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20ad6183"
      },
      "source": [
        "from langgraph.graph import StateGraph\n",
        "from typing import TypedDict, List, Dict, Any, Optional, Tuple\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import json\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "\n",
        "# Assume EnhancedRAGExpenseProcessor, TokenUsageTracker, EnhancedExpenseTaskManager,\n",
        "# FilenameBasedDocumentManager, EnhancedOCRProcessor, EnhancedIsolatedVectorStore\n",
        "# are defined in previous cells and available in the environment.\n",
        "# Also assume ollama is imported and configured.\n",
        "\n",
        "class EnhancedRAGWorkflowState(TypedDict):\n",
        "    \"\"\"Enhanced state for RAG workflow\"\"\"\n",
        "    file_paths: List[str] # Modified to accept a list of file paths\n",
        "    current_file_index: int\n",
        "    processed_filenames: List[str]\n",
        "    current_filename: str\n",
        "    task_results: Dict[str, Dict[str, Any]]\n",
        "    workflow_status: str\n",
        "    error: Optional[str]\n",
        "\n",
        "def create_enhanced_rag_workflow(processor: EnhancedRAGExpenseProcessor) -> StateGraph:\n",
        "    \"\"\"Create enhanced LangGraph workflow for RAG processing\"\"\"\n",
        "\n",
        "    def enhanced_ingestion_node(state: EnhancedRAGWorkflowState) -> EnhancedRAGWorkflowState:\n",
        "        \"\"\"Enhanced ingestion with detailed tracking\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"🔄 WORKFLOW: ENHANCED INGESTION PHASE STARTING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Get file paths from the state\n",
        "        file_paths = state.get(\"file_paths\", [])\n",
        "        processed_filenames = []\n",
        "\n",
        "        if not file_paths:\n",
        "            state[\"workflow_status\"] = \"ingestion_failed\"\n",
        "            state[\"error\"] = \"No file paths provided for ingestion.\"\n",
        "            print(\"❌ INGESTION FAILED: No file paths provided.\")\n",
        "            return state\n",
        "\n",
        "\n",
        "        for i, file_path in enumerate(file_paths, 1):\n",
        "            print(f\"\\n[{i}/{len(file_paths)}] Processing file: {Path(file_path).name}\")\n",
        "\n",
        "            try:\n",
        "                # Use the processor to ingest the document\n",
        "                filename = processor.ingest_document(file_path)\n",
        "                if filename:\n",
        "                    processed_filenames.append(filename)\n",
        "                    print(f\"✅ Successfully ingested: {filename}\")\n",
        "                else:\n",
        "                    print(f\"❌ Failed to ingest: {Path(file_path).name}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error ingesting {Path(file_path).name}: {e}\")\n",
        "                state[\"error\"] = str(e) # Store the error in state\n",
        "\n",
        "        state[\"processed_filenames\"] = processed_filenames\n",
        "        state[\"workflow_status\"] = \"ingestion_complete\" if processed_filenames else \"ingestion_failed\"\n",
        "\n",
        "        print(f\"\\n📊 INGESTION PHASE COMPLETED\")\n",
        "        print(f\"   ✅ Successfully processed: {len(processed_filenames)} files\")\n",
        "        print(f\"   ❌ Failed: {len(file_paths) - len(processed_filenames)} files\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    def enhanced_task_processing_node(state: EnhancedRAGWorkflowState) -> EnhancedRAGWorkflowState:\n",
        "        \"\"\"Enhanced task processing with detailed tracking\"\"\"\n",
        "\n",
        "        print(\"\\n🎯 WORKFLOW: ENHANCED TASK PROCESSING PHASE STARTING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        processed_filenames = state.get(\"processed_filenames\", [])\n",
        "        task_results = state.get(\"task_results\", {}) # Initialize or get existing results\n",
        "\n",
        "        if not processed_filenames:\n",
        "            state[\"workflow_status\"] = \"processing_skipped\"\n",
        "            print(\"⚠️ TASK PROCESSING SKIPPED: No documents successfully ingested.\")\n",
        "            return state\n",
        "\n",
        "\n",
        "        for i, filename in enumerate(processed_filenames, 1):\n",
        "            print(f\"\\n[{i}/{len(processed_filenames)}] Processing tasks for: {filename}\")\n",
        "\n",
        "            try:\n",
        "                results = processor.process_all_tasks_for_document(filename)\n",
        "                task_results[filename] = results\n",
        "                print(f\"✅ Completed all tasks for: {filename}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing tasks for {filename}: {e}\")\n",
        "                task_results[filename] = {\"error\": str(e)}\n",
        "                state[\"error\"] = str(e) # Store the error in state\n",
        "\n",
        "\n",
        "        state[\"task_results\"] = task_results\n",
        "        state[\"workflow_status\"] = \"processing_complete\"\n",
        "\n",
        "        print(f\"\\n📊 TASK PROCESSING PHASE COMPLETED\")\n",
        "        print(f\"   📄 Documents processed: {len(task_results)}\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    def enhanced_results_compilation_node(state: EnhancedRAGWorkflowState) -> EnhancedRAGWorkflowState:\n",
        "        \"\"\"Enhanced results compilation with detailed stats and CSV export\"\"\"\n",
        "\n",
        "        print(\"\\n📊 WORKFLOW: ENHANCED RESULTS COMPILATION STARTING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        task_results = state.get(\"task_results\", {})\n",
        "        # token_tracker is a global instance assumed to be available\n",
        "        # processor instance (and its vector_store) is also assumed to be available\n",
        "\n",
        "        if not task_results:\n",
        "             state[\"workflow_status\"] = \"compilation_skipped\"\n",
        "             print(\"⚠️ RESULTS COMPILATION SKIPPED: No task results to compile.\")\n",
        "             token_tracker.print_summary() # Print summary even if compilation skipped\n",
        "             return state\n",
        "\n",
        "        # Compile detailed statistics\n",
        "        total_documents = len(task_results)\n",
        "        successful_documents = sum(1 for r in task_results.values() if \"error\" not in r)\n",
        "\n",
        "        # Save results with timestamp\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        results_file = f\"enhanced_rag_expense_results_{timestamp}.json\"\n",
        "        csv_file = f\"enhanced_rag_expense_results_{timestamp}.csv\"\n",
        "\n",
        "        # Create comprehensive results package\n",
        "        comprehensive_results = {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"summary\": {\n",
        "                \"total_documents\": total_documents,\n",
        "                \"successful_documents\": successful_documents,\n",
        "                \"failed_documents\": total_documents - successful_documents\n",
        "            },\n",
        "            \"token_usage_summary\": {\n",
        "                \"total_calls\": token_tracker.call_count,\n",
        "                \"total_input_tokens\": token_tracker.total_input_tokens,\n",
        "                \"total_output_tokens\": token_tracker.total_output_tokens,\n",
        "                \"total_tokens\": token_tracker.total_tokens\n",
        "            },\n",
        "            \"document_results\": task_results,\n",
        "            \"token_call_history\": token_tracker.call_history\n",
        "        }\n",
        "\n",
        "        # Save JSON results\n",
        "        try:\n",
        "            with open(results_file, 'w') as f:\n",
        "                json.dump(comprehensive_results, f, indent=2, default=str)\n",
        "            print(f\"💾 JSON RESULTS SAVED TO: {results_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error saving JSON results: {e}\")\n",
        "            state[\"error\"] = f\"Error saving JSON results: {e}\"\n",
        "\n",
        "\n",
        "        # Create CSV from results\n",
        "        csv_rows = []\n",
        "\n",
        "        for filename, doc_results in task_results.items():\n",
        "            if \"error\" in doc_results:\n",
        "                # Add error row\n",
        "                csv_rows.append({\n",
        "                    \"filename\": filename,\n",
        "                    \"task\": \"workflow_error\", # Indicate workflow level error for this document\n",
        "                    \"response\": doc_results[\"error\"],\n",
        "                    \"context_chunks_used\": 0,\n",
        "                    \"input_tokens\": 0,\n",
        "                    \"output_tokens\": 0,\n",
        "                    \"total_tokens\": 0\n",
        "                })\n",
        "            else:\n",
        "                # Process each task for this document\n",
        "                for task_name, task_result in doc_results.items():\n",
        "                    if isinstance(task_result, dict):\n",
        "                        token_usage = task_result.get(\"token_usage\", {})\n",
        "                        csv_rows.append({\n",
        "                            \"filename\": filename,\n",
        "                            \"task\": task_name,\n",
        "                            \"response\": task_result.get(\"response\", \"\"),\n",
        "                            \"context_chunks_used\": task_result.get(\"context_chunks_used\", 0),\n",
        "                            \"input_tokens\": token_usage.get(\"input_tokens\", 0),\n",
        "                            \"output_tokens\": token_usage.get(\"output_tokens\", 0),\n",
        "                            \"total_tokens\": token_usage.get(\"total_tokens\", 0)\n",
        "                        })\n",
        "                    else:\n",
        "                         # Handle task-specific errors\n",
        "                        csv_rows.append({\n",
        "                            \"filename\": filename,\n",
        "                            \"task\": task_name,\n",
        "                            \"response\": f\"Task error: {task_result}\",\n",
        "                            \"context_chunks_used\": 0,\n",
        "                            \"input_tokens\": 0,\n",
        "                            \"output_tokens\": 0,\n",
        "                            \"total_tokens\": 0\n",
        "                        })\n",
        "\n",
        "\n",
        "        # Save CSV\n",
        "        if csv_rows:\n",
        "            try:\n",
        "                df = pd.DataFrame(csv_rows)\n",
        "\n",
        "                # Reorder columns for better readability - handle missing columns gracefully\n",
        "                column_order = [\n",
        "                    \"filename\", \"task\", \"response\",\n",
        "                    \"context_chunks_used\", \"input_tokens\",\n",
        "                    \"output_tokens\", \"total_tokens\"\n",
        "                ]\n",
        "                existing_columns = [col for col in column_order if col in df.columns]\n",
        "                df = df[existing_columns]\n",
        "\n",
        "\n",
        "                # Save to CSV\n",
        "                df.to_csv(csv_file, index=False, encoding='utf-8')\n",
        "                print(f\"💾 CSV RESULTS SAVED TO: {csv_file}\")\n",
        "\n",
        "                # Display summary statistics from CSV\n",
        "                print(f\"\\n📊 CSV Summary:\")\n",
        "                print(f\"   📄 Total rows: {len(df)}\")\n",
        "                # Ensure 'filename' column exists before calling nunique\n",
        "                if 'filename' in df.columns:\n",
        "                    print(f\"   📁 Documents: {df['filename'].nunique()}\")\n",
        "                     # Handle case where no tasks were processed successfully\n",
        "                    if not df[df['task'] != 'workflow_error'].empty and 'filename' in df.columns:\n",
        "                         print(f\"   🎯 Tasks per document: {df[df['task'] != 'workflow_error'].groupby('filename').size().mean():.1f}\")\n",
        "                    else:\n",
        "                        print(\"   🎯 Tasks per document: N/A (No successful tasks)\")\n",
        "\n",
        "                # Ensure 'total_tokens' column exists and is numeric before summing\n",
        "                if 'total_tokens' in df.columns:\n",
        "                    try:\n",
        "                        df['total_tokens'] = pd.to_numeric(df['total_tokens'], errors='coerce').fillna(0)\n",
        "                        print(f\"   🔢 Total tokens used: {df['total_tokens'].sum():,}\")\n",
        "                    except Exception as e:\n",
        "                         print(f\"⚠️ Could not calculate total tokens from CSV: {e}\")\n",
        "                else:\n",
        "                    print(\"⚠️ 'total_tokens' column not found in CSV.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing or saving CSV results: {e}\")\n",
        "                state[\"error\"] = f\"Error processing or saving CSV results: {e}\"\n",
        "\n",
        "\n",
        "        # Also save a summary CSV with aggregated data per document\n",
        "        summary_csv_file = f\"enhanced_rag_expense_summary_{timestamp}.csv\"\n",
        "        summary_rows = []\n",
        "\n",
        "        for filename, doc_results in task_results.items():\n",
        "            if \"error\" not in doc_results:\n",
        "                row = {\"filename\": filename}\n",
        "\n",
        "                # Extract key information from each task\n",
        "                for task_name in [\"extract_amount\", \"extract_date\", \"extract_vendor\",\n",
        "                                \"extract_category\", \"extract_tax\"]:\n",
        "                    if task_name in doc_results and isinstance(doc_results[task_name], dict):\n",
        "                        response = doc_results[task_name].get(\"response\", \"\")\n",
        "                        # Clean the response (take first line or first 100 chars)\n",
        "                        cleaned = response.split('\\n')[0][:100] if response else \"\"\n",
        "                        row[task_name] = cleaned\n",
        "                    else:\n",
        "                        row[task_name] = f\"Task {task_name} failed or not found\"\n",
        "\n",
        "\n",
        "                # Add token totals\n",
        "                total_tokens = sum(\n",
        "                    doc_results.get(task, {}).get(\"token_usage\", {}).get(\"total_tokens\", 0)\n",
        "                    for task in doc_results if isinstance(doc_results.get(task), dict)\n",
        "                )\n",
        "                row[\"total_tokens_used\"] = total_tokens\n",
        "\n",
        "                summary_rows.append(row)\n",
        "            else:\n",
        "                 summary_rows.append({\"filename\": filename, \"workflow_error\": doc_results[\"error\"]}) # Add error to summary\n",
        "\n",
        "\n",
        "        if summary_rows:\n",
        "            try:\n",
        "                summary_df = pd.DataFrame(summary_rows)\n",
        "                summary_df.to_csv(summary_csv_file, index=False, encoding='utf-8')\n",
        "                print(f\"💾 SUMMARY CSV SAVED TO: {summary_csv_file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error saving summary CSV: {e}\")\n",
        "                state[\"error\"] = f\"Error saving summary CSV: {e}\"\n",
        "\n",
        "\n",
        "        print(f\"\\n📊 FILES SAVED:\")\n",
        "        print(f\"   📄 Detailed JSON: {results_file}\")\n",
        "        print(f\"   📄 Detailed CSV: {csv_file}\")\n",
        "        print(f\"   📄 Summary CSV: {summary_csv_file}\")\n",
        "        print(f\"   📄 Documents processed: {successful_documents}/{total_documents}\")\n",
        "\n",
        "        # Print token usage summary\n",
        "        token_tracker.print_summary()\n",
        "\n",
        "        state[\"workflow_status\"] = \"complete\"\n",
        "        return state\n",
        "\n",
        "\n",
        "    # Build enhanced workflow\n",
        "    workflow = StateGraph(EnhancedRAGWorkflowState)\n",
        "\n",
        "    workflow.add_node(\"enhanced_ingestion\", enhanced_ingestion_node)\n",
        "    workflow.add_node(\"enhanced_task_processing\", enhanced_task_processing_node)\n",
        "    workflow.add_node(\"enhanced_results_compilation\", enhanced_results_compilation_node)\n",
        "\n",
        "    workflow.set_entry_point(\"enhanced_ingestion\")\n",
        "    workflow.add_edge(\"enhanced_ingestion\", \"enhanced_task_processing\")\n",
        "    workflow.add_edge(\"enhanced_task_processing\", \"enhanced_results_compilation\")\n",
        "    workflow.set_finish_point(\"enhanced_results_compilation\")\n",
        "\n",
        "    return workflow.compile()\n",
        "\n",
        "# Update the Gradio UI function to call the workflow\n",
        "def process_document_ui(file):\n",
        "    \"\"\"Function to receive the uploaded file, run the workflow, and display results.\"\"\"\n",
        "    if file is None:\n",
        "        return \"Please upload a file.\", \"No file uploaded.\", pd.DataFrame() # Return empty DataFrame\n",
        "\n",
        "    file_path = file.name # Gradio provides the temporary path here\n",
        "\n",
        "    status = f\"Received file: {file_path}. Starting RAG workflow...\"\n",
        "    print(status) # Print status to console\n",
        "\n",
        "    # Initialize the RAG processor and workflow\n",
        "    # Assuming processor and token_tracker are accessible globally or initialized here\n",
        "    try:\n",
        "        processor = EnhancedRAGExpenseProcessor()\n",
        "        workflow = create_enhanced_rag_workflow(processor)\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error initializing RAG components: {e}\"\n",
        "        print(f\"❌ {error_msg}\")\n",
        "        return error_msg, \"Initialization failed.\", pd.DataFrame()\n",
        "\n",
        "\n",
        "    # Execute enhanced workflow with the uploaded file path\n",
        "    initial_state = {\n",
        "        \"file_paths\": [file_path], # Pass the uploaded file path as a list\n",
        "        \"current_file_index\": 0,\n",
        "        \"processed_filenames\": [],\n",
        "        \"current_filename\": \"\",\n",
        "        \"task_results\": {},\n",
        "        \"workflow_status\": \"initialized\",\n",
        "        \"error\": None\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        final_state = workflow.invoke(initial_state)\n",
        "        status = f\"Workflow completed with status: {final_state.get('workflow_status')}\"\n",
        "        print(status) # Print final status\n",
        "\n",
        "        # Process final state to display results\n",
        "        task_results = final_state.get(\"task_results\", {})\n",
        "        if final_state.get(\"error\"):\n",
        "             details = f\"Workflow Error: {final_state['error']}\"\n",
        "             # Attempt to display any partial results if available\n",
        "             if task_results:\n",
        "                 details += \"\\n\\nPartial Results:\"\n",
        "                 # Create a simple string representation of partial results\n",
        "                 for filename, doc_results in task_results.items():\n",
        "                      details += f\"\\nDocument: {filename}\"\n",
        "                      if \"error\" in doc_results:\n",
        "                           details += f\"\\n  Error: {doc_results['error']}\"\n",
        "                      else:\n",
        "                          for task_name, task_result in doc_results.items():\n",
        "                              if isinstance(task_result, dict):\n",
        "                                   details += f\"\\n  {task_name}: {task_result.get('response', 'No response')[:100]}...\"\n",
        "                              else:\n",
        "                                  details += f\"\\n  {task_name}: Error - {task_result}\"\n",
        "\n",
        "             return status, details, pd.DataFrame() # Return empty dataframe on error\n",
        "\n",
        "        # Compile results for display\n",
        "        if task_results:\n",
        "            compiled_details = \"\"\n",
        "            csv_rows = [] # Prepare data for DataFrame display\n",
        "\n",
        "            for filename, results in task_results.items():\n",
        "                 compiled_details += f\"\\n📄 DOCUMENT: {filename}\\n\" + \"─\" * 50 + \"\\n\"\n",
        "                 if \"error\" in results:\n",
        "                     compiled_details += f\"❌ Error: {results['error']}\\n\"\n",
        "                      # Add error row to CSV data\n",
        "                     csv_rows.append({\n",
        "                        \"filename\": filename,\n",
        "                        \"task\": \"workflow_error\",\n",
        "                        \"response\": results[\"error\"],\n",
        "                        \"context_chunks_used\": 0,\n",
        "                        \"input_tokens\": 0,\n",
        "                        \"output_tokens\": 0,\n",
        "                        \"total_tokens\": 0\n",
        "                    })\n",
        "                 else:\n",
        "                    for task_name, task_result in results.items():\n",
        "                        if isinstance(task_result, dict):\n",
        "                            response = task_result.get(\"response\", \"No response\")\n",
        "                            chunks_used = task_result.get(\"context_chunks_used\", 0)\n",
        "                            token_usage = task_result.get(\"token_usage\", {})\n",
        "\n",
        "                            compiled_details += f\"\\n🎯 {task_name.upper()}:\\n\"\n",
        "                            compiled_details += f\"   📝 Response: {response[:200]}...\\n\" # Limit display length\n",
        "                            compiled_details += f\"   📚 Chunks used: {chunks_used}\\n\"\n",
        "                            if token_usage:\n",
        "                                compiled_details += f\"   🔢 Tokens: {token_usage.get('total_tokens', 0)}\\n\"\n",
        "\n",
        "                             # Add task result to CSV data\n",
        "                            csv_rows.append({\n",
        "                                \"filename\": filename,\n",
        "                                \"task\": task_name,\n",
        "                                \"response\": response,\n",
        "                                \"context_chunks_used\": chunks_used,\n",
        "                                \"input_tokens\": token_usage.get(\"input_tokens\", 0),\n",
        "                                \"output_tokens\": token_usage.get(\"output_tokens\", 0),\n",
        "                                \"total_tokens\": token_usage.get(\"total_tokens\", 0)\n",
        "                            })\n",
        "                        else:\n",
        "                            # Handle task-specific errors in display and CSV\n",
        "                             compiled_details += f\"\\n🎯 {task_name.upper()}:\\n\"\n",
        "                             compiled_details += f\"   ❌ Task Error: {task_result}\\n\"\n",
        "                             csv_rows.append({\n",
        "                                \"filename\": filename,\n",
        "                                \"task\": task_name,\n",
        "                                \"response\": f\"Task error: {task_result}\",\n",
        "                                \"context_chunks_used\": 0,\n",
        "                                \"input_tokens\": 0,\n",
        "                                \"output_tokens\": 0,\n",
        "                                \"total_tokens\": 0\n",
        "                            })\n",
        "\n",
        "\n",
        "            # Create DataFrame for the summary output\n",
        "            if csv_rows:\n",
        "                results_df = pd.DataFrame(csv_rows)\n",
        "                 # Ensure column order\n",
        "                column_order = [\n",
        "                    \"filename\", \"task\", \"response\",\n",
        "                    \"context_chunks_used\", \"input_tokens\",\n",
        "                    \"output_tokens\", \"total_tokens\"\n",
        "                ]\n",
        "                existing_columns = [col for col in column_order if col in results_df.columns]\n",
        "                results_df = results_df[existing_columns]\n",
        "\n",
        "                # Convert token columns to numeric, handling errors\n",
        "                for token_col in ['input_tokens', 'output_tokens', 'total_tokens']:\n",
        "                     if token_col in results_df.columns:\n",
        "                        results_df[token_col] = pd.to_numeric(results_df[token_col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "\n",
        "            else:\n",
        "                 results_df = pd.DataFrame({\"Status\": [\"No task results generated.\"]}) # Empty DataFrame or error indicator\n",
        "\n",
        "\n",
        "            return status, compiled_details, results_df # Return DataFrame for display\n",
        "\n",
        "        else:\n",
        "            return status, \"No task results generated.\", pd.DataFrame() # Return empty DataFrame if no task results\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Workflow execution failed: {e}\"\n",
        "        print(f\"❌ {error_msg}\")\n",
        "        return status, error_msg, pd.DataFrame() # Return empty DataFrame on workflow execution error\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Enhanced RAG-Based Expense Claims Processing\")\n",
        "    gr.Markdown(\"Upload your expense claim document (PDF, JPG, PNG, etc.) to extract details.\")\n",
        "\n",
        "    file_input = gr.File(label=\"Upload Expense Document\", file_types=[\".pdf\", \".jpg\", \".jpeg\", \".png\", \".tiff\"])\n",
        "    process_button = gr.Button(\"Process Document\")\n",
        "\n",
        "    status_output = gr.Textbox(label=\"Processing Status\", max_lines=5)\n",
        "    details_output = gr.Textbox(label=\"Extracted Details (Raw)\", interactive=False, lines=20)\n",
        "    summary_output = gr.Dataframe(label=\"Extracted Details (Structured)\") # Use Dataframe for structured output\n",
        "\n",
        "    process_button.click(\n",
        "        process_document_ui,\n",
        "        inputs=file_input,\n",
        "        outputs=[status_output, details_output, summary_output]\n",
        "    )\n",
        "\n",
        "# Note: demo.launch() will be called separately when running the Gradio app\n",
        "# For this subtask, we define the function but don't call launch() yet."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac7dd57a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed because the `langgraph` library was not found. I need to install this library before I can use `StateGraph`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tlTYbsSW1tA"
      },
      "source": [
        "!pip install langgraph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faf1dc59"
      },
      "source": [
        "**Reasoning**:\n",
        "I have successfully installed `langgraph`. Now I can re-run the code to modify the RAG workflow and the Gradio UI function to handle the uploaded file path.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c0a0b75"
      },
      "source": [
        "from langgraph.graph import StateGraph\n",
        "from typing import TypedDict, List, Dict, Any, Optional, Tuple\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import json\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "\n",
        "# Assume EnhancedRAGExpenseProcessor, TokenUsageTracker, EnhancedExpenseTaskManager,\n",
        "# FilenameBasedDocumentManager, EnhancedOCRProcessor, EnhancedIsolatedVectorStore\n",
        "# are defined in previous cells and available in the environment.\n",
        "# Also assume ollama is imported and configured.\n",
        "\n",
        "class EnhancedRAGWorkflowState(TypedDict):\n",
        "    \"\"\"Enhanced state for RAG workflow\"\"\"\n",
        "    file_paths: List[str] # Modified to accept a list of file paths\n",
        "    current_file_index: int\n",
        "    processed_filenames: List[str]\n",
        "    current_filename: str\n",
        "    task_results: Dict[str, Dict[str, Any]]\n",
        "    workflow_status: str\n",
        "    error: Optional[str]\n",
        "\n",
        "def create_enhanced_rag_workflow(processor: EnhancedRAGExpenseProcessor) -> StateGraph:\n",
        "    \"\"\"Create enhanced LangGraph workflow for RAG processing\"\"\"\n",
        "\n",
        "    def enhanced_ingestion_node(state: EnhancedRAGWorkflowState) -> EnhancedRAGWorkflowState:\n",
        "        \"\"\"Enhanced ingestion with detailed tracking\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"🔄 WORKFLOW: ENHANCED INGESTION PHASE STARTING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Get file paths from the state\n",
        "        file_paths = state.get(\"file_paths\", [])\n",
        "        processed_filenames = []\n",
        "\n",
        "        if not file_paths:\n",
        "            state[\"workflow_status\"] = \"ingestion_failed\"\n",
        "            state[\"error\"] = \"No file paths provided for ingestion.\"\n",
        "            print(\"❌ INGESTION FAILED: No file paths provided.\")\n",
        "            return state\n",
        "\n",
        "\n",
        "        for i, file_path in enumerate(file_paths, 1):\n",
        "            print(f\"\\n[{i}/{len(file_paths)}] Processing file: {Path(file_path).name}\")\n",
        "\n",
        "            try:\n",
        "                # Use the processor to ingest the document\n",
        "                filename = processor.ingest_document(file_path)\n",
        "                if filename:\n",
        "                    processed_filenames.append(filename)\n",
        "                    print(f\"✅ Successfully ingested: {filename}\")\n",
        "                else:\n",
        "                    print(f\"❌ Failed to ingest: {Path(file_path).name}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error ingesting {Path(file_path).name}: {e}\")\n",
        "                state[\"error\"] = str(e) # Store the error in state\n",
        "\n",
        "        state[\"processed_filenames\"] = processed_filenames\n",
        "        state[\"workflow_status\"] = \"ingestion_complete\" if processed_filenames else \"ingestion_failed\"\n",
        "\n",
        "        print(f\"\\n📊 INGESTION PHASE COMPLETED\")\n",
        "        print(f\"   ✅ Successfully processed: {len(processed_filenames)} files\")\n",
        "        print(f\"   ❌ Failed: {len(file_paths) - len(processed_filenames)} files\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    def enhanced_task_processing_node(state: EnhancedRAGWorkflowState) -> EnhancedRAGWorkflowState:\n",
        "        \"\"\"Enhanced task processing with detailed tracking\"\"\"\n",
        "\n",
        "        print(\"\\n🎯 WORKFLOW: ENHANCED TASK PROCESSING PHASE STARTING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        processed_filenames = state.get(\"processed_filenames\", [])\n",
        "        task_results = state.get(\"task_results\", {}) # Initialize or get existing results\n",
        "\n",
        "        if not processed_filenames:\n",
        "            state[\"workflow_status\"] = \"processing_skipped\"\n",
        "            print(\"⚠️ TASK PROCESSING SKIPPED: No documents successfully ingested.\")\n",
        "            return state\n",
        "\n",
        "\n",
        "        for i, filename in enumerate(processed_filenames, 1):\n",
        "            print(f\"\\n[{i}/{len(processed_filenames)}] Processing tasks for: {filename}\")\n",
        "\n",
        "            try:\n",
        "                results = processor.process_all_tasks_for_document(filename)\n",
        "                task_results[filename] = results\n",
        "                print(f\"✅ Completed all tasks for: {filename}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing tasks for {filename}: {e}\")\n",
        "                task_results[filename] = {\"error\": str(e)}\n",
        "                state[\"error\"] = str(e) # Store the error in state\n",
        "\n",
        "\n",
        "        state[\"task_results\"] = task_results\n",
        "        state[\"workflow_status\"] = \"processing_complete\"\n",
        "\n",
        "        print(f\"\\n📊 TASK PROCESSING PHASE COMPLETED\")\n",
        "        print(f\"   📄 Documents processed: {len(task_results)}\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    def enhanced_results_compilation_node(state: EnhancedRAGWorkflowState) -> EnhancedRAGWorkflowState:\n",
        "        \"\"\"Enhanced results compilation with detailed stats and CSV export\"\"\"\n",
        "\n",
        "        print(\"\\n📊 WORKFLOW: ENHANCED RESULTS COMPILATION STARTING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        task_results = state.get(\"task_results\", {})\n",
        "        # token_tracker is a global instance assumed to be available\n",
        "        # processor instance (and its vector_store) is also assumed to be available\n",
        "\n",
        "        if not task_results:\n",
        "             state[\"workflow_status\"] = \"compilation_skipped\"\n",
        "             print(\"⚠️ RESULTS COMPILATION SKIPPED: No task results to compile.\")\n",
        "             token_tracker.print_summary() # Print summary even if compilation skipped\n",
        "             return state\n",
        "\n",
        "\n",
        "        # Compile detailed statistics\n",
        "        total_documents = len(task_results)\n",
        "        successful_documents = sum(1 for r in task_results.values() if \"error\" not in r)\n",
        "\n",
        "        # Save results with timestamp\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        results_file = f\"enhanced_rag_expense_results_{timestamp}.json\"\n",
        "        csv_file = f\"enhanced_rag_expense_results_{timestamp}.csv\"\n",
        "\n",
        "        # Create comprehensive results package\n",
        "        comprehensive_results = {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"summary\": {\n",
        "                \"total_documents\": total_documents,\n",
        "                \"successful_documents\": successful_documents,\n",
        "                \"failed_documents\": total_documents - successful_documents\n",
        "            },\n",
        "            \"token_usage_summary\": {\n",
        "                \"total_calls\": token_tracker.call_count,\n",
        "                \"total_input_tokens\": token_tracker.total_input_tokens,\n",
        "                \"total_output_tokens\": token_output_tokens,\n",
        "                \"total_tokens\": token_tracker.total_tokens\n",
        "            },\n",
        "            \"document_results\": task_results,\n",
        "            \"token_call_history\": token_tracker.call_history\n",
        "        }\n",
        "\n",
        "        # Save JSON results\n",
        "        try:\n",
        "            with open(results_file, 'w') as f:\n",
        "                json.dump(comprehensive_results, f, indent=2, default=str)\n",
        "            print(f\"💾 JSON RESULTS SAVED TO: {results_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error saving JSON results: {e}\")\n",
        "            state[\"error\"] = f\"Error saving JSON results: {e}\"\n",
        "\n",
        "\n",
        "        # Create CSV from results\n",
        "        csv_rows = []\n",
        "\n",
        "        for filename, doc_results in task_results.items():\n",
        "            if \"error\" in doc_results:\n",
        "                # Add error row\n",
        "                csv_rows.append({\n",
        "                    \"filename\": filename,\n",
        "                    \"task\": \"workflow_error\", # Indicate workflow level error for this document\n",
        "                    \"response\": doc_results[\"error\"],\n",
        "                    \"context_chunks_used\": 0,\n",
        "                    \"input_tokens\": 0,\n",
        "                    \"output_tokens\": 0,\n",
        "                    \"total_tokens\": 0\n",
        "                })\n",
        "            else:\n",
        "                # Process each task for this document\n",
        "                for task_name, task_result in doc_results.items():\n",
        "                    if isinstance(task_result, dict):\n",
        "                        token_usage = task_result.get(\"token_usage\", {})\n",
        "                        csv_rows.append({\n",
        "                            \"filename\": filename,\n",
        "                            \"task\": task_name,\n",
        "                            \"response\": task_result.get(\"response\", \"\"),\n",
        "                            \"context_chunks_used\": task_result.get(\"context_chunks_used\", 0),\n",
        "                            \"input_tokens\": token_usage.get(\"input_tokens\", 0),\n",
        "                            \"output_tokens\": token_usage.get(\"output_tokens\", 0),\n",
        "                            \"total_tokens\": token_usage.get(\"total_tokens\", 0)\n",
        "                        })\n",
        "                    else:\n",
        "                         # Handle task-specific errors\n",
        "                        csv_rows.append({\n",
        "                            \"filename\": filename,\n",
        "                            \"task\": task_name,\n",
        "                            \"response\": f\"Task error: {task_result}\",\n",
        "                            \"context_chunks_used\": 0,\n",
        "                            \"input_tokens\": 0,\n",
        "                            \"output_tokens\": 0,\n",
        "                            \"total_tokens\": 0\n",
        "                        })\n",
        "\n",
        "\n",
        "        # Save CSV\n",
        "        if csv_rows:\n",
        "            try:\n",
        "                df = pd.DataFrame(csv_rows)\n",
        "\n",
        "                # Reorder columns for better readability - handle missing columns gracefully\n",
        "                column_order = [\n",
        "                    \"filename\", \"task\", \"response\",\n",
        "                    \"context_chunks_used\", \"input_tokens\",\n",
        "                    \"output_tokens\", \"total_tokens\"\n",
        "                ]\n",
        "                existing_columns = [col for col in column_order if col in df.columns]\n",
        "                df = df[existing_columns]\n",
        "\n",
        "\n",
        "                # Save to CSV\n",
        "                df.to_csv(csv_file, index=False, encoding='utf-8')\n",
        "                print(f\"💾 CSV RESULTS SAVED TO: {csv_file}\")\n",
        "\n",
        "                # Display summary statistics from CSV\n",
        "                print(f\"\\n📊 CSV Summary:\")\n",
        "                print(f\"   📄 Total rows: {len(df)}\")\n",
        "                # Ensure 'filename' column exists before calling nunique\n",
        "                if 'filename' in df.columns:\n",
        "                    print(f\"   📁 Documents: {df['filename'].nunique()}\")\n",
        "                     # Handle case where no tasks were processed successfully\n",
        "                    if not df[df['task'] != 'workflow_error'].empty and 'filename' in df.columns:\n",
        "                         print(f\"   🎯 Tasks per document: {df[df['task'] != 'workflow_error'].groupby('filename').size().mean():.1f}\")\n",
        "                    else:\n",
        "                        print(\"   🎯 Tasks per document: N/A (No successful tasks)\")\n",
        "\n",
        "                # Ensure 'total_tokens' column exists and is numeric before summing\n",
        "                if 'total_tokens' in df.columns:\n",
        "                    try:\n",
        "                        df['total_tokens'] = pd.to_numeric(df['total_tokens'], errors='coerce').fillna(0).astype(int)\n",
        "                        print(f\"   🔢 Total tokens used: {df['total_tokens'].sum():,}\")\n",
        "                    except Exception as e:\n",
        "                         print(f\"⚠️ Could not calculate total tokens from CSV: {e}\")\n",
        "                else:\n",
        "                    print(\"⚠️ 'total_tokens' column not found in CSV.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing or saving CSV results: {e}\")\n",
        "                state[\"error\"] = f\"Error processing or saving CSV results: {e}\"\n",
        "\n",
        "\n",
        "        # Also save a summary CSV with aggregated data per document\n",
        "        summary_csv_file = f\"enhanced_rag_expense_summary_{timestamp}.csv\"\n",
        "        summary_rows = []\n",
        "\n",
        "        for filename, doc_results in task_results.items():\n",
        "            if \"error\" not in doc_results:\n",
        "                row = {\"filename\": filename}\n",
        "\n",
        "                # Extract key information from each task\n",
        "                for task_name in [\"extract_amount\", \"extract_date\", \"extract_vendor\",\n",
        "                                \"extract_category\", \"extract_tax\"]:\n",
        "                    if task_name in doc_results and isinstance(doc_results[task_name], dict):\n",
        "                        response = doc_results[task_name].get(\"response\", \"\")\n",
        "                        # Clean the response (take first line or first 100 chars)\n",
        "                        cleaned = response.split('\\n')[0][:100] if response else \"\"\n",
        "                        row[task_name] = cleaned\n",
        "                    else:\n",
        "                        row[task_name] = f\"Task {task_name} failed or not found\"\n",
        "\n",
        "\n",
        "                # Add token totals\n",
        "                total_tokens = sum(\n",
        "                    doc_results.get(task, {}).get(\"token_usage\", {}).get(\"total_tokens\", 0)\n",
        "                    for task in doc_results if isinstance(doc_results.get(task), dict)\n",
        "                )\n",
        "                row[\"total_tokens_used\"] = total_tokens\n",
        "\n",
        "                summary_rows.append(row)\n",
        "            else:\n",
        "                 summary_rows.append({\"filename\": filename, \"workflow_error\": doc_results[\"error\"]}) # Add error to summary\n",
        "\n",
        "\n",
        "        if summary_rows:\n",
        "            try:\n",
        "                summary_df = pd.DataFrame(summary_rows)\n",
        "                summary_df.to_csv(summary_csv_file, index=False, encoding='utf-8')\n",
        "                print(f\"💾 SUMMARY CSV SAVED TO: {summary_csv_file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error saving summary CSV: {e}\")\n",
        "                state[\"error\"] = f\"Error saving summary CSV: {e}\"\n",
        "\n",
        "\n",
        "        print(f\"\\n📊 FILES SAVED:\")\n",
        "        print(f\"   📄 Detailed JSON: {results_file}\")\n",
        "        print(f\"   📄 Detailed CSV: {csv_file}\")\n",
        "        print(f\"   📄 Summary CSV: {summary_csv_file}\")\n",
        "        print(f\"   📄 Documents processed: {successful_documents}/{total_documents}\")\n",
        "\n",
        "        # Print token usage summary\n",
        "        token_tracker.print_summary()\n",
        "\n",
        "        state[\"workflow_status\"] = \"complete\"\n",
        "        return state\n",
        "\n",
        "\n",
        "    # Build enhanced workflow\n",
        "    workflow = StateGraph(EnhancedRAGWorkflowState)\n",
        "\n",
        "    workflow.add_node(\"enhanced_ingestion\", enhanced_ingestion_node)\n",
        "    workflow.add_node(\"enhanced_task_processing\", enhanced_task_processing_node)\n",
        "    workflow.add_node(\"enhanced_results_compilation\", enhanced_results_compilation_node)\n",
        "\n",
        "    workflow.set_entry_point(\"enhanced_ingestion\")\n",
        "    workflow.add_edge(\"enhanced_ingestion\", \"enhanced_task_processing\")\n",
        "    workflow.add_edge(\"enhanced_task_processing\", \"enhanced_results_compilation\")\n",
        "    workflow.set_finish_point(\"enhanced_results_compilation\")\n",
        "\n",
        "    return workflow.compile()\n",
        "\n",
        "# Update the Gradio UI function to call the workflow\n",
        "def process_document_ui(file):\n",
        "    \"\"\"Function to receive the uploaded file, run the workflow, and display results.\"\"\"\n",
        "    if file is None:\n",
        "        return \"Please upload a file.\", \"No file uploaded.\", pd.DataFrame() # Return empty DataFrame\n",
        "\n",
        "    file_path = file.name # Gradio provides the temporary path here\n",
        "\n",
        "    status = f\"Received file: {file_path}. Starting RAG workflow...\"\n",
        "    print(status) # Print status to console\n",
        "\n",
        "    # Initialize the RAG processor and workflow\n",
        "    # Assuming processor and token_tracker are accessible globally or initialized here\n",
        "    try:\n",
        "        processor = EnhancedRAGExpenseProcessor()\n",
        "        workflow = create_enhanced_rag_workflow(processor)\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error initializing RAG components: {e}\"\n",
        "        print(f\"❌ {error_msg}\")\n",
        "        return error_msg, \"Initialization failed.\", pd.DataFrame()\n",
        "\n",
        "\n",
        "    # Execute enhanced workflow with the uploaded file path\n",
        "    initial_state = {\n",
        "        \"file_paths\": [file_path], # Pass the uploaded file path as a list\n",
        "        \"current_file_index\": 0,\n",
        "        \"processed_filenames\": [],\n",
        "        \"current_filename\": \"\",\n",
        "        \"task_results\": {},\n",
        "        \"workflow_status\": \"initialized\",\n",
        "        \"error\": None\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        final_state = workflow.invoke(initial_state)\n",
        "        status = f\"Workflow completed with status: {final_state.get('workflow_status')}\"\n",
        "        print(status) # Print final status\n",
        "\n",
        "        # Process final state to display results\n",
        "        task_results = final_state.get(\"task_results\", {})\n",
        "        if final_state.get(\"error\"):\n",
        "             details = f\"Workflow Error: {final_state['error']}\"\n",
        "             # Attempt to display any partial results if available\n",
        "             if task_results:\n",
        "                 details += \"\\n\\nPartial Results:\"\n",
        "                 # Create a simple string representation of partial results\n",
        "                 for filename, doc_results in task_results.items():\n",
        "                      details += f\"\\nDocument: {filename}\"\n",
        "                      if \"error\" in doc_results:\n",
        "                           details += f\"\\n  Error: {doc_results['error']}\"\n",
        "                      else:\n",
        "                          for task_name, task_result in doc_results.items():\n",
        "                              if isinstance(task_result, dict):\n",
        "                                   details += f\"\\n  {task_name}: {task_result.get('response', 'No response')[:100]}...\"\n",
        "                              else:\n",
        "                                  details += f\"\\n  {task_name}: Error - {task_result}\"\n",
        "\n",
        "             return status, details, pd.DataFrame() # Return empty dataframe on error\n",
        "\n",
        "        # Compile results for display\n",
        "        if task_results:\n",
        "            compiled_details = \"\"\n",
        "            csv_rows = [] # Prepare data for DataFrame display\n",
        "\n",
        "            for filename, results in task_results.items():\n",
        "                 compiled_details += f\"\\n📄 DOCUMENT: {filename}\\n\" + \"─\" * 50 + \"\\n\"\n",
        "                 if \"error\" in results:\n",
        "                     compiled_details += f\"❌ Error: {results['error']}\\n\"\n",
        "                      # Add error row to CSV data\n",
        "                     csv_rows.append({\n",
        "                        \"filename\": filename,\n",
        "                        \"task\": \"workflow_error\",\n",
        "                        \"response\": results[\"error\"],\n",
        "                        \"context_chunks_used\": 0,\n",
        "                        \"input_tokens\": 0,\n",
        "                        \"output_tokens\": 0,\n",
        "                        \"total_tokens\": 0\n",
        "                    })\n",
        "                 else:\n",
        "                    for task_name, task_result in results.items():\n",
        "                        if isinstance(task_result, dict):\n",
        "                            response = task_result.get(\"response\", \"No response\")\n",
        "                            chunks_used = task_result.get(\"context_chunks_used\", 0)\n",
        "                            token_usage = task_result.get(\"token_usage\", {})\n",
        "\n",
        "                            compiled_details += f\"\\n🎯 {task_name.upper()}:\\n\"\n",
        "                            compiled_details += f\"   📝 Response: {response[:200]}...\\n\" # Limit display length\n",
        "                            compiled_details += f\"   📚 Chunks used: {chunks_used}\\n\"\n",
        "                            if token_usage:\n",
        "                                compiled_details += f\"   🔢 Tokens: {token_usage.get('total_tokens', 0)}\\n\"\n",
        "\n",
        "                             # Add task result to CSV data\n",
        "                            csv_rows.append({\n",
        "                                \"filename\": filename,\n",
        "                                \"task\": task_name,\n",
        "                                \"response\": response,\n",
        "                                \"context_chunks_used\": chunks_used,\n",
        "                                \"input_tokens\": token_usage.get(\"input_tokens\", 0),\n",
        "                                \"output_tokens\": token_usage.get(\"output_tokens\", 0),\n",
        "                                \"total_tokens\": token_usage.get(\"total_tokens\", 0)\n",
        "                            })\n",
        "                        else:\n",
        "                            # Handle task-specific errors in display and CSV\n",
        "                             compiled_details += f\"\\n🎯 {task_name.upper()}:\\n\"\n",
        "                             compiled_details += f\"   ❌ Task Error: {task_result}\\n\"\n",
        "                             csv_rows.append({\n",
        "                                \"filename\": filename,\n",
        "                                \"task\": task_name,\n",
        "                                \"response\": f\"Task error: {task_result}\",\n",
        "                                \"context_chunks_used\": 0,\n",
        "                                \"input_tokens\": 0,\n",
        "                                \"output_tokens\": 0,\n",
        "                                \"total_tokens\": 0\n",
        "                            })\n",
        "\n",
        "\n",
        "            # Create DataFrame for the summary output\n",
        "            if csv_rows:\n",
        "                results_df = pd.DataFrame(csv_rows)\n",
        "                 # Ensure column order\n",
        "                column_order = [\n",
        "                    \"filename\", \"task\", \"response\",\n",
        "                    \"context_chunks_used\", \"input_tokens\",\n",
        "                    \"output_tokens\", \"total_tokens\"\n",
        "                ]\n",
        "                existing_columns = [col for col in column_order if col in results_df.columns]\n",
        "                results_df = results_df[existing_columns]\n",
        "\n",
        "                # Convert token columns to numeric, handling errors\n",
        "                for token_col in ['input_tokens', 'output_tokens', 'total_tokens']:\n",
        "                     if token_col in results_df.columns:\n",
        "                        results_df[token_col] = pd.to_numeric(results_df[token_col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "\n",
        "            else:\n",
        "                 results_df = pd.DataFrame({\"Status\": [\"No task results generated.\"]}) # Empty DataFrame or error indicator\n",
        "\n",
        "\n",
        "            return status, compiled_details, results_df # Return DataFrame for display\n",
        "\n",
        "        else:\n",
        "            return status, \"No task results generated.\", pd.DataFrame() # Return empty DataFrame if no task results\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Workflow execution failed: {e}\"\n",
        "        print(f\"❌ {error_msg}\")\n",
        "        return status, error_msg, pd.DataFrame() # Return empty DataFrame on workflow execution error\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Enhanced RAG-Based Expense Claims Processing\")\n",
        "    gr.Markdown(\"Upload your expense claim document (PDF, JPG, PNG, etc.) to extract details.\")\n",
        "\n",
        "    file_input = gr.File(label=\"Upload Expense Document\", file_types=[\".pdf\", \".jpg\", \".jpeg\", \".png\", \".tiff\"])\n",
        "    process_button = gr.Button(\"Process Document\")\n",
        "\n",
        "    status_output = gr.Textbox(label=\"Processing Status\", max_lines=5)\n",
        "    details_output = gr.Textbox(label=\"Extracted Details (Raw)\", interactive=False, lines=20)\n",
        "    summary_output = gr.Dataframe(label=\"Extracted Details (Structured)\") # Use Dataframe for structured output\n",
        "\n",
        "    process_button.click(\n",
        "        process_document_ui,\n",
        "        inputs=file_input,\n",
        "        outputs=[status_output, details_output, summary_output]\n",
        "    )\n",
        "\n",
        "# Note: demo.launch() will be called separately when running the Gradio app\n",
        "# For this subtask, we define the function but don't call launch() yet."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6262cdf2"
      },
      "source": [
        "## Run the workflow\n",
        "\n",
        "### Subtask:\n",
        "Trigger the RAG workflow with the uploaded file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "616f4d09"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed because the necessary classes (`EnhancedRAGExpenseProcessor`, `TokenUsageTracker`, etc.) were not defined in the current environment. I need to re-execute the code cells that define these classes and initialize the `processor` and `token_tracker` instances before I can trigger the workflow in the Gradio function. I will also ensure the `create_enhanced_rag_workflow` function is defined, which it appears to be in the provided history, but I need to make sure the function is accessible. Finally, I will update the `process_document_ui` function to invoke the workflow with the uploaded file's path and process the results for display in the Gradio interface.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03e67059"
      },
      "source": [
        "# Re-define or ensure availability of necessary classes and instances\n",
        "# Assuming the previous cells defining these are re-executed\n",
        "\n",
        "# Initialize the RAG processor and workflow globally for efficiency in Gradio\n",
        "# This avoids re-initializing the processor (and thus ChromaDB client/collection)\n",
        "# on every file upload in the Gradio app.\n",
        "\n",
        "# Ensure the TokenUsageTracker is also a persistent instance\n",
        "try:\n",
        "    # Re-initialize token_tracker as it might have been reset by the environment\n",
        "    token_tracker = TokenUsageTracker()\n",
        "    processor = EnhancedRAGExpenseProcessor()\n",
        "    # Create the workflow instance\n",
        "    enhanced_rag_workflow = create_enhanced_rag_workflow(processor)\n",
        "    print(\"\\n✅ RAG Processor and Workflow initialized globally.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Error initializing RAG components globally: {e}\")\n",
        "    # Handle this error appropriately if initialization fails\n",
        "\n",
        "def process_document_ui(file):\n",
        "    \"\"\"Function to receive the uploaded file, run the workflow, and display results.\"\"\"\n",
        "    # Use the globally initialized workflow and processor\n",
        "    global enhanced_rag_workflow, processor, token_tracker\n",
        "\n",
        "    if file is None:\n",
        "        return \"Please upload a file.\", \"No file uploaded.\", pd.DataFrame() # Return empty DataFrame\n",
        "\n",
        "    file_path = file.name # Gradio provides the temporary path here\n",
        "\n",
        "    status = f\"Received file: {file_path}. Starting RAG workflow...\"\n",
        "    print(status) # Print status to console\n",
        "\n",
        "    # Check if workflow initialization was successful\n",
        "    if enhanced_rag_workflow is None:\n",
        "         error_msg = \"RAG Workflow failed to initialize. Cannot process file.\"\n",
        "         print(f\"❌ {error_msg}\")\n",
        "         return error_msg, \"Initialization failed.\", pd.DataFrame()\n",
        "\n",
        "\n",
        "    # Execute enhanced workflow with the uploaded file path\n",
        "    # Reset the token tracker for this specific workflow run if needed,\n",
        "    # or let it accumulate total usage across all runs.\n",
        "    # For this demo, let's let it accumulate, as it shows total usage.\n",
        "    # If per-file usage is needed, reset here: token_tracker = TokenUsageTracker()\n",
        "\n",
        "\n",
        "    initial_state = {\n",
        "        \"file_paths\": [file_path], # Pass the uploaded file path as a list\n",
        "        \"current_file_index\": 0, # Not strictly used with single file processing, but keep for state structure\n",
        "        \"processed_filenames\": [],\n",
        "        \"current_filename\": \"\", # Not strictly used with single file processing\n",
        "        \"task_results\": {},\n",
        "        \"workflow_status\": \"initialized\",\n",
        "        \"error\": None\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        print(f\"Executing workflow with state: {initial_state}\")\n",
        "        final_state = enhanced_rag_workflow.invoke(initial_state)\n",
        "        workflow_final_status = final_state.get('workflow_status', 'unknown')\n",
        "        status = f\"Workflow completed with status: {workflow_final_status}\"\n",
        "        print(status) # Print final status\n",
        "        print(f\"Final state: {final_state}\") # Print final state for debugging\n",
        "\n",
        "\n",
        "        # Process final state to display results\n",
        "        task_results = final_state.get(\"task_results\", {})\n",
        "        compiled_details = \"\"\n",
        "        results_df = pd.DataFrame() # Default to empty DataFrame\n",
        "\n",
        "\n",
        "        if final_state.get(\"error\"):\n",
        "             compiled_details = f\"Workflow Error: {final_state['error']}\\n\\n\"\n",
        "             # Attempt to display any partial results if available\n",
        "             if task_results:\n",
        "                 compiled_details += \"Partial Results:\\n\"\n",
        "                 for filename, doc_results in task_results.items():\n",
        "                      compiled_details += f\"\\nDocument: {filename}\\n\"\n",
        "                      if \"error\" in doc_results:\n",
        "                           compiled_details += f\"  Error: {doc_results['error']}\\n\"\n",
        "                      else:\n",
        "                          for task_name, task_result in doc_results.items():\n",
        "                              if isinstance(task_result, dict):\n",
        "                                   response_preview = task_result.get('response', 'No response')\n",
        "                                   compiled_details += f\"  {task_name}: {response_preview[:100]}...\\n\"\n",
        "                              else:\n",
        "                                  compiled_details += f\"  {task_name}: Error - {task_result}\\n\"\n",
        "\n",
        "             # If there are partial results that can be put in a DataFrame, try that\n",
        "             if task_results:\n",
        "                  try:\n",
        "                     csv_rows = []\n",
        "                     for filename, doc_results in task_results.items():\n",
        "                         if isinstance(doc_results, dict):\n",
        "                            if \"error\" in doc_results:\n",
        "                                csv_rows.append({\n",
        "                                    \"filename\": filename,\n",
        "                                    \"task\": \"workflow_error\",\n",
        "                                    \"response\": doc_results[\"error\"],\n",
        "                                    \"context_chunks_used\": 0,\n",
        "                                    \"input_tokens\": 0,\n",
        "                                    \"output_tokens\": 0,\n",
        "                                    \"total_tokens\": 0\n",
        "                                })\n",
        "                            else:\n",
        "                                for task_name, task_result in doc_results.items():\n",
        "                                    if isinstance(task_result, dict):\n",
        "                                        token_usage = task_result.get(\"token_usage\", {})\n",
        "                                        csv_rows.append({\n",
        "                                            \"filename\": filename,\n",
        "                                            \"task\": task_name,\n",
        "                                            \"response\": task_result.get(\"response\", \"\"),\n",
        "                                            \"context_chunks_used\": task_result.get(\"context_chunks_used\", 0),\n",
        "                                            \"input_tokens\": token_usage.get(\"input_tokens\", 0),\n",
        "                                            \"output_tokens\": token_usage.get(\"output_tokens\", 0),\n",
        "                                            \"total_tokens\": token_usage.get(\"total_tokens\", 0)\n",
        "                                        })\n",
        "                                    else:\n",
        "                                         csv_rows.append({\n",
        "                                            \"filename\": filename,\n",
        "                                            \"task\": task_name,\n",
        "                                            \"response\": f\"Task error: {task_result}\",\n",
        "                                            \"context_chunks_used\": 0,\n",
        "                                            \"input_tokens\": 0,\n",
        "                                            \"output_tokens\": 0,\n",
        "                                            \"total_tokens\": 0\n",
        "                                        })\n",
        "                         else:\n",
        "                              csv_rows.append({\n",
        "                                    \"filename\": filename,\n",
        "                                    \"task\": \"processing_failed\",\n",
        "                                    \"response\": f\"Document processing failed: {doc_results}\",\n",
        "                                    \"context_chunks_used\": 0,\n",
        "                                    \"input_tokens\": 0,\n",
        "                                    \"output_tokens\": 0,\n",
        "                                    \"total_tokens\": 0\n",
        "                                })\n",
        "\n",
        "                     if csv_rows:\n",
        "                        results_df = pd.DataFrame(csv_rows)\n",
        "                        column_order = [\n",
        "                            \"filename\", \"task\", \"response\",\n",
        "                            \"context_chunks_used\", \"input_tokens\",\n",
        "                            \"output_tokens\", \"total_tokens\"\n",
        "                        ]\n",
        "                        existing_columns = [col for col in column_order if col in results_df.columns]\n",
        "                        results_df = results_df[existing_columns]\n",
        "                        for token_col in ['input_tokens', 'output_tokens', 'total_tokens']:\n",
        "                            if token_col in results_df.columns:\n",
        "                                results_df[token_col] = pd.to_numeric(results_df[token_col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "                  except Exception as csv_e:\n",
        "                       compiled_details += f\"\\nError compiling partial results DataFrame: {csv_e}\"\n",
        "                       results_df = pd.DataFrame({\"Error\": [f\"Could not compile results: {csv_e}\"]}) # Indicate error in DataFrame too\n",
        "\n",
        "\n",
        "             return status, compiled_details, results_df # Return DataFrame even on error\n",
        "\n",
        "\n",
        "        # Compile results for display if no workflow-level error\n",
        "        if task_results:\n",
        "            compiled_details = \"\"\n",
        "            csv_rows = [] # Prepare data for DataFrame display\n",
        "\n",
        "            for filename, results in task_results.items():\n",
        "                 compiled_details += f\"\\n📄 DOCUMENT: {filename}\\n\" + \"─\" * 50 + \"\\n\"\n",
        "                 if \"error\" in results:\n",
        "                     compiled_details += f\"❌ Error: {results['error']}\\n\"\n",
        "                     csv_rows.append({\n",
        "                        \"filename\": filename,\n",
        "                        \"task\": \"document_error\",\n",
        "                        \"response\": results[\"error\"],\n",
        "                        \"context_chunks_used\": 0,\n",
        "                        \"input_tokens\": 0,\n",
        "                        \"output_tokens\": 0,\n",
        "                        \"total_tokens\": 0\n",
        "                    })\n",
        "                 else:\n",
        "                    for task_name, task_result in results.items():\n",
        "                        if isinstance(task_result, dict):\n",
        "                            response = task_result.get(\"response\", \"No response\")\n",
        "                            chunks_used = task_result.get(\"context_chunks_used\", 0)\n",
        "                            token_usage = task_result.get(\"token_usage\", {})\n",
        "\n",
        "                            compiled_details += f\"\\n🎯 {task_name.upper()}:\\n\"\n",
        "                            compiled_details += f\"   📝 Response: {response[:200]}...\\n\" # Limit display length\n",
        "                            compiled_details += f\"   📚 Chunks used: {chunks_used}\\n\"\n",
        "                            if token_usage:\n",
        "                                compiled_details += f\"   🔢 Tokens: {token_usage.get('total_tokens', 0)}\\n\"\n",
        "\n",
        "                            csv_rows.append({\n",
        "                                \"filename\": filename,\n",
        "                                \"task\": task_name,\n",
        "                                \"response\": response,\n",
        "                                \"context_chunks_used\": chunks_used,\n",
        "                                \"input_tokens\": token_usage.get(\"input_tokens\", 0),\n",
        "                                \"output_tokens\": token_usage.get(\"output_tokens\", 0),\n",
        "                                \"total_tokens\": token_usage.get(\"total_tokens\", 0)\n",
        "                            })\n",
        "                        else:\n",
        "                             compiled_details += f\"\\n🎯 {task_name.upper()}:\\n\"\n",
        "                             compiled_details += f\"   ❌ Task Error: {task_result}\\n\"\n",
        "                             csv_rows.append({\n",
        "                                \"filename\": filename,\n",
        "                                \"task\": task_name,\n",
        "                                \"response\": f\"Task error: {task_result}\",\n",
        "                                \"context_chunks_used\": 0,\n",
        "                                \"input_tokens\": 0,\n",
        "                                \"output_tokens\": 0,\n",
        "                                \"total_tokens\": 0\n",
        "                            })\n",
        "\n",
        "\n",
        "            # Create DataFrame for the summary output\n",
        "            if csv_rows:\n",
        "                results_df = pd.DataFrame(csv_rows)\n",
        "                column_order = [\n",
        "                    \"filename\", \"task\", \"response\",\n",
        "                    \"context_chunks_used\", \"input_tokens\",\n",
        "                    \"output_tokens\", \"total_tokens\"\n",
        "                ]\n",
        "                existing_columns = [col for col in column_order if col in results_df.columns]\n",
        "                results_df = results_df[existing_columns]\n",
        "                for token_col in ['input_tokens', 'output_tokens', 'total_tokens']:\n",
        "                     if token_col in results_df.columns:\n",
        "                        results_df[token_col] = pd.to_numeric(results_df[token_col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "            else:\n",
        "                 results_df = pd.DataFrame({\"Status\": [\"No task results generated.\"]})\n",
        "\n",
        "\n",
        "            # Display token usage summary in the details output as well\n",
        "            compiled_details += \"\\n\\n📊 TOTAL TOKEN USAGE SUMMARY:\\n\"\n",
        "            compiled_details += f\"🔢 Total LLM Calls: {token_tracker.call_count}\\n\"\n",
        "            compiled_details += f\"📥 Total Input Tokens: {token_tracker.total_input_tokens:,}\\n\"\n",
        "            compiled_details += f\"📤 Total Output Tokens: {token_tracker.total_output_tokens:,}\\n\"\n",
        "            compiled_details += f\"🎯 Grand Total Tokens: {token_tracker.total_tokens:,}\\n\"\n",
        "            if token_tracker.call_count > 0:\n",
        "                compiled_details += f\"📊 Average per call: {token_tracker.total_tokens/token_tracker.call_count:.1f} tokens\\n\"\n",
        "\n",
        "\n",
        "            return status, compiled_details, results_df # Return DataFrame for display\n",
        "\n",
        "        else:\n",
        "            return status, \"No task results generated.\", pd.DataFrame() # Return empty DataFrame if no task results\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Workflow execution failed: {e}\"\n",
        "        print(f\"❌ {error_msg}\")\n",
        "        # Attempt to return any partial state/error info\n",
        "        partial_details = f\"An unexpected error occurred during workflow execution: {e}\"\n",
        "        if 'final_state' in locals() and final_state:\n",
        "             partial_details += f\"\\n\\nPartial workflow state: {final_state.get('workflow_status', 'unknown')}\"\n",
        "             if final_state.get('error'):\n",
        "                  partial_details += f\"\\nInternal state error: {final_state['error']}\"\n",
        "             if final_state.get('task_results'):\n",
        "                  partial_details += \"\\nPartial task results available (check console for details).\"\n",
        "                  # Optionally add a simplified representation of partial task results to partial_details\n",
        "\n",
        "        # Attempt to create a simple error DataFrame\n",
        "        error_df = pd.DataFrame({\"Error\": [error_msg]})\n",
        "\n",
        "        return \"Workflow Failed\", partial_details, error_df\n",
        "\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Enhanced RAG-Based Expense Claims Processing\")\n",
        "    gr.Markdown(\"Upload your expense claim document (PDF, JPG, PNG, etc.) to extract details.\")\n",
        "\n",
        "    file_input = gr.File(label=\"Upload Expense Document\", file_types=[\".pdf\", \".jpg\", \".jpeg\", \".png\", \".tiff\"])\n",
        "    process_button = gr.Button(\"Process Document\")\n",
        "\n",
        "    status_output = gr.Textbox(label=\"Processing Status\", max_lines=5)\n",
        "    details_output = gr.Textbox(label=\"Extracted Details (Raw)\", interactive=False, lines=20)\n",
        "    summary_output = gr.Dataframe(label=\"Extracted Details (Structured)\") # Use Dataframe for structured output\n",
        "\n",
        "    process_button.click(\n",
        "        process_document_ui,\n",
        "        inputs=file_input,\n",
        "        outputs=[status_output, details_output, summary_output]\n",
        "    )\n",
        "\n",
        "# Launch the Gradio interface (only when running the script, not needed during subtask execution)\n",
        "# demo.launch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a04a178f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed because `TokenUsageTracker` and other classes (`EnhancedRAGExpenseProcessor`, etc.) were not defined. These classes were defined in previous code cells according to the notebook history, but the execution environment lost their definitions. I need to re-execute the code cells that contain the definitions of all necessary components (`TokenUsageTracker`, `EnhancedRAGExpenseProcessor`, `create_enhanced_rag_workflow` function, and all classes that `EnhancedRAGExpenseProcessor` depends on) to make them available in the current environment. Once these are defined, I will re-attempt to initialize the global processor and workflow instances and then define the `process_document_ui` function which uses them and launch the Gradio app.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57f3f33c"
      },
      "source": [
        "# Re-execute code from previous cells to ensure all necessary components are defined.\n",
        "# This includes TokenUsageTracker, ClaimDocument, FilenameBasedDocumentManager,\n",
        "# EnhancedOCRProcessor, EnhancedIsolatedVectorStore, EnhancedExpenseTaskManager,\n",
        "# EnhancedRAGExpenseProcessor, and create_enhanced_rag_workflow.\n",
        "\n",
        "# ================================\n",
        "# STEP 2: TOKEN USAGE TRACKING (Re-definition)\n",
        "# ================================\n",
        "\n",
        "class TokenUsageTracker:\n",
        "    \"\"\"Track token usage across all LLM calls\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.call_history = []\n",
        "        self.total_input_tokens = 0\n",
        "        self.total_output_tokens = 0\n",
        "        self.total_tokens = 0\n",
        "        self.call_count = 0\n",
        "\n",
        "    def track_call(self, operation: str, filename: str, task: str, response):\n",
        "        \"\"\"Track a single LLM call and extract usage info\"\"\"\n",
        "\n",
        "        usage_info = {\n",
        "            \"operation\": operation,\n",
        "            \"filename\": filename,\n",
        "            \"task\": task,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"input_tokens\": 0,\n",
        "            \"output_tokens\": 0,\n",
        "            \"total_tokens\": 0,\n",
        "            \"duration_ms\": 0\n",
        "        }\n",
        "\n",
        "        # Extract token usage from response\n",
        "        try:\n",
        "            if hasattr(response, 'usage_metadata') and response.usage_metadata:\n",
        "                usage_info[\"input_tokens\"] = response.usage_metadata.get('input_tokens', 0)\n",
        "                usage_info[\"output_tokens\"] = response.usage_metadata.get('output_tokens', 0)\n",
        "                usage_info[\"total_tokens\"] = response.usage_metadata.get('total_tokens', 0)\n",
        "\n",
        "            # Fallback: try response_metadata\n",
        "            elif hasattr(response, 'response_metadata') and response.response_metadata:\n",
        "                metadata = response.response_metadata\n",
        "                usage_info[\"input_tokens\"] = metadata.get('prompt_eval_count', 0)\n",
        "                usage_info[\"output_tokens\"] = metadata.get('eval_count', 0)\n",
        "                usage_info[\"total_tokens\"] = usage_info[\"input_tokens\"] + usage_info[\"output_tokens\"]\n",
        "                usage_info[\"duration_ms\"] = metadata.get('total_duration', 0) // 1000000  # Convert to ms\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not extract token usage: {e}\")\n",
        "\n",
        "        # Update totals\n",
        "        self.total_input_tokens += usage_info[\"input_tokens\"]\n",
        "        self.total_output_tokens += usage_info[\"output_tokens\"]\n",
        "        self.total_tokens += usage_info[\"total_tokens\"]\n",
        "        self.call_count += 1\n",
        "\n",
        "        # Store call history\n",
        "        self.call_history.append(usage_info)\n",
        "\n",
        "        # Print usage info\n",
        "        self.print_usage_info(usage_info)\n",
        "\n",
        "        return usage_info\n",
        "\n",
        "\n",
        "    def print_usage_info(self, usage_info: Dict[str, Any]):\n",
        "        \"\"\"Print formatted usage information\"\"\"\n",
        "        print(f\"📊 TOKEN USAGE - {usage_info['operation']} | {usage_info['filename']} | {usage_info['task']}\")\n",
        "        print(f\"   📥 Input: {usage_info['input_tokens']} tokens\")\n",
        "        print(f\"   📤 Output: {usage_info['output_tokens']} tokens\")\n",
        "        print(f\"   🔢 Total: {usage_info['total_tokens']} tokens\")\n",
        "        if usage_info['duration_ms'] > 0:\n",
        "            print(f\"   ⏱️ Duration: {usage_info['duration_ms']}ms\")\n",
        "        print()\n",
        "\n",
        "    def print_summary(self):\n",
        "        \"\"\"Print overall token usage summary\"\"\"\n",
        "        print(\"=\" * 60)\n",
        "        print(\"📊 TOTAL TOKEN USAGE SUMMARY\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"🔢 Total LLM Calls: {self.call_count}\")\n",
        "        print(f\"📥 Total Input Tokens: {self.total_input_tokens:,}\")\n",
        "        print(f\"📤 Total Output Tokens: {self.total_output_tokens:,}\")\n",
        "        print(f\"🎯 Grand Total Tokens: {self.total_tokens:,}\")\n",
        "\n",
        "        if self.call_count > 0:\n",
        "            print(f\"📊 Average per call: {self.total_tokens/self.call_count:.1f} tokens\")\n",
        "        print()\n",
        "\n",
        "# Global token tracker\n",
        "token_tracker = TokenUsageTracker()\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - TokenUsageTracker!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ================================\n",
        "# STEP 3: FILENAME-BASED DOCUMENT MANAGEMENT (Re-definition)\n",
        "# ================================\n",
        "\n",
        "@dataclass\n",
        "class ClaimDocument:\n",
        "    \"\"\"Document with filename-based identification\"\"\"\n",
        "    filename: str  # Primary identifier (no more UUIDs!)\n",
        "    file_path: str\n",
        "    raw_text: str\n",
        "    chunks: List[str]\n",
        "    metadata: Dict[str, Any]\n",
        "    processed_timestamp: datetime\n",
        "\n",
        "class FilenameBasedDocumentManager:\n",
        "    \"\"\"Manages documents using filenames as primary identifiers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.documents_registry = {}  # filename -> ClaimDocument\n",
        "        self.chunk_to_file_map = {}  # chunk_id -> filename\n",
        "\n",
        "    def register_document(self, file_path: str, raw_text: str) -> str:\n",
        "        \"\"\"Register document using filename as ID\"\"\"\n",
        "\n",
        "        filename = Path(file_path).stem  # Get filename without extension\n",
        "\n",
        "        print(f\"📋 REGISTERING DOCUMENT: {filename}\")\n",
        "        print(f\"   📁 Source: {Path(file_path).name}\")\n",
        "        print(f\"   📄 Text length: {len(raw_text)} characters\")\n",
        "\n",
        "        # Create isolated chunks for this document\n",
        "        chunks = self.create_document_chunks(raw_text, filename)\n",
        "\n",
        "        claim_doc = ClaimDocument(\n",
        "            filename=filename,\n",
        "            file_path=file_path,\n",
        "            raw_text=raw_text,\n",
        "            chunks=chunks,\n",
        "            metadata={\n",
        "                \"file_name\": Path(file_path).name,\n",
        "                \"file_extension\": Path(file_path).suffix,\n",
        "                \"chunk_count\": len(chunks),\n",
        "                \"source\": \"ocr_extraction\"\n",
        "            },\n",
        "            processed_timestamp=datetime.now()\n",
        "        )\n",
        "\n",
        "        self.documents_registry[filename] = claim_doc\n",
        "\n",
        "        # Update chunk mapping\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk_id = f\"{filename}_chunk_{i}\"\n",
        "            self.chunk_to_file_map[chunk_id] = filename\n",
        "\n",
        "        print(f\"✅ Document registered: {filename} with {len(chunks)} chunks\")\n",
        "        return filename\n",
        "\n",
        "    def create_document_chunks(self, text: str, filename: str) -> List[str]:\n",
        "        \"\"\"Create chunks with filename-specific context isolation\"\"\"\n",
        "\n",
        "        print(f\"🔪 CHUNKING DOCUMENT: {filename}\")\n",
        "\n",
        "        lines = text.split('\\n')\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "        max_chunk_size = 500\n",
        "\n",
        "        # Expense document section markers\n",
        "        section_markers = [\n",
        "            'total', 'amount', 'date', 'vendor', 'receipt', 'invoice',\n",
        "            'item', 'quantity', 'price', 'tax', 'subtotal'\n",
        "        ]\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            line_length = len(line)\n",
        "            is_section_start = any(marker in line.lower() for marker in section_markers)\n",
        "\n",
        "            if (current_length + line_length > max_chunk_size) or \\\n",
        "               (is_section_start and current_chunk and current_length > 200):\n",
        "\n",
        "                chunk_text = '\\n'.join(current_chunk)\n",
        "                if chunk_text.strip():\n",
        "                    # Add filename isolation metadata to chunk\n",
        "                    isolated_chunk = f\"[DOCUMENT: {filename}]\\n{chunk_text}\"\n",
        "                    chunks.append(isolated_chunk)\n",
        "\n",
        "                current_chunk = [line]\n",
        "                current_length = line_length\n",
        "            else:\n",
        "                current_chunk.append(line)\n",
        "                current_length += line_length + 1\n",
        "\n",
        "        # Add final chunk\n",
        "        if current_chunk:\n",
        "            chunk_text = '\\n'.join(current_chunk)\n",
        "            if chunk_text.strip():\n",
        "                isolated_chunk = f\"[DOCUMENT: {filename}]\\n{chunk_text}\"\n",
        "                chunks.append(isolated_chunk)\n",
        "\n",
        "        print(f\"   🔪 Created {len(chunks)} chunks (avg {len(text)//len(chunks) if chunks else 0} chars each)\")\n",
        "        return chunks\n",
        "\n",
        "\n",
        "\n",
        "    def get_document_context(self, filename: str) -> Optional[ClaimDocument]:\n",
        "        \"\"\"Get complete context for a specific document\"\"\"\n",
        "        return self.documents_registry.get(filename)\n",
        "\n",
        "    def list_all_documents(self) -> List[str]:\n",
        "        \"\"\"List all registered filenames\"\"\"\n",
        "        return list(self.documents_registry.keys())\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - FILENAME-BASED DOCUMENT MANAGEMENT!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ================================\n",
        "# STEP 4: ENHANCED OCR PROCESSOR (Re-definition)\n",
        "# ================================\n",
        "\n",
        "class EnhancedOCRProcessor:\n",
        "    \"\"\"OCR processing with detailed progress tracking\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.supported_formats = ['.pdf', '.jpg', '.jpeg', '.png', '.tiff']\n",
        "\n",
        "    def extract_text_from_document(self, file_path: str) -> str:\n",
        "        \"\"\"Extract text with detailed progress tracking\"\"\"\n",
        "\n",
        "        filename = Path(file_path).name\n",
        "        print(f\"🔍 EXTRACTING TEXT FROM: {filename}\")\n",
        "        print(f\"   📁 Full path: {file_path}\")\n",
        "        print(f\"   📊 File size: {Path(file_path).stat().st_size / 1024:.1f} KB\")\n",
        "\n",
        "        try:\n",
        "            from unstructured.partition.auto import partition\n",
        "\n",
        "            print(f\"   🔄 Processing with UnstructuredIO...\")\n",
        "\n",
        "            # Process document with UnstructuredIO\n",
        "            elements = partition(filename=file_path)\n",
        "\n",
        "            print(f\"   📋 Found {len(elements)} document elements\")\n",
        "\n",
        "            # Extract text from all elements\n",
        "            full_text = \"\"\n",
        "            for i, element in enumerate(elements):\n",
        "                if hasattr(element, 'text') and element.text:\n",
        "                    full_text += element.text + \"\\n\"\n",
        "                    if i < 5:  # Show first few elements\n",
        "                        print(f\"     Element {i+1}: {element.text[:50]}...\")\n",
        "\n",
        "            # Clean and normalize text\n",
        "            full_text = self.clean_extracted_text(full_text)\n",
        "\n",
        "            print(f\"   ✅ Extracted {len(full_text)} characters\")\n",
        "            print(f\"   📝 Text preview: {full_text[:100]}...\")\n",
        "            return full_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ OCR extraction failed: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def clean_extracted_text(self, text: str) -> str:\n",
        "        \"\"\"Clean extracted text with progress info\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        original_length = len(text)\n",
        "        lines = text.split('\\n')\n",
        "        cleaned_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line and len(line) > 2:\n",
        "                cleaned_lines.append(line)\n",
        "\n",
        "        cleaned_text = '\\n'.join(cleaned_lines)\n",
        "        print(f\"   🧹 Cleaned: {original_length} → {len(cleaned_text)} chars ({len(cleaned_lines)} lines)\")\n",
        "\n",
        "        return cleaned_text\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - FILENAME-BASED DOCUMENT MANAGEMENT!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ================================\n",
        "# STEP 5: ENHANCED VECTOR STORE (Re-definition)\n",
        "# ================================\n",
        "\n",
        "class EnhancedIsolatedVectorStore:\n",
        "    \"\"\"ChromaDB with enhanced tracking and filename-based isolation\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model: str = \"nomic-embed-text\"): # Use default or passed model\n",
        "        import chromadb\n",
        "\n",
        "        self.embedding_model = embedding_model\n",
        "\n",
        "        print(f\"🗄️ INITIALIZING VECTOR STORE\")\n",
        "        print(f\"   🤖 Embedding Model: {embedding_model}\")\n",
        "\n",
        "        # Initialize ChromaDB client using new API\n",
        "        self.client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "        # Create collection\n",
        "        self.collection = self.client.get_or_create_collection(\n",
        "            name=\"filename_based_expense_claims\",\n",
        "            metadata={\"hnsw:space\": \"cosine\"}\n",
        "        )\n",
        "\n",
        "        print(f\"   ✅ ChromaDB initialized\")\n",
        "\n",
        "    def embed_text(self, text: str, filename: str = \"unknown\") -> List[float]:\n",
        "        \"\"\"Generate embeddings with progress tracking\"\"\"\n",
        "\n",
        "        print(f\"🔢 GENERATING EMBEDDING: {filename}\")\n",
        "        print(f\"   📝 Text length: {len(text)} chars\")\n",
        "\n",
        "        try:\n",
        "            # Ensure ollama is imported and available\n",
        "            import ollama\n",
        "            response = ollama.embeddings(model=self.embedding_model, prompt=text)\n",
        "            embedding = response['embedding']\n",
        "            print(f\"   ✅ Generated {len(embedding)}-dimensional embedding\")\n",
        "            return embedding\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Embedding error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def add_document_chunks(self, filename: str, chunks: List[str], metadata: Dict[str, Any]):\n",
        "        \"\"\"Add chunks for a specific document with detailed tracking\"\"\"\n",
        "\n",
        "        print(f\"📚 ADDING CHUNKS TO VECTOR STORE: {filename}\")\n",
        "        print(f\"   📊 Number of chunks: {len(chunks)}\")\n",
        "\n",
        "        embeddings = []\n",
        "        chunk_ids = []\n",
        "        metadatas = []\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            print(f\"   🔄 Processing chunk {i+1}/{len(chunks)}\")\n",
        "\n",
        "            # Generate embedding\n",
        "            embedding = self.embed_text(chunk, f\"{filename}_chunk_{i}\")\n",
        "            if not embedding:\n",
        "                print(f\"   ⚠️ Skipping chunk {i+1} - no embedding generated\")\n",
        "                continue\n",
        "\n",
        "            chunk_id = f\"{filename}_chunk_{i}\"\n",
        "            chunk_metadata = {\n",
        "                **metadata,\n",
        "                \"filename\": filename,\n",
        "                \"chunk_index\": i,\n",
        "                \"chunk_id\": chunk_id,\n",
        "                \"isolated\": True\n",
        "            }\n",
        "\n",
        "            embeddings.append(embedding)\n",
        "            chunk_ids.append(chunk_id)\n",
        "            metadatas.append(chunk_metadata)\n",
        "\n",
        "        # Add to ChromaDB\n",
        "        if embeddings:\n",
        "            self.collection.add(\n",
        "                embeddings=embeddings,\n",
        "                documents=chunks,\n",
        "                metadatas=metadatas,\n",
        "                ids=chunk_ids\n",
        "            )\n",
        "\n",
        "            print(f\"   ✅ Added {len(embeddings)} chunks to vector store\")\n",
        "        else:\n",
        "            print(f\"   ❌ No chunks added - all embeddings failed\")\n",
        "\n",
        "    def query_document_specific(self, query: str, filename: str, n_results: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"Query specific document only - prevents cross-contamination\"\"\"\n",
        "\n",
        "        print(f\"🔍 QUERYING VECTOR STORE: {filename}\")\n",
        "        print(f\"   ❓ Query: {query}\")\n",
        "        print(f\"   📊 Requesting {n_results} results\")\n",
        "\n",
        "        query_embedding = self.embed_text(query, f\"query_{filename}\")\n",
        "        if not query_embedding:\n",
        "            return {\"error\": \"Failed to generate query embedding\"}\n",
        "\n",
        "        # Query with filename filter to ensure isolation\n",
        "        results = self.collection.query(\n",
        "            query_embeddings=[query_embedding],\n",
        "            n_results=n_results,\n",
        "            where={\"filename\": filename},  # CRITICAL: Isolates to specific document\n",
        "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
        "        )\n",
        "\n",
        "        print(f\"   ✅ Found {len(results['documents'][0]) if results['documents'] else 0} relevant chunks\")\n",
        "\n",
        "        return {\n",
        "            \"documents\": results['documents'][0] if results['documents'] else [],\n",
        "            \"metadatas\": results['metadatas'][0] if results['metadatas'] else [],\n",
        "            \"distances\": results['distances'][0] if results['distances'] else [],\n",
        "            \"filename\": filename\n",
        "        }\n",
        "\n",
        "    def get_collection_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get detailed statistics about stored documents\"\"\"\n",
        "\n",
        "        print(\"📊 GENERATING COLLECTION STATISTICS\")\n",
        "\n",
        "        count = self.collection.count()\n",
        "\n",
        "        # Get unique filenames\n",
        "        all_metadata = self.collection.get(include=[\"metadatas\"])\n",
        "        filenames = set()\n",
        "        if all_metadata and all_metadata['metadatas']: # Added check for all_metadata existence\n",
        "            for meta in all_metadata['metadatas']:\n",
        "                if 'filename' in meta:\n",
        "                    filenames.add(meta['filename'])\n",
        "\n",
        "        stats = {\n",
        "            \"total_chunks\": count,\n",
        "            \"unique_documents\": len(filenames),\n",
        "            \"filenames\": list(filenames)\n",
        "        }\n",
        "\n",
        "        print(f\"   📚 Total chunks: {stats['total_chunks']}\")\n",
        "        print(f\"   📄 Unique documents: {stats['unique_documents']}\")\n",
        "        print(f\"   📝 Documents: {', '.join(stats['filenames'])}\")\n",
        "\n",
        "        return stats\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - ENHANCED VECTOR STORE!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ================================\n",
        "# STEP 6: ENHANCED EXPENSE TASK MANAGER (Re-definition)\n",
        "# ================================\n",
        "\n",
        "class EnhancedExpenseTaskManager:\n",
        "    \"\"\"Manages predefined expense extraction tasks with better tracking\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.predefined_tasks = {\n",
        "            \"extract_amount\": {\n",
        "                \"query\": \"total amount due payment cost price sum money dollar\",\n",
        "                \"description\": \"Extract the total amount from this expense document\",\n",
        "                \"expected_format\": \"numeric value with currency\"\n",
        "            },\n",
        "            \"extract_date\": {\n",
        "                \"query\": \"date transaction purchase invoice receipt timestamp when\",\n",
        "                \"description\": \"Extract the date from this expense document\",\n",
        "                \"expected_format\": \"date in YYYY-MM-DD format\"\n",
        "            },\n",
        "            \"extract_vendor\": {\n",
        "                \"query\": \"vendor merchant company business supplier store restaurant hotel\",\n",
        "                \"description\": \"Extract vendor/merchant name from this expense document\",\n",
        "                \"expected_format\": \"company or business name\"\n",
        "            },\n",
        "            \"extract_category\": {\n",
        "                \"query\": \"category type classification expense kind service product item\",\n",
        "                \"description\": \"Determine expense category from this document\",\n",
        "                \"expected_format\": \"expense category classification\"\n",
        "            },\n",
        "            \"extract_items\": {\n",
        "                \"query\": \"items products services line items purchases description details\",\n",
        "                \"description\": \"Extract itemized details from this expense document\",\n",
        "                \"expected_format\": \"list of items or services\"\n",
        "            },\n",
        "            \"extract_tax\": {\n",
        "                \"query\": \"tax VAT GST sales tax tax rate percentage\",\n",
        "                \"description\": \"Extract tax information from this expense document\",\n",
        "                \"expected_format\": \"tax amount and rate\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def get_task_info(self, task_name: str) -> Dict[str, str]:\n",
        "        \"\"\"Get complete task information\"\"\"\n",
        "        return self.predefined_tasks.get(task_name, {})\n",
        "\n",
        "    def list_available_tasks(self) -> List[str]:\n",
        "        \"\"\"List all available extraction tasks\"\"\"\n",
        "        return list(self.predefined_tasks.keys())\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - ENHANCED EXPENSE TASK MANAGER!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "\n",
        "# ================================\n",
        "# STEP 7: ENHANCED RAG PROCESSOR (Re-definition)\n",
        "# ================================\n",
        "\n",
        "class EnhancedRAGExpenseProcessor:\n",
        "    \"\"\"RAG-based expense processor with comprehensive tracking\"\"\"\n",
        "\n",
        "    def __init__(self, text_model: str = \"gemma3:1b\"): # Use default or passed model\n",
        "        from langchain_ollama import ChatOllama\n",
        "\n",
        "        print(f\"🚀 INITIALIZING RAG EXPENSE PROCESSOR\")\n",
        "        print(f\"   🤖 Text Model: {text_model}\")\n",
        "\n",
        "        self.llm = ChatOllama(\n",
        "            model=text_model,\n",
        "            temperature=0.1,\n",
        "            base_url=\"http://127.0.0.1:11434\"\n",
        "        )\n",
        "\n",
        "        self.vector_store = EnhancedIsolatedVectorStore()\n",
        "        self.task_manager = EnhancedExpenseTaskManager()\n",
        "        self.document_manager = FilenameBasedDocumentManager()\n",
        "        self.ocr_processor = EnhancedOCRProcessor()\n",
        "\n",
        "        print(\"   ✅ All components initialized\")\n",
        "\n",
        "    def ingest_document(self, file_path: str) -> str:\n",
        "        \"\"\"INGESTION PHASE: Process document and store in vector DB\"\"\"\n",
        "\n",
        "        filename = Path(file_path).name\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"🔄 INGESTION PHASE STARTING\")\n",
        "        print(f\"📄 FILE: {filename}\")\n",
        "        print(f\"📁 PATH: {file_path}\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Step 1: OCR extraction\n",
        "        raw_text = self.ocr_processor.extract_text_from_document(file_path)\n",
        "        if not raw_text:\n",
        "            print(\"❌ INGESTION FAILED: No text extracted\")\n",
        "            return None\n",
        "\n",
        "        # Step 2: Register document with filename-based system\n",
        "        filename_id = self.document_manager.register_document(file_path, raw_text)\n",
        "\n",
        "        # Step 3: Get document context\n",
        "        document = self.document_manager.get_document_context(filename_id)\n",
        "\n",
        "        # Step 4: Store in vector database\n",
        "        metadata = {\n",
        "            **document.metadata,\n",
        "            \"ingestion_timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        self.vector_store.add_document_chunks(\n",
        "            filename=filename_id,\n",
        "            chunks=document.chunks,\n",
        "            metadata=metadata\n",
        "        )\n",
        "\n",
        "        print(f\"✅ INGESTION COMPLETED: {filename_id}\")\n",
        "        print(\"=\"*70)\n",
        "        return filename_id\n",
        "\n",
        "    def process_expense_task(self, filename: str, task_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"INFERENCE PHASE: Process specific task for document\"\"\"\n",
        "\n",
        "        print(f\"\\n🎯 INFERENCE PHASE STARTING\")\n",
        "        print(f\"📄 DOCUMENT: {filename}\")\n",
        "        print(f\"🎯 TASK: {task_name}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Step 1: Get task information\n",
        "        task_info = self.task_manager.get_task_info(task_name)\n",
        "        if not task_info:\n",
        "            return {\"error\": f\"Unknown task: {task_name}\"}\n",
        "\n",
        "        task_query = task_info.get(\"query\", \"\")\n",
        "        task_description = task_info.get(\"description\", \"\")\n",
        "\n",
        "        print(f\"📋 Task Description: {task_description}\")\n",
        "        print(f\"🔍 Search Query: {task_query}\")\n",
        "\n",
        "        # Step 2: Retrieve relevant chunks (ISOLATED to this document)\n",
        "        retrieval_results = self.vector_store.query_document_specific(\n",
        "            query=task_query,\n",
        "            filename=filename,\n",
        "            n_results=3\n",
        "        )\n",
        "\n",
        "        if retrieval_results.get(\"error\"):\n",
        "            return retrieval_results\n",
        "\n",
        "        # Step 3: Prepare optimized context\n",
        "        context = self.optimize_context(retrieval_results, task_name)\n",
        "\n",
        "        # Step 4: Generate response with LLM (WITH TOKEN TRACKING)\n",
        "        response_text, token_usage = self.generate_task_response_with_tracking(\n",
        "            context, task_name, task_description, filename\n",
        "        )\n",
        "\n",
        "        result = {\n",
        "            \"task\": task_name,\n",
        "            \"filename\": filename,\n",
        "            \"response\": response_text,\n",
        "            \"context_chunks_used\": len(retrieval_results[\"documents\"]),\n",
        "            \"token_usage\": token_usage\n",
        "        }\n",
        "\n",
        "        print(f\"✅ INFERENCE COMPLETED: {task_name} for {filename}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def optimize_context(self, retrieval_results: Dict[str, Any], task_name: str) -> str:\n",
        "        \"\"\"CONTEXT OPTIMIZATION: Reduce context overloading\"\"\"\n",
        "\n",
        "        documents = retrieval_results.get(\"documents\", [])\n",
        "        distances = retrieval_results.get(\"distances\", [])\n",
        "        filename = retrieval_results.get(\"filename\", \"unknown\")\n",
        "\n",
        "        print(f\"🔧 OPTIMIZING CONTEXT: {filename}\")\n",
        "        print(f\"   📊 Raw chunks: {len(documents)}\")\n",
        "\n",
        "        if not documents:\n",
        "            return \"No relevant context found\"\n",
        "\n",
        "        # Rank documents by relevance\n",
        "        doc_scores = list(zip(documents, distances))\n",
        "        doc_scores.sort(key=lambda x: x[1])\n",
        "\n",
        "        optimized_chunks = []\n",
        "        total_length = 0\n",
        "        max_context_length = 1500\n",
        "\n",
        "        for i, (doc, score) in enumerate(doc_scores):\n",
        "            # Remove document prefix from chunks\n",
        "            clean_doc = doc.replace(f\"[DOCUMENT: {filename}]\\n\", \"\")\n",
        "\n",
        "            if total_length + len(clean_doc) <= max_context_length:\n",
        "                optimized_chunks.append(clean_doc)\n",
        "                total_length += len(clean_doc)\n",
        "                print(f\"   ✅ Chunk {i+1}: {len(clean_doc)} chars (relevance: {score:.3f})\")\n",
        "            else:\n",
        "                remaining_space = max_context_length - total_length\n",
        "                if remaining_space > 100:\n",
        "                    truncated = clean_doc[:remaining_space] + \"...\"\n",
        "                    optimized_chunks.append(truncated)\n",
        "                    print(f\"   ✂️ Chunk {i+1}: truncated to {len(truncated)} chars\")\n",
        "                break\n",
        "\n",
        "        context = \"\\n\\n---\\n\\n\".join(optimized_chunks)\n",
        "        print(f\"   🎯 Final context: {len(context)} chars from {len(optimized_chunks)} chunks\")\n",
        "\n",
        "        return context\n",
        "\n",
        "    def generate_task_response_with_tracking(self, context: str, task_name: str, task_description: str, filename: str) -> Tuple[str, Dict[str, Any]]:\n",
        "        \"\"\"Generate LLM response with token usage tracking\"\"\"\n",
        "\n",
        "        print(f\"🤖 GENERATING LLM RESPONSE: {task_name} | {filename}\")\n",
        "\n",
        "        prompt = f\"\"\"You are an expert expense analyst. {task_description}\n",
        "\n",
        "CONTEXT FROM EXPENSE DOCUMENT ({filename}):\n",
        "{context}\n",
        "\n",
        "TASK: {task_name}\n",
        "INSTRUCTION: {task_description}\n",
        "\n",
        "Based ONLY on the context provided above, extract the requested information. Be precise and factual. If the information is not clearly present in the context, state \"Information not found in provided context.\"\n",
        "\n",
        "Response:\"\"\"\n",
        "\n",
        "        print(f\"   📝 Prompt length: {len(prompt)} characters\")\n",
        "\n",
        "        try:\n",
        "            response = self.llm.invoke(prompt)\n",
        "\n",
        "            # Track token usage\n",
        "            token_usage = token_tracker.track_call(\"llm_inference\", filename, task_name, response)\n",
        "\n",
        "            return response.content.strip(), token_usage\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error generating response: {e}\"\n",
        "            print(f\"   ❌ {error_msg}\")\n",
        "            return error_msg, {}\n",
        "\n",
        "    def process_all_tasks_for_document(self, filename: str) -> Dict[str, Any]:\n",
        "        \"\"\"Process all predefined tasks for a document\"\"\"\n",
        "\n",
        "        print(f\"\\n📊 PROCESSING ALL TASKS FOR: {filename}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        tasks = self.task_manager.list_available_tasks()\n",
        "        results = {}\n",
        "\n",
        "        for i, task in enumerate(tasks, 1):\n",
        "            print(f\"\\n[{i}/{len(tasks)}] Starting task: {task}\")\n",
        "            result = self.process_expense_task(filename, task)\n",
        "            results[task] = result\n",
        "\n",
        "        print(f\"\\n✅ ALL TASKS COMPLETED FOR: {filename}\")\n",
        "        return results\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - ENHANCED RAG PROCESSOR!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "\n",
        "# ================================\n",
        "# STEP 8: ENHANCED WORKFLOW (Re-definition)\n",
        "# ================================\n",
        "\n",
        "from langgraph.graph import StateGraph\n",
        "from typing import TypedDict\n",
        "\n",
        "class EnhancedRAGWorkflowState(TypedDict):\n",
        "    \"\"\"Enhanced state for RAG workflow\"\"\"\n",
        "    file_paths: List[str]\n",
        "    current_file_index: int\n",
        "    processed_filenames: List[str]\n",
        "    current_filename: str\n",
        "    task_results: Dict[str, Dict[str, Any]]\n",
        "    workflow_status: str\n",
        "    error: Optional[str]\n",
        "\n",
        "def create_enhanced_rag_workflow(processor: EnhancedRAGExpenseProcessor) -> StateGraph:\n",
        "    \"\"\"Create enhanced LangGraph workflow for RAG processing\"\"\"\n",
        "\n",
        "    def enhanced_ingestion_node(state: EnhancedRAGWorkflowState) -> EnhancedRAGWorkflowState:\n",
        "        \"\"\"Enhanced ingestion with detailed tracking\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"🔄 WORKFLOW: ENHANCED INGESTION PHASE STARTING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        file_paths = state.get(\"file_paths\", [])\n",
        "        processed_filenames = []\n",
        "\n",
        "        if not file_paths:\n",
        "            state[\"workflow_status\"] = \"ingestion_failed\"\n",
        "            state[\"error\"] = \"No file paths provided for ingestion.\"\n",
        "            print(\"❌ INGESTION FAILED: No file paths provided.\")\n",
        "            return state\n",
        "\n",
        "\n",
        "        for i, file_path in enumerate(file_paths, 1):\n",
        "            print(f\"\\n[{i}/{len(file_paths)}] Processing file: {Path(file_path).name}\")\n",
        "\n",
        "            try:\n",
        "                # Use the processor to ingest the document\n",
        "                filename = processor.ingest_document(file_path)\n",
        "                if filename:\n",
        "                    processed_filenames.append(filename)\n",
        "                    print(f\"✅ Successfully ingested: {filename}\")\n",
        "                else:\n",
        "                    print(f\"❌ Failed to ingest: {Path(file_path).name}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error ingesting {Path(file_path).name}: {e}\")\n",
        "                state[\"error\"] = str(e) # Store the error in state\n",
        "                # Decide if you want to stop on first ingestion error or continue\n",
        "                # For now, let's continue to process other files if possible\n",
        "                processed_filenames.append({\"error\": str(e), \"filename\": Path(file_path).name})\n",
        "\n",
        "\n",
        "        state[\"processed_filenames\"] = processed_filenames\n",
        "        state[\"workflow_status\"] = \"ingestion_complete\" if any(isinstance(f, str) for f in processed_filenames) else \"ingestion_failed\" # Check if at least one file was successfully processed\n",
        "\n",
        "        print(f\"\\n📊 INGESTION PHASE COMPLETED\")\n",
        "        successful_count = sum(1 for f in processed_filenames if isinstance(f, str))\n",
        "        print(f\"   ✅ Successfully processed: {successful_count} files\")\n",
        "        print(f\"   ❌ Failed: {len(file_paths) - successful_count} files\")\n",
        "\n",
        "\n",
        "        return state\n",
        "\n",
        "    def enhanced_task_processing_node(state: EnhancedRAGWorkflowState) -> EnhancedRAGWorkflowState:\n",
        "        \"\"\"Enhanced task processing with detailed tracking\"\"\"\n",
        "\n",
        "        print(\"\\n🎯 WORKFLOW: ENHANCED TASK PROCESSING PHASE STARTING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Filter out ingestion errors before processing tasks\n",
        "        processable_filenames = [f for f in state.get(\"processed_filenames\", []) if isinstance(f, str)]\n",
        "        task_results = state.get(\"task_results\", {}) # Initialize or get existing results\n",
        "\n",
        "        if not processable_filenames:\n",
        "            state[\"workflow_status\"] = \"processing_skipped\"\n",
        "            print(\"⚠️ TASK PROCESSING SKIPPED: No documents successfully ingested.\")\n",
        "            return state\n",
        "\n",
        "\n",
        "        for i, filename in enumerate(processable_filenames, 1):\n",
        "            print(f\"\\n[{i}/{len(processable_filenames)}] Processing tasks for: {filename}\")\n",
        "\n",
        "            try:\n",
        "                results = processor.process_all_tasks_for_document(filename)\n",
        "                task_results[filename] = results\n",
        "                print(f\"✅ Completed all tasks for: {filename}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing tasks for {filename}: {e}\")\n",
        "                task_results[filename] = {\"error\": str(e)}\n",
        "                state[\"error\"] = str(e) # Store the error in state\n",
        "\n",
        "\n",
        "        state[\"task_results\"] = task_results\n",
        "        state[\"workflow_status\"] = \"processing_complete\"\n",
        "\n",
        "        print(f\"\\n📊 TASK PROCESSING PHASE COMPLETED\")\n",
        "        print(f\"   📄 Documents processed: {len(task_results)}\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    def enhanced_results_compilation_node(state: EnhancedRAGWorkflowState) -> EnhancedRAGWorkflowState:\n",
        "        \"\"\"Enhanced results compilation with detailed stats and CSV export\"\"\"\n",
        "\n",
        "        print(\"\\n📊 WORKFLOW: ENHANCED RESULTS COMPILATION STARTING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        task_results = state.get(\"task_results\", {})\n",
        "        # token_tracker is a global instance assumed to be available\n",
        "        # processor instance (and its vector_store) is also assumed to be available\n",
        "\n",
        "        if not task_results:\n",
        "             state[\"workflow_status\"] = \"compilation_skipped\"\n",
        "             print(\"⚠️ RESULTS COMPILATION SKIPPED: No task results to compile.\")\n",
        "             token_tracker.print_summary() # Print summary even if compilation skipped\n",
        "             return state\n",
        "\n",
        "\n",
        "        # Compile detailed statistics\n",
        "        total_documents_attempted_ingestion = len(state.get(\"file_paths\", [])) # Count original files\n",
        "        successfully_ingested_filenames = [f for f in state.get(\"processed_filenames\", []) if isinstance(f, str)]\n",
        "        total_documents_successfully_processed = len(task_results) # Count documents with task results (successful or not)\n",
        "\n",
        "\n",
        "        # Save results with timestamp\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        results_file = f\"enhanced_rag_expense_results_{timestamp}.json\"\n",
        "        csv_file = f\"enhanced_rag_expense_results_{timestamp}.csv\"\n",
        "\n",
        "        # Create comprehensive results package\n",
        "        comprehensive_results = {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"summary\": {\n",
        "                \"total_documents_attempted_ingestion\": total_documents_attempted_ingestion,\n",
        "                \"successfully_ingested_documents\": len(successfully_ingested_filenames),\n",
        "                \"documents_with_task_results\": total_documents_successfully_processed,\n",
        "                \"failed_ingestion\": total_documents_attempted_ingestion - len(successfully_ingested_filenames)\n",
        "\n",
        "            },\n",
        "            \"token_usage_summary\": {\n",
        "                \"total_calls\": token_tracker.call_count,\n",
        "                \"total_input_tokens\": token_tracker.total_input_tokens,\n",
        "                \"total_output_tokens\": token_tracker.total_output_tokens, # Fixed typo here\n",
        "                \"total_tokens\": token_tracker.total_tokens\n",
        "            },\n",
        "            \"document_results\": task_results,\n",
        "            \"token_call_history\": token_tracker.call_history,\n",
        "            \"initial_state\": state.get(\"initial_state\", {}) # Include initial state for debugging\n",
        "        }\n",
        "\n",
        "        # Save JSON results\n",
        "        try:\n",
        "            with open(results_file, 'w') as f:\n",
        "                json.dump(comprehensive_results, f, indent=2, default=str)\n",
        "            print(f\"💾 JSON RESULTS SAVED TO: {results_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error saving JSON results: {e}\")\n",
        "            state[\"error\"] = f\"Error saving JSON results: {e}\"\n",
        "\n",
        "\n",
        "        # Create CSV from results\n",
        "        csv_rows = []\n",
        "\n",
        "        # Add rows for documents that failed ingestion\n",
        "        failed_ingestion_info = [f for f in state.get(\"processed_filenames\", []) if isinstance(f, dict) and \"error\" in f]\n",
        "        for fail_info in failed_ingestion_info:\n",
        "             csv_rows.append({\n",
        "                \"filename\": fail_info.get(\"filename\", \"unknown\"),\n",
        "                \"task\": \"ingestion_failed\",\n",
        "                \"response\": fail_info.get(\"error\", \"Unknown ingestion error\"),\n",
        "                \"context_chunks_used\": 0,\n",
        "                \"input_tokens\": 0,\n",
        "                \"output_tokens\": 0,\n",
        "                \"total_tokens\": 0\n",
        "            })\n",
        "\n",
        "\n",
        "        for filename, doc_results in task_results.items():\n",
        "            if \"error\" in doc_results:\n",
        "                # Add error row for document that failed task processing after successful ingestion\n",
        "                csv_rows.append({\n",
        "                    \"filename\": filename,\n",
        "                    \"task\": \"processing_failed\",\n",
        "                    \"response\": doc_results[\"error\"],\n",
        "                    \"context_chunks_used\": 0,\n",
        "                    \"input_tokens\": 0,\n",
        "                    \"output_tokens\": 0,\n",
        "                    \"total_tokens\": 0\n",
        "                })\n",
        "            else:\n",
        "                # Process each task for this document\n",
        "                for task_name, task_result in doc_results.items():\n",
        "                    if isinstance(task_result, dict):\n",
        "                        token_usage = task_result.get(\"token_usage\", {})\n",
        "                        csv_rows.append({\n",
        "                            \"filename\": filename,\n",
        "                            \"task\": task_name,\n",
        "                            \"response\": task_result.get(\"response\", \"\"),\n",
        "                            \"context_chunks_used\": task_result.get(\"context_chunks_used\", 0),\n",
        "                            \"input_tokens\": token_usage.get(\"input_tokens\", 0),\n",
        "                            \"output_tokens\": token_usage.get(\"output_tokens\", 0),\n",
        "                            \"total_tokens\": token_usage.get(\"total_tokens\", 0)\n",
        "                        })\n",
        "                    else:\n",
        "                         # Handle task-specific errors (if task_result is not a dict but an error string)\n",
        "                        csv_rows.append({\n",
        "                            \"filename\": filename,\n",
        "                            \"task\": task_name,\n",
        "                            \"response\": f\"Task error: {task_result}\",\n",
        "                            \"context_chunks_used\": 0,\n",
        "                            \"input_tokens\": 0,\n",
        "                            \"output_tokens\": 0,\n",
        "                            \"total_tokens\": 0\n",
        "                        })\n",
        "\n",
        "\n",
        "        # Save CSV\n",
        "        df = pd.DataFrame(csv_rows) # Create DataFrame even if empty\n",
        "\n",
        "        if not df.empty: # Check if DataFrame is not empty before processing\n",
        "            try:\n",
        "                # Reorder columns for better readability - handle missing columns gracefully\n",
        "                column_order = [\n",
        "                    \"filename\", \"task\", \"response\",\n",
        "                    \"context_chunks_used\", \"input_tokens\",\n",
        "                    \"output_tokens\", \"total_tokens\"\n",
        "                ]\n",
        "                existing_columns = [col for col in column_order if col in df.columns]\n",
        "                df = df[existing_columns]\n",
        "\n",
        "\n",
        "                # Convert token columns to numeric, handling errors\n",
        "                for token_col in ['input_tokens', 'output_tokens', 'total_tokens']:\n",
        "                     if token_col in df.columns:\n",
        "                        df[token_col] = pd.to_numeric(df[token_col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "\n",
        "                # Save to CSV\n",
        "                df.to_csv(csv_file, index=False, encoding='utf-8')\n",
        "                print(f\"💾 CSV RESULTS SAVED TO: {csv_file}\")\n",
        "\n",
        "                # Display summary statistics from CSV\n",
        "                print(f\"\\n📊 CSV Summary:\")\n",
        "                print(f\"   📄 Total rows: {len(df)}\")\n",
        "                # Ensure 'filename' column exists before calling nunique\n",
        "                if 'filename' in df.columns:\n",
        "                    print(f\"   📁 Documents: {df['filename'].nunique()}\")\n",
        "                     # Handle case where no tasks were processed successfully\n",
        "                    successful_task_rows = df[~df['task'].isin(['ingestion_failed', 'processing_failed', 'workflow_error'])]\n",
        "                    if not successful_task_rows.empty and 'filename' in successful_task_rows.columns:\n",
        "                         print(f\"   🎯 Avg tasks per successful doc: {successful_task_rows.groupby('filename').size().mean():.1f}\")\n",
        "                    else:\n",
        "                        print(\"   🎯 Avg tasks per successful doc: N/A (No successful tasks)\")\n",
        "\n",
        "                # Ensure 'total_tokens' column exists and is numeric before summing\n",
        "                if 'total_tokens' in df.columns:\n",
        "                    try:\n",
        "                        total_tokens_sum = df['total_tokens'].sum()\n",
        "                        print(f\"   🔢 Total tokens used: {total_tokens_sum:,}\")\n",
        "                         # Update comprehensive_results with the sum from CSV if needed\n",
        "                        comprehensive_results['token_usage_summary']['total_tokens_from_csv'] = total_tokens_sum\n",
        "\n",
        "                    except Exception as e:\n",
        "                         print(f\"⚠️ Could not calculate total tokens from CSV: {e}\")\n",
        "                else:\n",
        "                    print(\"⚠️ 'total_tokens' column not found in CSV.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing or saving CSV results: {e}\")\n",
        "                state[\"error\"] = f\"Error processing or saving CSV results: {e}\"\n",
        "\n",
        "        else:\n",
        "             print(\"⚠️ No CSV rows generated. Skipping CSV save.\")\n",
        "\n",
        "\n",
        "        # Also save a summary CSV with aggregated data per document\n",
        "        summary_csv_file = f\"enhanced_rag_expense_summary_{timestamp}.csv\"\n",
        "        summary_rows = []\n",
        "\n",
        "        # Include documents that failed ingestion in summary\n",
        "        for fail_info in failed_ingestion_info:\n",
        "            summary_rows.append({\n",
        "                \"filename\": fail_info.get(\"filename\", \"unknown\"),\n",
        "                \"status\": \"Ingestion Failed\",\n",
        "                \"error_message\": fail_info.get(\"error\", \"Unknown error\")\n",
        "            })\n",
        "\n",
        "\n",
        "        for filename, doc_results in task_results.items():\n",
        "            if \"error\" not in doc_results:\n",
        "                row = {\"filename\": filename, \"status\": \"Processed\"}\n",
        "\n",
        "                # Extract key information from each task\n",
        "                for task_name in [\"extract_amount\", \"extract_date\", \"extract_vendor\",\n",
        "                                \"extract_category\", \"extract_tax\", \"extract_items\", \"extract_tax\"]: # Include all relevant tasks\n",
        "                    if task_name in doc_results and isinstance(doc_results[task_name], dict):\n",
        "                        response = doc_results[task_name].get(\"response\", \"\")\n",
        "                        # Clean the response (take first line or first 100 chars)\n",
        "                        cleaned = response.split('\\n')[0][:100] if response else \"\"\n",
        "                        row[task_name] = cleaned\n",
        "                    elif task_name in doc_results:\n",
        "                         # Handle task-specific errors\n",
        "                         row[task_name] = f\"Error: {doc_results[task_name]}\"\n",
        "                    else:\n",
        "                        row[task_name] = \"Task Not Run\"\n",
        "\n",
        "\n",
        "                # Add token totals\n",
        "                total_tokens = sum(\n",
        "                    doc_results.get(task, {}).get(\"token_usage\", {}).get(\"total_tokens\", 0)\n",
        "                    for task in doc_results if isinstance(doc_results.get(task), dict)\n",
        "                )\n",
        "                row[\"total_tokens_used\"] = total_tokens\n",
        "\n",
        "                summary_rows.append(row)\n",
        "            else:\n",
        "                 # Add document that failed task processing after ingestion\n",
        "                 summary_rows.append({\n",
        "                     \"filename\": filename,\n",
        "                     \"status\": \"Task Processing Failed\",\n",
        "                     \"error_message\": doc_results[\"error\"]\n",
        "                 })\n",
        "\n",
        "\n",
        "        if summary_rows:\n",
        "            try:\n",
        "                summary_df = pd.DataFrame(summary_rows)\n",
        "                # Ensure 'status' and 'error_message' columns exist and are placed early\n",
        "                summary_column_order = [\"filename\", \"status\", \"error_message\"] + [col for col in summary_df.columns if col not in [\"filename\", \"status\", \"error_message\"]]\n",
        "                summary_df = summary_df.get(summary_column_order, summary_df) # Use .get to handle missing columns\n",
        "\n",
        "                summary_df.to_csv(summary_csv_file, index=False, encoding='utf-8')\n",
        "                print(f\"💾 SUMMARY CSV SAVED TO: {summary_csv_file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error saving summary CSV: {e}\")\n",
        "                state[\"error\"] = f\"Error saving summary CSV: {e}\"\n",
        "\n",
        "\n",
        "        print(f\"\\n📊 FILES SAVED:\")\n",
        "        if os.path.exists(results_file): print(f\"   📄 Detailed JSON: {results_file}\")\n",
        "        if os.path.exists(csv_file) and not df.empty: print(f\"   📄 Detailed CSV: {csv_file}\")\n",
        "        if os.path.exists(summary_csv_file) and summary_rows: print(f\"   📄 Summary CSV: {summary_csv_file}\")\n",
        "        print(f\"   📄 Documents processed successfully (ingestion+tasks): {total_documents_successfully_processed}/{total_documents_attempted_ingestion}\")\n",
        "\n",
        "        # Print token usage summary\n",
        "        token_tracker.print_summary()\n",
        "\n",
        "        state[\"workflow_status\"] = \"complete\"\n",
        "        return state\n",
        "\n",
        "\n",
        "    # Build enhanced workflow\n",
        "    workflow = StateGraph(EnhancedRAGWorkflowState)\n",
        "\n",
        "    workflow.add_node(\"enhanced_ingestion\", enhanced_ingestion_node)\n",
        "    workflow.add_node(\"enhanced_task_processing\", enhanced_task_processing_node)\n",
        "    workflow.add_node(\"enhanced_results_compilation\", enhanced_results_compilation_node)\n",
        "\n",
        "    workflow.set_entry_point(\"enhanced_ingestion\")\n",
        "    workflow.add_edge(\"enhanced_ingestion\", \"enhanced_task_processing\")\n",
        "    workflow.add_edge(\"enhanced_task_processing\", \"enhanced_results_compilation\")\n",
        "    workflow.set_finish_point(\"enhanced_results_compilation\")\n",
        "\n",
        "    return workflow.compile()\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - ENHANCED WORKFLOW!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "\n",
        "# Initialize the RAG processor and workflow globally for efficiency in Gradio\n",
        "# This avoids re-initializing the processor (and thus ChromaDB client/collection)\n",
        "# on every file upload in the Gradio app.\n",
        "\n",
        "# Ensure the TokenUsageTracker is also a persistent instance\n",
        "try:\n",
        "    # Re-initialize token_tracker to clear previous runs' data if desired,\n",
        "    # or let it accumulate total usage across all runs.\n",
        "    # For this demo, let's re-initialize for per-file/per-run tracking clarity in UI.\n",
        "    token_tracker = TokenUsageTracker()\n",
        "    processor = EnhancedRAGExpenseProcessor()\n",
        "    # Create the workflow instance\n",
        "    enhanced_rag_workflow = create_enhanced_rag_workflow(processor)\n",
        "    print(\"\\n✅ RAG Processor and Workflow initialized globally.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Error initializing RAG components globally: {e}\")\n",
        "    processor = None # Set to None if initialization fails\n",
        "    enhanced_rag_workflow = None # Set to None if initialization fails\n",
        "\n",
        "\n",
        "# Update the Gradio UI function to call the workflow\n",
        "def process_document_ui(file):\n",
        "    \"\"\"Function to receive the uploaded file, run the workflow, and display results.\"\"\"\n",
        "    # Use the globally initialized workflow and processor\n",
        "    global enhanced_rag_workflow, processor, token_tracker # Added token_tracker here\n",
        "\n",
        "    if file is None:\n",
        "        return \"Please upload a file.\", \"No file uploaded.\", pd.DataFrame() # Return empty DataFrame\n",
        "\n",
        "    file_path = file.name # Gradio provides the temporary path here\n",
        "\n",
        "    status = f\"Received file: {file_path}. Starting RAG workflow...\"\n",
        "    print(status) # Print status to console\n",
        "\n",
        "    # Re-initialize token tracker for this specific run to track usage per file upload\n",
        "    token_tracker = TokenUsageTracker()\n",
        "    print(\"\\n📊 Token usage tracker reset for new upload.\")\n",
        "\n",
        "\n",
        "    # Check if workflow initialization was successful\n",
        "    if enhanced_rag_workflow is None:\n",
        "         error_msg = \"RAG Workflow failed to initialize during startup. Cannot process file.\"\n",
        "         print(f\"❌ {error_msg}\")\n",
        "         return error_msg, \"RAG components failed to initialize. Check server logs.\", pd.DataFrame()\n",
        "\n",
        "\n",
        "    # Execute enhanced workflow with the uploaded file path\n",
        "    initial_state = {\n",
        "        \"file_paths\": [file_path], # Pass the uploaded file path as a list\n",
        "        \"current_file_index\": 0, # Not strictly used with single file processing, but keep for state structure\n",
        "        \"processed_filenames\": [],\n",
        "        \"current_filename\": \"\", # Not strictly used with single file processing\n",
        "        \"task_results\": {},\n",
        "        \"workflow_status\": \"initialized\",\n",
        "        \"error\": None,\n",
        "        \"initial_state\": {\"file_paths\": [file_path]} # Store initial state for compilation node\n",
        "    }\n",
        "\n",
        "    final_state = None # Initialize final_state to None\n",
        "    try:\n",
        "        print(f\"Executing workflow with state: {initial_state}\")\n",
        "        final_state = enhanced_rag_workflow.invoke(initial_state)\n",
        "        workflow_final_status = final_state.get('workflow_status', 'unknown')\n",
        "        status = f\"Workflow completed with status: {workflow_final_status}\"\n",
        "        print(status) # Print final status\n",
        "        print(f\"Final state: {final_state}\") # Print final state for debugging\n",
        "\n",
        "\n",
        "        # Process final state to display results\n",
        "        task_results = final_state.get(\"task_results\", {})\n",
        "        compiled_details = \"\"\n",
        "        results_df = pd.DataFrame() # Default to empty DataFrame\n",
        "\n",
        "\n",
        "        if final_state.get(\"error\"):\n",
        "             compiled_details = f\"Workflow Error: {final_state['error']}\\n\\n\"\n",
        "             # Attempt to display any partial results if available\n",
        "             if task_results:\n",
        "                 compiled_details += \"Partial Results:\\n\"\n",
        "                 for filename, doc_results in task_results.items():\n",
        "                      compiled_details += f\"\\n📄 Document: {filename}\\n\"\n",
        "                      if isinstance(doc_results, dict) and \"error\" in doc_results:\n",
        "                           compiled_details += f\"  ❌ Error: {doc_results['error']}\\n\"\n",
        "                      elif isinstance(doc_results, dict):\n",
        "                          for task_name, task_result in doc_results.items():\n",
        "                              if isinstance(task_result, dict):\n",
        "                                   response_preview = task_result.get('response', 'No response')\n",
        "                                   compiled_details += f\"  🎯 {task_name}: {response_preview[:100]}...\\n\"\n",
        "                              else:\n",
        "                                  compiled_details += f\"  🎯 {task_name}: Error - {task_result}\\n\"\n",
        "                      else:\n",
        "                           compiled_details += f\"  ❌ Document processing failed: {doc_results}\\n\"\n",
        "\n",
        "\n",
        "             # If there are partial results that can be put in a DataFrame, try that\n",
        "             if task_results or final_state.get(\"processed_filenames\"): # Include ingestion errors\n",
        "                  try:\n",
        "                     csv_rows = []\n",
        "                     # Add ingestion errors first\n",
        "                     failed_ingestion_info = [f for f in final_state.get(\"processed_filenames\", []) if isinstance(f, dict) and \"error\" in f]\n",
        "                     for fail_info in failed_ingestion_info:\n",
        "                          csv_rows.append({\n",
        "                            \"filename\": fail_info.get(\"filename\", \"unknown\"),\n",
        "                            \"task\": \"ingestion_failed\",\n",
        "                            \"response\": fail_info.get(\"error\", \"Unknown ingestion error\"),\n",
        "                            \"context_chunks_used\": 0,\n",
        "                            \"input_tokens\": 0,\n",
        "                            \"output_tokens\": 0,\n",
        "                            \"total_tokens\": 0\n",
        "                        })\n",
        "\n",
        "                     # Add task processing results/errors\n",
        "                     for filename, doc_results in task_results.items():\n",
        "                         if isinstance(doc_results, dict):\n",
        "                            if \"error\" in doc_results:\n",
        "                                csv_rows.append({\n",
        "                                    \"filename\": filename,\n",
        "                                    \"task\": \"processing_failed\",\n",
        "                                    \"response\": doc_results[\"error\"],\n",
        "                                    \"context_chunks_used\": 0,\n",
        "                                    \"input_tokens\": 0,\n",
        "                                    \"output_tokens\": 0,\n",
        "                                    \"total_tokens\": 0\n",
        "                                })\n",
        "                            else:\n",
        "                                for task_name, task_result in doc_results.items():\n",
        "                                    if isinstance(task_result, dict):\n",
        "                                        token_usage = task_result.get(\"token_usage\", {})\n",
        "                                        csv_rows.append({\n",
        "                                            \"filename\": filename,\n",
        "                                            \"task\": task_name,\n",
        "                                            \"response\": task_result.get(\"response\", \"\"),\n",
        "                                            \"context_chunks_used\": task_result.get(\"context_chunks_used\", 0),\n",
        "                                            \"input_tokens\": token_usage.get(\"input_tokens\", 0),\n",
        "                                            \"output_tokens\": token_usage.get(\"output_tokens\", 0),\n",
        "                                            \"total_tokens\": token_usage.get(\"total_tokens\", 0)\n",
        "                                        })\n",
        "                                    else:\n",
        "                                         csv_rows.append({\n",
        "                                            \"filename\": filename,\n",
        "                                            \"task\": task_name,\n",
        "                                            \"response\": f\"Task error: {task_result}\",\n",
        "                                            \"context_chunks_used\": 0,\n",
        "                                            \"input_tokens\": 0,\n",
        "                                            \"output_tokens\": 0,\n",
        "                                            \"total_tokens\": 0\n",
        "                                        })\n",
        "                         else:\n",
        "                              csv_rows.append({\n",
        "                                    \"filename\": filename,\n",
        "                                    \"task\": \"processing_failed\",\n",
        "                                    \"response\": f\"Document processing failed: {doc_results}\",\n",
        "                                    \"context_chunks_used\": 0,\n",
        "                                    \"input_tokens\": 0,\n",
        "                                    \"output_tokens\": 0,\n",
        "                                    \"total_tokens\": 0\n",
        "                                })\n",
        "\n",
        "\n",
        "                     if csv_rows:\n",
        "                        results_df = pd.DataFrame(csv_rows)\n",
        "                        column_order = [\n",
        "                            \"filename\", \"task\", \"response\",\n",
        "                            \"context_chunks_used\", \"input_tokens\",\n",
        "                            \"output_tokens\", \"total_tokens\"\n",
        "                        ]\n",
        "                        existing_columns = [col for col in column_order if col in results_df.columns]\n",
        "                        results_df = results_df[existing_columns]\n",
        "                        for token_col in ['input_tokens', 'output_tokens', 'total_tokens']:\n",
        "                             if token_col in results_df.columns:\n",
        "                                results_df[token_col] = pd.to_numeric(results_df[token_col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "                  except Exception as csv_e:\n",
        "                       compiled_details += f\"\\nError compiling partial results DataFrame: {csv_e}\"\n",
        "                       results_df = pd.DataFrame({\"Error\": [f\"Could not compile results: {csv_e}\"]}) # Indicate error in DataFrame too\n",
        "\n",
        "\n",
        "             return status, compiled_details, results_df # Return DataFrame even on error\n",
        "\n",
        "\n",
        "        # Compile results for display if no workflow-level error\n",
        "        if task_results:\n",
        "            compiled_details = \"\"\n",
        "            csv_rows = [] # Prepare data for DataFrame display\n",
        "\n",
        "            # Add ingestion errors first\n",
        "            failed_ingestion_info = [f for f in final_state.get(\"processed_filenames\", []) if isinstance(f, dict) and \"error\" in f]\n",
        "            for fail_info in failed_ingestion_info:\n",
        "                 csv_rows.append({\n",
        "                    \"filename\": fail_info.get(\"filename\", \"unknown\"),\n",
        "                    \"task\": \"ingestion_failed\",\n",
        "                    \"response\": fail_info.get(\"error\", \"Unknown ingestion error\"),\n",
        "                    \"context_chunks_used\": 0,\n",
        "                    \"input_tokens\": 0,\n",
        "                    \"output_tokens\": 0,\n",
        "                    \"total_tokens\": 0\n",
        "                })\n",
        "\n",
        "\n",
        "            for filename, results in task_results.items():\n",
        "                 compiled_details += f\"\\n📄 DOCUMENT: {filename}\\n\" + \"─\" * 50 + \"\\n\"\n",
        "                 if isinstance(results, dict) and \"error\" in results:\n",
        "                     compiled_details += f\"❌ Document processing failed: {results['error']}\\n\"\n",
        "                     csv_rows.append({\n",
        "                        \"filename\": filename,\n",
        "                        \"task\": \"processing_failed\",\n",
        "                        \"response\": results[\"error\"],\n",
        "                        \"context_chunks_used\": 0,\n",
        "                        \"input_tokens\": 0,\n",
        "                        \"output_tokens\": 0,\n",
        "                        \"total_tokens\": 0\n",
        "                    })\n",
        "                 elif isinstance(results, dict):\n",
        "                    for task_name, task_result in results.items():\n",
        "                        if isinstance(task_result, dict):\n",
        "                            response = task_result.get(\"response\", \"No response\")\n",
        "                            chunks_used = task_result.get(\"context_chunks_used\", 0)\n",
        "                            token_usage = task_result.get(\"token_usage\", {})\n",
        "\n",
        "                            compiled_details += f\"\\n🎯 {task_name.upper()}:\\n\"\n",
        "                            compiled_details += f\"   📝 Response: {response[:200]}...\\n\" # Limit display length\n",
        "                            compiled_details += f\"   📚 Chunks used: {chunks_used}\\n\"\n",
        "                            if token_usage:\n",
        "                                compiled_details += f\"   🔢 Tokens: {token_usage.get('total_tokens', 0)}\\n\"\n",
        "\n",
        "                            csv_rows.append({\n",
        "                                \"filename\": filename,\n",
        "                                \"task\": task_name,\n",
        "                                \"response\": response,\n",
        "                                \"context_chunks_used\": chunks_used,\n",
        "                                \"input_tokens\": token_usage.get(\"input_tokens\", 0),\n",
        "                                \"output_tokens\": token_usage.get(\"output_tokens\", 0),\n",
        "                                \"total_tokens\": token_usage.get(\"total_tokens\", 0)\n",
        "                            })\n",
        "                        else:\n",
        "                             compiled_details += f\"\\n🎯 {task_name.upper()}:\\n\"\n",
        "                             compiled_details += f\"   ❌ Task Error: {task_result}\\n\"\n",
        "                             csv_rows.append({\n",
        "                                \"filename\": filename,\n",
        "                                \"task\": task_name,\n",
        "                                \"response\": f\"Task error: {task_result}\",\n",
        "                                \"context_chunks_used\": 0,\n",
        "                                \"input_tokens\": 0,\n",
        "                                \"output_tokens\": 0,\n",
        "                                \"total_tokens\": 0\n",
        "                            })\n",
        "                 else:\n",
        "                     compiled_details += f\"❌ Document processing failed with non-dict result: {results}\\n\"\n",
        "                     csv_rows.append({\n",
        "                        \"filename\": filename,\n",
        "                        \"task\": \"processing_failed_unknown\",\n",
        "                        \"response\": f\"Processing failed with unexpected result type: {results}\",\n",
        "                        \"context_chunks_used\": 0,\n",
        "                        \"input_tokens\": 0,\n",
        "                        \"output_tokens\": 0,\n",
        "                        \"total_tokens\": 0\n",
        "                    })\n",
        "\n",
        "\n",
        "            # Create DataFrame for the summary output\n",
        "            df = pd.DataFrame(csv_rows) # Create DataFrame even if empty\n",
        "\n",
        "            if not df.empty:\n",
        "                results_df = df\n",
        "                column_order = [\n",
        "                    \"filename\", \"task\", \"response\",\n",
        "                    \"context_chunks_used\", \"input_tokens\",\n",
        "                    \"output_tokens\", \"total_tokens\"\n",
        "                ]\n",
        "                existing_columns = [col for col in column_order if col in results_df.columns]\n",
        "                results_df = results_df[existing_columns]\n",
        "                for token_col in ['input_tokens', 'output_tokens', 'total_tokens']:\n",
        "                     if token_col in results_df.columns:\n",
        "                        results_df[token_col] = pd.to_numeric(results_df[token_col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "            else:\n",
        "                 results_df = pd.DataFrame({\"Status\": [\"No task results or errors generated.\"]})\n",
        "\n",
        "\n",
        "            # Display token usage summary in the details output as well\n",
        "            compiled_details += \"\\n\\n📊 TOTAL TOKEN USAGE SUMMARY FOR THIS RUN:\\n\"\n",
        "            compiled_details += f\"🔢 Total LLM Calls: {token_tracker.call_count}\\n\"\n",
        "            compiled_details += f\"📥 Total Input Tokens: {token_tracker.total_input_tokens:,}\\n\"\n",
        "            compiled_details += f\"📤 Total Output Tokens: {token_tracker.total_output_tokens:,}\\n\"\n",
        "            compiled_details += f\"🎯 Grand Total Tokens: {token_tracker.total_tokens:,}\\n\"\n",
        "            if token_tracker.call_count > 0:\n",
        "                compiled_details += f\"📊 Average per call: {token_tracker.total_tokens/token_tracker.call_count:.1f} tokens\\n\"\n",
        "\n",
        "\n",
        "            return status, compiled_details, results_df # Return DataFrame for display\n",
        "\n",
        "        else:\n",
        "            # If task_results is empty but no workflow error, it means ingestion might have failed for all\n",
        "            ingestion_errors = [f for f in final_state.get(\"processed_filenames\", []) if isinstance(f, dict) and \"error\" in f]\n",
        "            if ingestion_errors:\n",
        "                compiled_details = \"Ingestion failed for the uploaded file:\\n\"\n",
        "                for err_info in ingestion_errors:\n",
        "                     compiled_details += f\"- {err_info.get('filename', 'unknown')}: {err_info.get('error', 'Unknown error')}\\n\"\n",
        "                return status, compiled_details, pd.DataFrame({\"Status\": [\"Ingestion Failed\"]})\n",
        "\n",
        "            return status, \"No task results generated and no specific errors reported.\", pd.DataFrame() # Return empty DataFrame if no task results\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"An unexpected workflow execution failed: {e}\"\n",
        "        print(f\"❌ {error_msg}\")\n",
        "        # Attempt to capture state before the crash if possible\n",
        "        debug_details = f\"An unexpected error occurred: {e}\"\n",
        "        if 'final_state' in locals() and final_state:\n",
        "             debug_details += f\"\\nPartial workflow state available. Status: {final_state.get('workflow_status', 'unknown')}\"\n",
        "             if final_state.get('error'):\n",
        "                  debug_details += f\"\\nInternal state error: {final_state['error']}\"\n",
        "             if final_state.get('task_results'):\n",
        "                  debug_details += f\"\\nPartial task results available for {len(final_state['task_results'])} documents.\"\n",
        "             if final_state.get('processed_filenames'):\n",
        "                   debug_details += f\"\\nProcessed filenames state: {final_state['processed_filenames']}\"\n",
        "\n",
        "\n",
        "        # Attempt to create a simple error DataFrame\n",
        "        error_df = pd.DataFrame({\"Workflow Error\": [error_msg], \"Details\": [debug_details]})\n",
        "\n",
        "        return \"Workflow Failed\", debug_details, error_df\n",
        "\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Enhanced RAG-Based Expense Claims Processing\")\n",
        "    gr.Markdown(\"Upload your expense claim document (PDF, JPG, PNG, etc.) to extract details.\")\n",
        "\n",
        "    file_input = gr.File(label=\"Upload Expense Document\", file_types=[\".pdf\", \".jpg\", \".jpeg\", \".png\", \".tiff\"])\n",
        "    process_button = gr.Button(\"Process Document\")\n",
        "\n",
        "    status_output = gr.Textbox(label=\"Processing Status\", max_lines=5)\n",
        "    details_output = gr.Textbox(label=\"Extracted Details (Raw)\", interactive=False, lines=20)\n",
        "    summary_output = gr.Dataframe(label=\"Extracted Details (Structured)\") # Use Dataframe for structured output\n",
        "\n",
        "    process_button.click(\n",
        "        process_document_ui,\n",
        "        inputs=file_input,\n",
        "        outputs=[status_output, details_output, summary_output]\n",
        "    )\n",
        "\n",
        "# Launch the Gradio interface\n",
        "demo.launch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9297e576"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback indicates that `dataclass` is not defined. Looking back at the history, `dataclass` is imported from the `dataclasses` module in the first code cell of the notebook. This confirms that the execution environment has been reset and previous imports are lost. I need to re-import the necessary modules, specifically `dataclasses`, and then re-execute the code block that defines all the classes and functions, including the global instances and the Gradio interface.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ynkbN2HXx4c"
      },
      "source": [
        "# Re-import necessary modules\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from dataclasses import dataclass, asdict # Re-import dataclass\n",
        "import warnings\n",
        "import uuid\n",
        "# Re-import other modules used in the definitions if necessary, based on previous cell imports\n",
        "import chromadb\n",
        "from unstructured.partition.auto import partition # Needed for EnhancedOCRProcessor\n",
        "from langchain_ollama import ChatOllama # Needed for EnhancedRAGExpenseProcessor\n",
        "import ollama # Needed for embedding and testing connection\n",
        "\n",
        "# Re-execute code from previous cells to ensure all necessary components are defined.\n",
        "# This includes TokenUsageTracker, ClaimDocument, FilenameBasedDocumentManager,\n",
        "# EnhancedOCRProcessor, EnhancedIsolatedVectorStore, EnhancedExpenseTaskManager,\n",
        "# EnhancedRAGExpenseProcessor, and create_enhanced_rag_workflow.\n",
        "\n",
        "# ================================\n",
        "# STEP 2: TOKEN USAGE TRACKING (Re-definition)\n",
        "# ================================\n",
        "\n",
        "class TokenUsageTracker:\n",
        "    \"\"\"Track token usage across all LLM calls\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.call_history = []\n",
        "        self.total_input_tokens = 0\n",
        "        self.total_output_tokens = 0\n",
        "        self.total_tokens = 0\n",
        "        self.call_count = 0\n",
        "\n",
        "    def track_call(self, operation: str, filename: str, task: str, response):\n",
        "        \"\"\"Track a single LLM call and extract usage info\"\"\"\n",
        "\n",
        "        usage_info = {\n",
        "            \"operation\": operation,\n",
        "            \"filename\": filename,\n",
        "            \"task\": task,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"input_tokens\": 0,\n",
        "            \"output_tokens\": 0,\n",
        "            \"total_tokens\": 0,\n",
        "            \"duration_ms\": 0\n",
        "        }\n",
        "\n",
        "        # Extract token usage from response\n",
        "        try:\n",
        "            if hasattr(response, 'usage_metadata') and response.usage_metadata:\n",
        "                usage_info[\"input_tokens\"] = response.usage_metadata.get('input_tokens', 0)\n",
        "                usage_info[\"output_tokens\"] = response.usage_metadata.get('output_tokens', 0)\n",
        "                usage_info[\"total_tokens\"] = response.usage_metadata.get('total_tokens', 0)\n",
        "\n",
        "            # Fallback: try response_metadata\n",
        "            elif hasattr(response, 'response_metadata') and response.response_metadata:\n",
        "                metadata = response.response_metadata\n",
        "                usage_info[\"input_tokens\"] = metadata.get('prompt_eval_count', 0)\n",
        "                usage_info[\"output_tokens\"] = metadata.get('eval_count', 0)\n",
        "                usage_info[\"total_tokens\"] = usage_info[\"input_tokens\"] + usage_info[\"output_tokens\"]\n",
        "                usage_info[\"duration_ms\"] = metadata.get('total_duration', 0) // 1000000  # Convert to ms\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not extract token usage: {e}\")\n",
        "\n",
        "        # Update totals\n",
        "        self.total_input_tokens += usage_info[\"input_tokens\"]\n",
        "        self.total_output_tokens += usage_info[\"output_tokens\"]\n",
        "        self.total_tokens += usage_info[\"total_tokens\"]\n",
        "        self.call_count += 1\n",
        "\n",
        "        # Store call history\n",
        "        self.call_history.append(usage_info)\n",
        "\n",
        "        # Print usage info\n",
        "        self.print_usage_info(usage_info)\n",
        "\n",
        "        return usage_info\n",
        "\n",
        "\n",
        "    def print_usage_info(self, usage_info: Dict[str, Any]):\n",
        "        \"\"\"Print formatted usage information\"\"\"\n",
        "        print(f\"📊 TOKEN USAGE - {usage_info['operation']} | {usage_info['filename']} | {usage_info['task']}\")\n",
        "        print(f\"   📥 Input: {usage_info['input_tokens']} tokens\")\n",
        "        print(f\"   📤 Output: {usage_info['output_tokens']} tokens\")\n",
        "        print(f\"   🔢 Total: {usage_info['total_tokens']} tokens\")\n",
        "        if usage_info['duration_ms'] > 0:\n",
        "            print(f\"   ⏱️ Duration: {usage_info['duration_ms']}ms\")\n",
        "        print()\n",
        "\n",
        "    def print_summary(self):\n",
        "        \"\"\"Print overall token usage summary\"\"\"\n",
        "        print(\"=\" * 60)\n",
        "        print(\"📊 TOTAL TOKEN USAGE SUMMARY\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"🔢 Total LLM Calls: {self.call_count}\")\n",
        "        print(f\"📥 Total Input Tokens: {self.total_input_tokens:,}\")\n",
        "        print(f\"📤 Total Output Tokens: {self.total_output_tokens:,}\")\n",
        "        print(f\"🎯 Grand Total Tokens: {self.total_tokens:,}\")\n",
        "\n",
        "        if self.call_count > 0:\n",
        "            print(f\"📊 Average per call: {self.total_tokens/self.call_count:.1f} tokens\")\n",
        "        print()\n",
        "\n",
        "# Global token tracker\n",
        "token_tracker = TokenUsageTracker()\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - TokenUsageTracker!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ================================\n",
        "# STEP 3: FILENAME-BASED DOCUMENT MANAGEMENT (Re-definition)\n",
        "# ================================\n",
        "\n",
        "@dataclass # Use the re-imported dataclass\n",
        "class ClaimDocument:\n",
        "    \"\"\"Document with filename-based identification\"\"\"\n",
        "    filename: str  # Primary identifier (no more UUIDs!)\n",
        "    file_path: str\n",
        "    raw_text: str\n",
        "    chunks: List[str]\n",
        "    metadata: Dict[str, Any]\n",
        "    processed_timestamp: datetime\n",
        "\n",
        "class FilenameBasedDocumentManager:\n",
        "    \"\"\"Manages documents using filenames as primary identifiers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.documents_registry = {}  # filename -> ClaimDocument\n",
        "        self.chunk_to_file_map = {}  # chunk_id -> filename\n",
        "\n",
        "    def register_document(self, file_path: str, raw_text: str) -> str:\n",
        "        \"\"\"Register document using filename as ID\"\"\"\n",
        "\n",
        "        filename = Path(file_path).stem  # Get filename without extension\n",
        "\n",
        "        print(f\"📋 REGISTERING DOCUMENT: {filename}\")\n",
        "        print(f\"   📁 Source: {Path(file_path).name}\")\n",
        "        print(f\"   📄 Text length: {len(raw_text)} characters\")\n",
        "\n",
        "        # Create isolated chunks for this document\n",
        "        chunks = self.create_document_chunks(raw_text, filename)\n",
        "\n",
        "        claim_doc = ClaimDocument(\n",
        "            filename=filename,\n",
        "            file_path=file_path,\n",
        "            raw_text=raw_text,\n",
        "            chunks=chunks,\n",
        "            metadata={\n",
        "                \"file_name\": Path(file_path).name,\n",
        "                \"file_extension\": Path(file_path).suffix,\n",
        "                \"chunk_count\": len(chunks),\n",
        "                \"source\": \"ocr_extraction\"\n",
        "            },\n",
        "            processed_timestamp=datetime.now()\n",
        "        )\n",
        "\n",
        "        self.documents_registry[filename] = claim_doc\n",
        "\n",
        "        # Update chunk mapping\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk_id = f\"{filename}_chunk_{i}\"\n",
        "            self.chunk_to_file_map[chunk_id] = filename\n",
        "\n",
        "        print(f\"✅ Document registered: {filename} with {len(chunks)} chunks\")\n",
        "        return filename\n",
        "\n",
        "    def create_document_chunks(self, text: str, filename: str) -> List[str]:\n",
        "        \"\"\"Create chunks with filename-specific context isolation\"\"\"\n",
        "\n",
        "        print(f\"🔪 CHUNKING DOCUMENT: {filename}\")\n",
        "\n",
        "        lines = text.split('\\n')\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "        max_chunk_size = 500\n",
        "\n",
        "        # Expense document section markers\n",
        "        section_markers = [\n",
        "            'total', 'amount', 'date', 'vendor', 'receipt', 'invoice',\n",
        "            'item', 'quantity', 'price', 'tax', 'subtotal'\n",
        "        ]\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            line_length = len(line)\n",
        "            is_section_start = any(marker in line.lower() for marker in section_markers)\n",
        "\n",
        "            if (current_length + line_length > max_chunk_size) or \\\n",
        "               (is_section_start and current_chunk and current_length > 200):\n",
        "\n",
        "                chunk_text = '\\n'.join(current_chunk)\n",
        "                if chunk_text.strip():\n",
        "                    # Add filename isolation metadata to chunk\n",
        "                    isolated_chunk = f\"[DOCUMENT: {filename}]\\n{chunk_text}\"\n",
        "                    chunks.append(isolated_chunk)\n",
        "\n",
        "                current_chunk = [line]\n",
        "                current_length = line_length\n",
        "            else:\n",
        "                current_chunk.append(line)\n",
        "                current_length += line_length + 1\n",
        "\n",
        "        # Add final chunk\n",
        "        if current_chunk:\n",
        "            chunk_text = '\\n'.join(current_chunk)\n",
        "            if chunk_text.strip():\n",
        "                isolated_chunk = f\"[DOCUMENT: {filename}]\\n{chunk_text}\"\n",
        "                chunks.append(isolated_chunk)\n",
        "\n",
        "        print(f\"   🔪 Created {len(chunks)} chunks (avg {len(text)//len(chunks) if chunks else 0} chars each)\")\n",
        "        return chunks\n",
        "\n",
        "\n",
        "\n",
        "    def get_document_context(self, filename: str) -> Optional[ClaimDocument]:\n",
        "        \"\"\"Get complete context for a specific document\"\"\"\n",
        "        return self.documents_registry.get(filename)\n",
        "\n",
        "    def list_all_documents(self) -> List[str]:\n",
        "        \"\"\"List all registered filenames\"\"\"\n",
        "        return list(self.documents_registry.keys())\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - FILENAME-BASED DOCUMENT MANAGEMENT!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ================================\n",
        "# STEP 4: ENHANCED OCR PROCESSOR (Re-definition)\n",
        "# ================================\n",
        "\n",
        "class EnhancedOCRProcessor:\n",
        "    \"\"\"OCR processing with detailed progress tracking\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.supported_formats = ['.pdf', '.jpg', '.jpeg', '.png', '.tiff']\n",
        "\n",
        "    def extract_text_from_document(self, file_path: str) -> str:\n",
        "        \"\"\"Extract text with detailed progress tracking\"\"\"\n",
        "\n",
        "        filename = Path(file_path).name\n",
        "        print(f\"🔍 EXTRACTING TEXT FROM: {filename}\")\n",
        "        print(f\"   📁 Full path: {file_path}\")\n",
        "        print(f\"   📊 File size: {Path(file_path).stat().st_size / 1024:.1f} KB\")\n",
        "\n",
        "        try:\n",
        "            # Ensure partition is imported from unstructured.partition.auto\n",
        "            from unstructured.partition.auto import partition\n",
        "\n",
        "            print(f\"   🔄 Processing with UnstructuredIO...\")\n",
        "\n",
        "            # Process document with UnstructuredIO\n",
        "            elements = partition(filename=file_path)\n",
        "\n",
        "            print(f\"   📋 Found {len(elements)} document elements\")\n",
        "\n",
        "            # Extract text from all elements\n",
        "            full_text = \"\"\n",
        "            for i, element in enumerate(elements):\n",
        "                if hasattr(element, 'text') and element.text:\n",
        "                    full_text += element.text + \"\\n\"\n",
        "                    if i < 5:  # Show first few elements\n",
        "                        print(f\"     Element {i+1}: {element.text[:50]}...\")\n",
        "\n",
        "            # Clean and normalize text\n",
        "            full_text = self.clean_extracted_text(full_text)\n",
        "\n",
        "            print(f\"   ✅ Extracted {len(full_text)} characters\")\n",
        "            print(f\"   📝 Text preview: {full_text[:100]}...\")\n",
        "            return full_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ OCR extraction failed: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def clean_extracted_text(self, text: str) -> str:\n",
        "        \"\"\"Clean extracted text with progress info\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        original_length = len(text)\n",
        "        lines = text.split('\\n')\n",
        "        cleaned_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line and len(line) > 2:\n",
        "                cleaned_lines.append(line)\n",
        "\n",
        "        cleaned_text = '\\n'.join(cleaned_lines)\n",
        "        print(f\"   🧹 Cleaned: {original_length} → {len(cleaned_text)} chars ({len(cleaned_lines)} lines)\")\n",
        "\n",
        "        return cleaned_text\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - FILENAME-BASED DOCUMENT MANAGEMENT!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ================================\n",
        "# STEP 5: ENHANCED VECTOR STORE (Re-definition)\n",
        "# ================================\n",
        "\n",
        "class EnhancedIsolatedVectorStore:\n",
        "    \"\"\"ChromaDB with enhanced tracking and filename-based isolation\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model: str = \"nomic-embed-text\"): # Use default or passed model\n",
        "        import chromadb # Ensure chromadb is imported\n",
        "\n",
        "        self.embedding_model = embedding_model\n",
        "\n",
        "        print(f\"🗄️ INITIALIZING VECTOR STORE\")\n",
        "        print(f\"   🤖 Embedding Model: {embedding_model}\")\n",
        "\n",
        "        # Initialize ChromaDB client using new API\n",
        "        self.client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "        # Create collection\n",
        "        self.collection = self.client.get_or_create_collection(\n",
        "            name=\"filename_based_expense_claims\",\n",
        "            metadata={\"hnsw:space\": \"cosine\"}\n",
        "        )\n",
        "\n",
        "        print(f\"   ✅ ChromaDB initialized\")\n",
        "\n",
        "    def embed_text(self, text: str, filename: str = \"unknown\") -> List[float]:\n",
        "        \"\"\"Generate embeddings with progress tracking\"\"\"\n",
        "\n",
        "        print(f\"🔢 GENERATING EMBEDDING: {filename}\")\n",
        "        print(f\"   📝 Text length: {len(text)} chars\")\n",
        "\n",
        "        try:\n",
        "            # Ensure ollama is imported and available\n",
        "            import ollama\n",
        "            response = ollama.embeddings(model=self.embedding_model, prompt=text)\n",
        "            embedding = response['embedding']\n",
        "            print(f\"   ✅ Generated {len(embedding)}-dimensional embedding\")\n",
        "            return embedding\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Embedding error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def add_document_chunks(self, filename: str, chunks: List[str], metadata: Dict[str, Any]):\n",
        "        \"\"\"Add chunks for a specific document with detailed tracking\"\"\"\n",
        "\n",
        "        print(f\"📚 ADDING CHUNKS TO VECTOR STORE: {filename}\")\n",
        "        print(f\"   📊 Number of chunks: {len(chunks)}\")\n",
        "\n",
        "        embeddings = []\n",
        "        chunk_ids = []\n",
        "        metadatas = []\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            print(f\"   🔄 Processing chunk {i+1}/{len(chunks)}\")\n",
        "\n",
        "            # Generate embedding\n",
        "            embedding = self.embed_text(chunk, f\"{filename}_chunk_{i}\")\n",
        "            if not embedding:\n",
        "                print(f\"   ⚠️ Skipping chunk {i+1} - no embedding generated\")\n",
        "                continue\n",
        "\n",
        "            chunk_id = f\"{filename}_chunk_{i}\"\n",
        "            chunk_metadata = {\n",
        "                **metadata,\n",
        "                \"filename\": filename,\n",
        "                \"chunk_index\": i,\n",
        "                \"chunk_id\": chunk_id,\n",
        "                \"isolated\": True\n",
        "            }\n",
        "\n",
        "            embeddings.append(embedding)\n",
        "            chunk_ids.append(chunk_id)\n",
        "            metadatas.append(chunk_metadata)\n",
        "\n",
        "        # Add to ChromaDB\n",
        "        if embeddings:\n",
        "            self.collection.add(\n",
        "                embeddings=embeddings,\n",
        "                documents=chunks,\n",
        "                metadatas=metadatas,\n",
        "                ids=chunk_ids\n",
        "            )\n",
        "\n",
        "            print(f\"   ✅ Added {len(embeddings)} chunks to vector store\")\n",
        "        else:\n",
        "            print(f\"   ❌ No chunks added - all embeddings failed\")\n",
        "\n",
        "    def query_document_specific(self, query: str, filename: str, n_results: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"Query specific document only - prevents cross-contamination\"\"\"\n",
        "\n",
        "        print(f\"🔍 QUERYING VECTOR STORE: {filename}\")\n",
        "        print(f\"   ❓ Query: {query}\")\n",
        "        print(f\"   📊 Requesting {n_results} results\")\n",
        "\n",
        "        query_embedding = self.embed_text(query, f\"query_{filename}\")\n",
        "        if not query_embedding:\n",
        "            return {\"error\": \"Failed to generate query embedding\"}\n",
        "\n",
        "        # Query with filename filter to ensure isolation\n",
        "        results = self.collection.query(\n",
        "            query_embeddings=[query_embedding],\n",
        "            n_results=n_results,\n",
        "            where={\"filename\": filename},  # CRITICAL: Isolates to specific document\n",
        "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
        "        )\n",
        "\n",
        "        print(f\"   ✅ Found {len(results['documents'][0]) if results['documents'] else 0} relevant chunks\")\n",
        "\n",
        "        return {\n",
        "            \"documents\": results['documents'][0] if results['documents'] else [],\n",
        "            \"metadatas\": results['metadatas'][0] if results['metadatas'] else [],\n",
        "            \"distances\": results['distances'][0] if results['distances'] else [],\n",
        "            \"filename\": filename\n",
        "        }\n",
        "\n",
        "    def get_collection_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get detailed statistics about stored documents\"\"\"\n",
        "\n",
        "        print(\"📊 GENERATING COLLECTION STATISTICS\")\n",
        "\n",
        "        count = self.collection.count()\n",
        "\n",
        "        # Get unique filenames\n",
        "        all_metadata = self.collection.get(include=[\"metadatas\"])\n",
        "        filenames = set()\n",
        "        if all_metadata and all_metadata['metadatas']: # Added check for all_metadata existence\n",
        "            for meta in all_metadata['metadatas']:\n",
        "                if 'filename' in meta:\n",
        "                    filenames.add(meta['filename'])\n",
        "\n",
        "        stats = {\n",
        "            \"total_chunks\": count,\n",
        "            \"unique_documents\": len(filenames),\n",
        "            \"filenames\": list(filenames)\n",
        "        }\n",
        "\n",
        "        print(f\"   📚 Total chunks: {stats['total_chunks']}\")\n",
        "        print(f\"   📄 Unique documents: {stats['unique_documents']}\")\n",
        "        print(f\"   📝 Documents: {', '.join(stats['filenames'])}\")\n",
        "\n",
        "        return stats\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - ENHANCED VECTOR STORE!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ================================\n",
        "# STEP 6: ENHANCED EXPENSE TASK MANAGER (Re-definition)\n",
        "# ================================\n",
        "\n",
        "class EnhancedExpenseTaskManager:\n",
        "    \"\"\"Manages predefined expense extraction tasks with better tracking\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.predefined_tasks = {\n",
        "            \"extract_amount\": {\n",
        "                \"query\": \"total amount due payment cost price sum money dollar\",\n",
        "                \"description\": \"Extract the total amount from this expense document\",\n",
        "                \"expected_format\": \"numeric value with currency\"\n",
        "            },\n",
        "            \"extract_date\": {\n",
        "                \"query\": \"date transaction purchase invoice receipt timestamp when\",\n",
        "                \"description\": \"Extract the date from this expense document\",\n",
        "                \"expected_format\": \"date in YYYY-MM-DD format\"\n",
        "            },\n",
        "            \"extract_vendor\": {\n",
        "                \"query\": \"vendor merchant company business supplier store restaurant hotel\",\n",
        "                \"description\": \"Extract vendor/merchant name from this expense document\",\n",
        "                \"expected_format\": \"company or business name\"\n",
        "            },\n",
        "            \"extract_category\": {\n",
        "                \"query\": \"category type classification expense kind service product item\",\n",
        "                \"description\": \"Determine expense category from this document\",\n",
        "                \"expected_format\": \"expense category classification\"\n",
        "            },\n",
        "            \"extract_items\": {\n",
        "                \"query\": \"items products services line items purchases description details\",\n",
        "                \"description\": \"Extract itemized details from this expense document\",\n",
        "                \"expected_format\": \"list of items or services\"\n",
        "            },\n",
        "            \"extract_tax\": {\n",
        "                \"query\": \"tax VAT GST sales tax tax rate percentage\",\n",
        "                \"description\": \"Extract tax information from this expense document\",\n",
        "                \"expected_format\": \"tax amount and rate\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def get_task_info(self, task_name: str) -> Dict[str, str]:\n",
        "        \"\"\"Get complete task information\"\"\"\n",
        "        return self.predefined_tasks.get(task_name, {})\n",
        "\n",
        "    def list_available_tasks(self) -> List[str]:\n",
        "        \"\"\"List all available extraction tasks\"\"\"\n",
        "        return list(self.predefined_tasks.keys())\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - ENHANCED EXPENSE TASK MANAGER!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "\n",
        "# ================================\n",
        "# STEP 7: ENHANCED RAG PROCESSOR (Re-definition)\n",
        "# ================================\n",
        "\n",
        "class EnhancedRAGExpenseProcessor:\n",
        "    \"\"\"RAG-based expense processor with comprehensive tracking\"\"\"\n",
        "\n",
        "    def __init__(self, text_model: str = \"gemma3:1b\"): # Use default or passed model\n",
        "        from langchain_ollama import ChatOllama # Ensure ChatOllama is imported\n",
        "\n",
        "        print(f\"🚀 INITIALIZING RAG EXPENSE PROCESSOR\")\n",
        "        print(f\"   🤖 Text Model: {text_model}\")\n",
        "\n",
        "        self.llm = ChatOllama(\n",
        "            model=text_model,\n",
        "            temperature=0.1,\n",
        "            base_url=\"http://127.0.0.1:11434\"\n",
        "        )\n",
        "\n",
        "        self.vector_store = EnhancedIsolatedVectorStore()\n",
        "        self.task_manager = EnhancedExpenseTaskManager()\n",
        "        self.document_manager = FilenameBasedDocumentManager()\n",
        "        self.ocr_processor = EnhancedOCRProcessor()\n",
        "\n",
        "        print(\"   ✅ All components initialized\")\n",
        "\n",
        "    def ingest_document(self, file_path: str) -> str:\n",
        "        \"\"\"INGESTION PHASE: Process document and store in vector DB\"\"\"\n",
        "\n",
        "        filename = Path(file_path).name\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"🔄 INGESTION PHASE STARTING\")\n",
        "        print(f\"📄 FILE: {filename}\")\n",
        "        print(f\"📁 PATH: {file_path}\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Step 1: OCR extraction\n",
        "        raw_text = self.ocr_processor.extract_text_from_document(file_path)\n",
        "        if not raw_text:\n",
        "            print(\"❌ INGESTION FAILED: No text extracted\")\n",
        "            return None\n",
        "\n",
        "        # Step 2: Register document with filename-based system\n",
        "        filename_id = self.document_manager.register_document(file_path, raw_text)\n",
        "\n",
        "        # Step 3: Get document context\n",
        "        document = self.document_manager.get_document_context(filename_id)\n",
        "\n",
        "        # Step 4: Store in vector database\n",
        "        metadata = {\n",
        "            **document.metadata,\n",
        "            \"ingestion_timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        self.vector_store.add_document_chunks(\n",
        "            filename=filename_id,\n",
        "            chunks=document.chunks,\n",
        "            metadata=metadata\n",
        "        )\n",
        "\n",
        "        print(f\"✅ INGESTION COMPLETED: {filename_id}\")\n",
        "        print(\"=\"*70)\n",
        "        return filename_id\n",
        "\n",
        "    def process_expense_task(self, filename: str, task_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"INFERENCE PHASE: Process specific task for document\"\"\"\n",
        "\n",
        "        print(f\"\\n🎯 INFERENCE PHASE STARTING\")\n",
        "        print(f\"📄 DOCUMENT: {filename}\")\n",
        "        print(f\"🎯 TASK: {task_name}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Step 1: Get task information\n",
        "        task_info = self.task_manager.get_task_info(task_name)\n",
        "        if not task_info:\n",
        "            return {\"error\": f\"Unknown task: {task_name}\"}\n",
        "\n",
        "        task_query = task_info.get(\"query\", \"\")\n",
        "        task_description = task_info.get(\"description\", \"\")\n",
        "\n",
        "        print(f\"📋 Task Description: {task_description}\")\n",
        "        print(f\"🔍 Search Query: {task_query}\")\n",
        "\n",
        "        # Step 2: Retrieve relevant chunks (ISOLATED to this document)\n",
        "        retrieval_results = self.vector_store.query_document_specific(\n",
        "            query=task_query,\n",
        "            filename=filename,\n",
        "            n_results=3\n",
        "        )\n",
        "\n",
        "        if retrieval_results.get(\"error\"):\n",
        "            return retrieval_results\n",
        "\n",
        "        # Step 3: Prepare optimized context\n",
        "        context = self.optimize_context(retrieval_results, task_name)\n",
        "\n",
        "        # Step 4: Generate response with LLM (WITH TOKEN TRACKING)\n",
        "        response_text, token_usage = self.generate_task_response_with_tracking(\n",
        "            context, task_name, task_description, filename\n",
        "        )\n",
        "\n",
        "        result = {\n",
        "            \"task\": task_name,\n",
        "            \"filename\": filename,\n",
        "            \"response\": response_text,\n",
        "            \"context_chunks_used\": len(retrieval_results[\"documents\"]),\n",
        "            \"token_usage\": token_usage\n",
        "        }\n",
        "\n",
        "        print(f\"✅ INFERENCE COMPLETED: {task_name} for {filename}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def optimize_context(self, retrieval_results: Dict[str, Any], task_name: str) -> str:\n",
        "        \"\"\"CONTEXT OPTIMIZATION: Reduce context overloading\"\"\"\n",
        "\n",
        "        documents = retrieval_results.get(\"documents\", [])\n",
        "        distances = retrieval_results.get(\"distances\", [])\n",
        "        filename = retrieval_results.get(\"filename\", \"unknown\")\n",
        "\n",
        "        print(f\"🔧 OPTIMIZING CONTEXT: {filename}\")\n",
        "        print(f\"   📊 Raw chunks: {len(documents)}\")\n",
        "\n",
        "        if not documents:\n",
        "            return \"No relevant context found\"\n",
        "\n",
        "        # Rank documents by relevance\n",
        "        doc_scores = list(zip(documents, distances))\n",
        "        doc_scores.sort(key=lambda x: x[1])\n",
        "\n",
        "        optimized_chunks = []\n",
        "        total_length = 0\n",
        "        max_context_length = 1500\n",
        "\n",
        "        for i, (doc, score) in enumerate(doc_scores):\n",
        "            # Remove document prefix from chunks\n",
        "            clean_doc = doc.replace(f\"[DOCUMENT: {filename}]\\n\", \"\")\n",
        "\n",
        "            if total_length + len(clean_doc) <= max_context_length:\n",
        "                optimized_chunks.append(clean_doc)\n",
        "                total_length += len(clean_doc)\n",
        "                print(f\"   ✅ Chunk {i+1}: {len(clean_doc)} chars (relevance: {score:.3f})\")\n",
        "            else:\n",
        "                remaining_space = max_context_length - total_length\n",
        "                if remaining_space > 100:\n",
        "                    truncated = clean_doc[:remaining_space] + \"...\"\n",
        "                    optimized_chunks.append(truncated)\n",
        "                    print(f\"   ✂️ Chunk {i+1}: truncated to {len(truncated)} chars\")\n",
        "                break\n",
        "\n",
        "        context = \"\\n\\n---\\n\\n\".join(optimized_chunks)\n",
        "        print(f\"   🎯 Final context: {len(context)} chars from {len(optimized_chunks)} chunks\")\n",
        "\n",
        "        return context\n",
        "\n",
        "    def generate_task_response_with_tracking(self, context: str, task_name: str, task_description: str, filename: str) -> Tuple[str, Dict[str, Any]]:\n",
        "        \"\"\"Generate LLM response with token usage tracking\"\"\"\n",
        "\n",
        "        print(f\"🤖 GENERATING LLM RESPONSE: {task_name} | {filename}\")\n",
        "\n",
        "        prompt = f\"\"\"You are an expert expense analyst. {task_description}\n",
        "\n",
        "CONTEXT FROM EXPENSE DOCUMENT ({filename}):\n",
        "{context}\n",
        "\n",
        "TASK: {task_name}\n",
        "INSTRUCTION: {task_description}\n",
        "\n",
        "Based ONLY on the context provided above, extract the requested information. Be precise and factual. If the information is not clearly present in the context, state \"Information not found in provided context.\"\n",
        "\n",
        "Response:\"\"\"\n",
        "\n",
        "        print(f\"   📝 Prompt length: {len(prompt)} characters\")\n",
        "\n",
        "        try:\n",
        "            response = self.llm.invoke(prompt)\n",
        "\n",
        "            # Track token usage\n",
        "            token_usage = token_tracker.track_call(\"llm_inference\", filename, task_name, response)\n",
        "\n",
        "            return response.content.strip(), token_usage\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error generating response: {e}\"\n",
        "            print(f\"   ❌ {error_msg}\")\n",
        "            return error_msg, {}\n",
        "\n",
        "    def process_all_tasks_for_document(self, filename: str) -> Dict[str, Any]:\n",
        "        \"\"\"Process all predefined tasks for a document\"\"\"\n",
        "\n",
        "        print(f\"\\n📊 PROCESSING ALL TASKS FOR: {filename}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        tasks = self.task_manager.list_available_tasks()\n",
        "        results = {}\n",
        "\n",
        "        for i, task in enumerate(tasks, 1):\n",
        "            print(f\"\\n[{i}/{len(tasks)}] Starting task: {task}\")\n",
        "            result = self.process_expense_task(filename, task)\n",
        "            results[task] = result\n",
        "\n",
        "        print(f\"\\n✅ ALL TASKS COMPLETED FOR: {filename}\")\n",
        "        return results\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - ENHANCED RAG PROCESSOR!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "\n",
        "# ================================\n",
        "# STEP 8: ENHANCED WORKFLOW (Re-definition)\n",
        "# ================================\n",
        "\n",
        "from langgraph.graph import StateGraph # Ensure StateGraph is imported\n",
        "from typing import TypedDict\n",
        "\n",
        "class EnhancedRAGWorkflowState(TypedDict):\n",
        "    \"\"\"Enhanced state for RAG workflow\"\"\"\n",
        "    file_paths: List[str]\n",
        "    current_file_index: int\n",
        "    processed_filenames: List[str]\n",
        "    current_filename: str\n",
        "    task_results: Dict[str, Dict[str, Any]]\n",
        "    workflow_status: str\n",
        "    error: Optional[str]\n",
        "\n",
        "def create_enhanced_rag_workflow(processor: EnhancedRAGExpenseProcessor) -> StateGraph:\n",
        "    \"\"\"Create enhanced LangGraph workflow for RAG processing\"\"\"\n",
        "\n",
        "    def enhanced_ingestion_node(state: EnhancedRAGWorkflowState) -> EnhancedRAGWorkflowState:\n",
        "        \"\"\"Enhanced ingestion with detailed tracking\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"🔄 WORKFLOW: ENHANCED INGESTION PHASE STARTING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        file_paths = state.get(\"file_paths\", [])\n",
        "        processed_filenames = []\n",
        "\n",
        "        if not file_paths:\n",
        "            state[\"workflow_status\"] = \"ingestion_failed\"\n",
        "            state[\"error\"] = \"No file paths provided for ingestion.\"\n",
        "            print(\"❌ INGESTION FAILED: No file paths provided.\")\n",
        "            return state\n",
        "\n",
        "\n",
        "        for i, file_path in enumerate(file_paths, 1):\n",
        "            print(f\"\\n[{i}/{len(file_paths)}] Processing file: {Path(file_path).name}\")\n",
        "\n",
        "            try:\n",
        "                # Use the processor to ingest the document\n",
        "                filename = processor.ingest_document(file_path)\n",
        "                if filename:\n",
        "                    processed_filenames.append(filename)\n",
        "                    print(f\"✅ Successfully ingested: {filename}\")\n",
        "                else:\n",
        "                    print(f\"❌ Failed to ingest: {Path(file_path).name}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error ingesting {Path(file_path).name}: {e}\")\n",
        "                state[\"error\"] = str(e) # Store the error in state\n",
        "                # Decide if you want to stop on first ingestion error or continue\n",
        "                # For now, let's continue to process other files if possible\n",
        "                processed_filenames.append({\"error\": str(e), \"filename\": Path(file_path).name})\n",
        "\n",
        "\n",
        "        state[\"processed_filenames\"] = processed_filenames\n",
        "        state[\"workflow_status\"] = \"ingestion_complete\" if any(isinstance(f, str) for f in processed_filenames) else \"ingestion_failed\" # Check if at least one file was successfully processed\n",
        "\n",
        "        print(f\"\\n📊 INGESTION PHASE COMPLETED\")\n",
        "        successful_count = sum(1 for f in processed_filenames if isinstance(f, str))\n",
        "        print(f\"   ✅ Successfully processed: {successful_count} files\")\n",
        "        print(f\"   ❌ Failed: {len(file_paths) - successful_count} files\")\n",
        "\n",
        "\n",
        "        return state\n",
        "\n",
        "    def enhanced_task_processing_node(state: EnhancedRAGWorkflowState) -> EnhancedRAGWorkflowState:\n",
        "        \"\"\"Enhanced task processing with detailed tracking\"\"\"\n",
        "\n",
        "        print(\"\\n🎯 WORKFLOW: ENHANCED TASK PROCESSING PHASE STARTING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Filter out ingestion errors before processing tasks\n",
        "        processable_filenames = [f for f in state.get(\"processed_filenames\", []) if isinstance(f, str)]\n",
        "        task_results = state.get(\"task_results\", {}) # Initialize or get existing results\n",
        "\n",
        "        if not processable_filenames:\n",
        "            state[\"workflow_status\"] = \"processing_skipped\"\n",
        "            print(\"⚠️ TASK PROCESSING SKIPPED: No documents successfully ingested.\")\n",
        "            return state\n",
        "\n",
        "\n",
        "        for i, filename in enumerate(processable_filenames, 1):\n",
        "            print(f\"\\n[{i}/{len(processable_filenames)}] Processing tasks for: {filename}\")\n",
        "\n",
        "            try:\n",
        "                results = processor.process_all_tasks_for_document(filename)\n",
        "                task_results[filename] = results\n",
        "                print(f\"✅ Completed all tasks for: {filename}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing tasks for {filename}: {e}\")\n",
        "                task_results[filename] = {\"error\": str(e)}\n",
        "                state[\"error\"] = str(e) # Store the error in state\n",
        "\n",
        "\n",
        "        state[\"task_results\"] = task_results\n",
        "        state[\"workflow_status\"] = \"processing_complete\"\n",
        "\n",
        "        print(f\"\\n📊 TASK PROCESSING PHASE COMPLETED\")\n",
        "        print(f\"   📄 Documents processed: {len(task_results)}\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    def enhanced_results_compilation_node(state: EnhancedRAGWorkflowState) -> EnhancedRAGWorkflowState:\n",
        "        \"\"\"Enhanced results compilation with detailed stats and CSV export\"\"\"\n",
        "\n",
        "        print(\"\\n📊 WORKFLOW: ENHANCED RESULTS COMPILATION STARTING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        task_results = state.get(\"task_results\", {})\n",
        "        # token_tracker is a global instance assumed to be available\n",
        "        # processor instance (and its vector_store) is also assumed to be available\n",
        "\n",
        "        if not task_results:\n",
        "             state[\"workflow_status\"] = \"compilation_skipped\"\n",
        "             print(\"⚠️ RESULTS COMPILATION SKIPPED: No task results to compile.\")\n",
        "             token_tracker.print_summary() # Print summary even if compilation skipped\n",
        "             return state\n",
        "\n",
        "\n",
        "        # Compile detailed statistics\n",
        "        total_documents_attempted_ingestion = len(state.get(\"file_paths\", [])) # Count original files\n",
        "        successfully_ingested_filenames = [f for f in state.get(\"processed_filenames\", []) if isinstance(f, str)]\n",
        "        total_documents_successfully_processed = len(task_results) # Count documents with task results (successful or not)\n",
        "\n",
        "\n",
        "        # Save results with timestamp\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        results_file = f\"enhanced_rag_expense_results_{timestamp}.json\"\n",
        "        csv_file = f\"enhanced_rag_expense_results_{timestamp}.csv\"\n",
        "\n",
        "        # Create comprehensive results package\n",
        "        comprehensive_results = {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"summary\": {\n",
        "                \"total_documents_attempted_ingestion\": total_documents_attempted_ingestion,\n",
        "                \"successfully_ingested_documents\": len(successfully_ingested_filenames),\n",
        "                \"documents_with_task_results\": total_documents_successfully_processed,\n",
        "                \"failed_ingestion\": total_documents_attempted_ingestion - len(successfully_ingested_filenames)\n",
        "\n",
        "            },\n",
        "            \"token_usage_summary\": {\n",
        "                \"total_calls\": token_tracker.call_count,\n",
        "                \"total_input_tokens\": token_tracker.total_input_tokens,\n",
        "                \"total_output_tokens\": token_tracker.total_output_tokens, # Fixed typo here\n",
        "                \"total_tokens\": token_tracker.total_tokens\n",
        "            },\n",
        "            \"document_results\": task_results,\n",
        "            \"token_call_history\": token_tracker.call_history,\n",
        "            \"initial_state\": state.get(\"initial_state\", {}) # Include initial state for debugging\n",
        "        }\n",
        "\n",
        "        # Save JSON results\n",
        "        try:\n",
        "            with open(results_file, 'w') as f:\n",
        "                json.dump(comprehensive_results, f, indent=2, default=str)\n",
        "            print(f\"💾 JSON RESULTS SAVED TO: {results_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error saving JSON results: {e}\")\n",
        "            state[\"error\"] = f\"Error saving JSON results: {e}\"\n",
        "\n",
        "\n",
        "        # Create CSV from results\n",
        "        csv_rows = []\n",
        "\n",
        "        # Add rows for documents that failed ingestion\n",
        "        failed_ingestion_info = [f for f in state.get(\"processed_filenames\", []) if isinstance(f, dict) and \"error\" in f]\n",
        "        for fail_info in failed_ingestion_info:\n",
        "             csv_rows.append({\n",
        "                \"filename\": fail_info.get(\"filename\", \"unknown\"),\n",
        "                \"task\": \"ingestion_failed\",\n",
        "                \"response\": fail_info.get(\"error\", \"Unknown ingestion error\"),\n",
        "                \"context_chunks_used\": 0,\n",
        "                \"input_tokens\": 0,\n",
        "                \"output_tokens\": 0,\n",
        "                \"total_tokens\": 0\n",
        "            })\n",
        "\n",
        "\n",
        "        for filename, doc_results in task_results.items():\n",
        "            if \"error\" in doc_results:\n",
        "                # Add error row for document that failed task processing after successful ingestion\n",
        "                csv_rows.append({\n",
        "                    \"filename\": filename,\n",
        "                    \"task\": \"processing_failed\",\n",
        "                    \"response\": doc_results[\"error\"],\n",
        "                    \"context_chunks_used\": 0,\n",
        "                    \"input_tokens\": 0,\n",
        "                    \"output_tokens\": 0,\n",
        "                    \"total_tokens\": 0\n",
        "                })\n",
        "            else:\n",
        "                # Process each task for this document\n",
        "                for task_name, task_result in doc_results.items():\n",
        "                    if isinstance(task_result, dict):\n",
        "                        token_usage = task_result.get(\"token_usage\", {})\n",
        "                        csv_rows.append({\n",
        "                            \"filename\": filename,\n",
        "                            \"task\": task_name,\n",
        "                            \"response\": task_result.get(\"response\", \"\"),\n",
        "                            \"context_chunks_used\": task_result.get(\"context_chunks_used\", 0),\n",
        "                            \"input_tokens\": token_usage.get(\"input_tokens\", 0),\n",
        "                            \"output_tokens\": token_usage.get(\"output_tokens\", 0),\n",
        "                            \"total_tokens\": token_usage.get(\"total_tokens\", 0)\n",
        "                        })\n",
        "                    else:\n",
        "                         # Handle task-specific errors (if task_result is not a dict but an error string)\n",
        "                        csv_rows.append({\n",
        "                            \"filename\": filename,\n",
        "                            \"task\": task_name,\n",
        "                            \"response\": f\"Task error: {task_result}\",\n",
        "                            \"context_chunks_used\": 0,\n",
        "                            \"input_tokens\": 0,\n",
        "                            \"output_tokens\": 0,\n",
        "                            \"total_tokens\": 0\n",
        "                        })\n",
        "\n",
        "\n",
        "        # Save CSV\n",
        "        df = pd.DataFrame(csv_rows) # Create DataFrame even if empty\n",
        "\n",
        "        if not df.empty: # Check if DataFrame is not empty before processing\n",
        "            try:\n",
        "                # Reorder columns for better readability - handle missing columns gracefully\n",
        "                column_order = [\n",
        "                    \"filename\", \"task\", \"response\",\n",
        "                    \"context_chunks_used\", \"input_tokens\",\n",
        "                    \"output_tokens\", \"total_tokens\"\n",
        "                ]\n",
        "                existing_columns = [col for col in column_order if col in df.columns]\n",
        "                df = df[existing_columns]\n",
        "\n",
        "\n",
        "                # Convert token columns to numeric, handling errors\n",
        "                for token_col in ['input_tokens', 'output_tokens', 'total_tokens']:\n",
        "                     if token_col in df.columns:\n",
        "                        df[token_col] = pd.to_numeric(df[token_col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "\n",
        "                # Save to CSV\n",
        "                df.to_csv(csv_file, index=False, encoding='utf-8')\n",
        "                print(f\"💾 CSV RESULTS SAVED TO: {csv_file}\")\n",
        "\n",
        "                # Display summary statistics from CSV\n",
        "                print(f\"\\n📊 CSV Summary:\")\n",
        "                print(f\"   📄 Total rows: {len(df)}\")\n",
        "                # Ensure 'filename' column exists before calling nunique\n",
        "                if 'filename' in df.columns:\n",
        "                    print(f\"   📁 Documents: {df['filename'].nunique()}\")\n",
        "                     # Handle case where no tasks were processed successfully\n",
        "                    successful_task_rows = df[~df['task'].isin(['ingestion_failed', 'processing_failed', 'workflow_error'])]\n",
        "                    if not successful_task_rows.empty and 'filename' in successful_task_rows.columns:\n",
        "                         print(f\"   🎯 Avg tasks per successful doc: {successful_task_rows.groupby('filename').size().mean():.1f}\")\n",
        "                    else:\n",
        "                        print(\"   🎯 Avg tasks per successful doc: N/A (No successful tasks)\")\n",
        "\n",
        "                # Ensure 'total_tokens' column exists and is numeric before summing\n",
        "                if 'total_tokens' in df.columns:\n",
        "                    try:\n",
        "                        total_tokens_sum = df['total_tokens'].sum()\n",
        "                        print(f\"   🔢 Total tokens used: {total_tokens_sum:,}\")\n",
        "                         # Update comprehensive_results with the sum from CSV if needed\n",
        "                        comprehensive_results['token_usage_summary']['total_tokens_from_csv'] = total_tokens_sum\n",
        "\n",
        "                    except Exception as e:\n",
        "                         print(f\"⚠️ Could not calculate total tokens from CSV: {e}\")\n",
        "                else:\n",
        "                    print(\"⚠️ 'total_tokens' column not found in CSV.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing or saving CSV results: {e}\")\n",
        "                state[\"error\"] = f\"Error processing or saving CSV results: {e}\"\n",
        "\n",
        "        else:\n",
        "             print(\"⚠️ No CSV rows generated. Skipping CSV save.\")\n",
        "\n",
        "\n",
        "        # Also save a summary CSV with aggregated data per document\n",
        "        summary_csv_file = f\"enhanced_rag_expense_summary_{timestamp}.csv\"\n",
        "        summary_rows = []\n",
        "\n",
        "        # Include documents that failed ingestion in summary\n",
        "        for fail_info in failed_ingestion_info:\n",
        "            summary_rows.append({\n",
        "                \"filename\": fail_info.get(\"filename\", \"unknown\"),\n",
        "                \"status\": \"Ingestion Failed\",\n",
        "                \"error_message\": fail_info.get(\"error\", \"Unknown error\")\n",
        "            })\n",
        "\n",
        "\n",
        "        for filename, doc_results in task_results.items():\n",
        "            if \"error\" not in doc_results:\n",
        "                row = {\"filename\": filename, \"status\": \"Processed\"}\n",
        "\n",
        "                # Extract key information from each task\n",
        "                for task_name in [\"extract_amount\", \"extract_date\", \"extract_vendor\",\n",
        "                                \"extract_category\", \"extract_tax\", \"extract_items\", \"extract_tax\"]: # Include all relevant tasks\n",
        "                    if task_name in doc_results and isinstance(doc_results[task_name], dict):\n",
        "                        response = doc_results[task_name].get(\"response\", \"\")\n",
        "                        # Clean the response (take first line or first 100 chars)\n",
        "                        cleaned = response.split('\\n')[0][:100] if response else \"\"\n",
        "                        row[task_name] = cleaned\n",
        "                    elif task_name in doc_results:\n",
        "                         # Handle task-specific errors\n",
        "                         row[task_name] = f\"Error: {doc_results[task_name]}\"\n",
        "                    else:\n",
        "                        row[task_name] = \"Task Not Run\"\n",
        "\n",
        "\n",
        "                # Add token totals\n",
        "                total_tokens = sum(\n",
        "                    doc_results.get(task, {}).get(\"token_usage\", {}).get(\"total_tokens\", 0)\n",
        "                    for task in doc_results if isinstance(doc_results.get(task), dict)\n",
        "                )\n",
        "                row[\"total_tokens_used\"] = total_tokens\n",
        "\n",
        "                summary_rows.append(row)\n",
        "            else:\n",
        "                 # Add document that failed task processing after ingestion\n",
        "                 summary_rows.append({\n",
        "                     \"filename\": filename,\n",
        "                     \"status\": \"Task Processing Failed\",\n",
        "                     \"error_message\": doc_results[\"error\"]\n",
        "                 })\n",
        "\n",
        "\n",
        "        if summary_rows:\n",
        "            try:\n",
        "                summary_df = pd.DataFrame(summary_rows)\n",
        "                # Ensure 'status' and 'error_message' columns exist and are placed early\n",
        "                summary_column_order = [\"filename\", \"status\", \"error_message\"] + [col for col in summary_df.columns if col not in [\"filename\", \"status\", \"error_message\"]]\n",
        "                summary_df = summary_df.get(summary_column_order, summary_df) # Use .get to handle missing columns\n",
        "\n",
        "                summary_df.to_csv(summary_csv_file, index=False, encoding='utf-8')\n",
        "                print(f\"💾 SUMMARY CSV SAVED TO: {summary_csv_file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error saving summary CSV: {e}\")\n",
        "                state[\"error\"] = f\"Error saving summary CSV: {e}\"\n",
        "\n",
        "\n",
        "        print(f\"\\n📊 FILES SAVED:\")\n",
        "        if os.path.exists(results_file): print(f\"   📄 Detailed JSON: {results_file}\")\n",
        "        if os.path.exists(csv_file) and not df.empty: print(f\"   📄 Detailed CSV: {csv_file}\")\n",
        "        if os.path.exists(summary_csv_file) and summary_rows: print(f\"   📄 Summary CSV: {summary_csv_file}\")\n",
        "        print(f\"   📄 Documents processed successfully (ingestion+tasks): {total_documents_successfully_processed}/{total_documents_attempted_ingestion}\")\n",
        "\n",
        "        # Print token usage summary\n",
        "        token_tracker.print_summary()\n",
        "\n",
        "        state[\"workflow_status\"] = \"complete\"\n",
        "        return state\n",
        "\n",
        "\n",
        "    # Build enhanced workflow\n",
        "    workflow = StateGraph(EnhancedRAGWorkflowState)\n",
        "\n",
        "    workflow.add_node(\"enhanced_ingestion\", enhanced_ingestion_node)\n",
        "    workflow.add_node(\"enhanced_task_processing\", enhanced_task_processing_node)\n",
        "    workflow.add_node(\"enhanced_results_compilation\", enhanced_results_compilation_node)\n",
        "\n",
        "    workflow.set_entry_point(\"enhanced_ingestion\")\n",
        "    workflow.add_edge(\"enhanced_ingestion\", \"enhanced_task_processing\")\n",
        "    workflow.add_edge(\"enhanced_task_processing\", \"enhanced_results_compilation\")\n",
        "    workflow.set_finish_point(\"enhanced_results_compilation\")\n",
        "\n",
        "    return workflow.compile()\n",
        "\n",
        "print(\"\\n✅ SETUP COMPLETE - ENHANCED WORKFLOW!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "\n",
        "# Initialize the RAG processor and workflow globally for efficiency in Gradio\n",
        "# This avoids re-initializing the processor (and thus ChromaDB client/collection)\n",
        "# on every file upload in the Gradio app.\n",
        "\n",
        "# Ensure the TokenUsageTracker is also a persistent instance\n",
        "try:\n",
        "    # Re-initialize token_tracker to clear previous runs' data if desired,\n",
        "    # or let it accumulate total usage across all runs.\n",
        "    # For this demo, let's re-initialize for per-file/per-run tracking clarity in UI.\n",
        "    token_tracker = TokenUsageTracker()\n",
        "    processor = EnhancedRAGExpenseProcessor()\n",
        "    # Create the workflow instance\n",
        "    enhanced_rag_workflow = create_enhanced_rag_workflow(processor)\n",
        "    print(\"\\n✅ RAG Processor and Workflow initialized globally.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Error initializing RAG components globally: {e}\")\n",
        "    processor = None # Set to None if initialization fails\n",
        "    enhanced_rag_workflow = None # Set to None if initialization fails\n",
        "\n",
        "\n",
        "# Update the Gradio UI function to call the workflow\n",
        "def process_document_ui(file):\n",
        "    \"\"\"Function to receive the uploaded file, run the workflow, and display results.\"\"\"\n",
        "    # Use the globally initialized workflow and processor\n",
        "    global enhanced_rag_workflow, processor, token_tracker # Added token_tracker here\n",
        "\n",
        "    if file is None:\n",
        "        return \"Please upload a file.\", \"No file uploaded.\", pd.DataFrame() # Return empty DataFrame\n",
        "\n",
        "    file_path = file.name # Gradio provides the temporary path here\n",
        "\n",
        "    status = f\"Received file: {file_path}. Starting RAG workflow...\"\n",
        "    print(status) # Print status to console\n",
        "\n",
        "    # Re-initialize token tracker for this specific run to track usage per file upload\n",
        "    # Check if token_tracker was initialized globally before resetting\n",
        "    if 'token_tracker' in globals() and isinstance(token_tracker, TokenUsageTracker):\n",
        "         token_tracker = TokenUsageTracker()\n",
        "         print(\"\\n📊 Token usage tracker reset for new upload.\")\n",
        "    else:\n",
        "         # If global token_tracker wasn't initialized, create one\n",
        "         token_tracker = TokenUsageTracker()\n",
        "         print(\"\\n📊 Token usage tracker initialized for new upload.\")\n",
        "\n",
        "\n",
        "\n",
        "    # Check if workflow initialization was successful\n",
        "    if enhanced_rag_workflow is None:\n",
        "         error_msg = \"RAG Workflow failed to initialize during startup. Cannot process file.\"\n",
        "         print(f\"❌ {error_msg}\")\n",
        "         return error_msg, \"RAG components failed to initialize. Check server logs.\", pd.DataFrame()\n",
        "\n",
        "\n",
        "    # Execute enhanced workflow with the uploaded file path\n",
        "    initial_state = {\n",
        "        \"file_paths\": [file_path], # Pass the uploaded file path as a list\n",
        "        \"current_file_index\": 0, # Not strictly used with single file processing, but keep for state structure\n",
        "        \"processed_filenames\": [],\n",
        "        \"current_filename\": \"\", # Not strictly used with single file processing\n",
        "        \"task_results\": {},\n",
        "        \"workflow_status\": \"initialized\",\n",
        "        \"error\": None,\n",
        "        \"initial_state\": {\"file_paths\": [file_path]} # Store initial state for compilation node\n",
        "    }\n",
        "\n",
        "    final_state = None # Initialize final_state to None\n",
        "    try:\n",
        "        print(f\"Executing workflow with state: {initial_state}\")\n",
        "        final_state = enhanced_rag_workflow.invoke(initial_state)\n",
        "        workflow_final_status = final_state.get('workflow_status', 'unknown')\n",
        "        status = f\"Workflow completed with status: {workflow_final_status}\"\n",
        "        print(status) # Print final status\n",
        "        print(f\"Final state: {final_state}\") # Print final state for debugging\n",
        "\n",
        "\n",
        "        # Process final state to display results\n",
        "        task_results = final_state.get(\"task_results\", {})\n",
        "        compiled_details = \"\"\n",
        "        results_df = pd.DataFrame() # Default to empty DataFrame\n",
        "\n",
        "\n",
        "        if final_state.get(\"error\"):\n",
        "             compiled_details = f\"Workflow Error: {final_state['error']}\\n\\n\"\n",
        "             # Attempt to display any partial results if available\n",
        "             if task_results or final_state.get(\"processed_filenames\"): # Include ingestion errors\n",
        "                 compiled_details += \"Partial Results:\\n\"\n",
        "                 # Add ingestion errors first\n",
        "                 failed_ingestion_info = [f for f in final_state.get(\"processed_filenames\", []) if isinstance(f, dict) and \"error\" in f]\n",
        "                 for fail_info in failed_ingestion_info:\n",
        "                     compiled_details += f\"\\n📄 Document: {fail_info.get('filename', 'unknown')}\\n\"\n",
        "                     compiled_details += f\"  ❌ Ingestion Error: {fail_info.get('error', 'Unknown ingestion error')}\\n\"\n",
        "\n",
        "\n",
        "                 # Add task processing results/errors\n",
        "                 for filename, doc_results in task_results.items():\n",
        "                      compiled_details += f\"\\n📄 Document: {filename}\\n\"\n",
        "                      if isinstance(doc_results, dict) and \"error\" in doc_results:\n",
        "                           compiled_details += f\"  ❌ Task Processing Error: {doc_results['error']}\\n\"\n",
        "                      elif isinstance(doc_results, dict):\n",
        "                          for task_name, task_result in doc_results.items():\n",
        "                              if isinstance(task_result, dict):\n",
        "                                   response_preview = task_result.get('response', 'No response')\n",
        "                                   compiled_details += f\"  🎯 {task_name}: {response_preview[:100]}...\\n\"\n",
        "                              else:\n",
        "                                  compiled_details += f\"  🎯 {task_name}: Error - {task_result}\\n\"\n",
        "                      else:\n",
        "                           compiled_details += f\"  ❌ Document processing failed: {doc_results}\\n\"\n",
        "\n",
        "\n",
        "             # If there are partial results that can be put in a DataFrame, try that\n",
        "             if task_results or final_state.get(\"processed_filenames\"): # Include ingestion errors\n",
        "                  try:\n",
        "                     csv_rows = []\n",
        "                     # Add ingestion errors first\n",
        "                     failed_ingestion_info = [f for f in final_state.get(\"processed_filenames\", []) if isinstance(f, dict) and \"error\" in f]\n",
        "                     for fail_info in failed_ingestion_info:\n",
        "                          csv_rows.append({\n",
        "                            \"filename\": fail_info.get(\"filename\", \"unknown\"),\n",
        "                            \"task\": \"ingestion_failed\",\n",
        "                            \"response\": fail_info.get(\"error\", \"Unknown ingestion error\"),\n",
        "                            \"context_chunks_used\": 0,\n",
        "                            \"input_tokens\": 0,\n",
        "                            \"output_tokens\": 0,\n",
        "                            \"total_tokens\": 0\n",
        "                        })\n",
        "\n",
        "                     # Add task processing results/errors\n",
        "                     for filename, doc_results in task_results.items():\n",
        "                         if isinstance(doc_results, dict):\n",
        "                            if \"error\" in doc_results:\n",
        "                                csv_rows.append({\n",
        "                                    \"filename\": filename,\n",
        "                                    \"task\": \"processing_failed\",\n",
        "                                    \"response\": doc_results[\"error\"],\n",
        "                                    \"context_chunks_used\": 0,\n",
        "                                    \"input_tokens\": 0,\n",
        "                                    \"output_tokens\": 0,\n",
        "                                    \"total_tokens\": 0\n",
        "                                })\n",
        "                            else:\n",
        "                                for task_name, task_result in doc_results.items():\n",
        "                                    if isinstance(task_result, dict):\n",
        "                                        token_usage = task_result.get(\"token_usage\", {})\n",
        "                                        csv_rows.append({\n",
        "                                            \"filename\": filename,\n",
        "                                            \"task\": task_name,\n",
        "                                            \"response\": task_result.get(\"response\", \"\"),\n",
        "                                            \"context_chunks_used\": task_result.get(\"context_chunks_used\", 0),\n",
        "                                            \"input_tokens\": token_usage.get(\"input_tokens\", 0),\n",
        "                                            \"output_tokens\": token_usage.get(\"output_tokens\", 0),\n",
        "                                            \"total_tokens\": token_usage.get(\"total_tokens\", 0)\n",
        "                                        })\n",
        "                                    else:\n",
        "                                         csv_rows.append({\n",
        "                                            \"filename\": filename,\n",
        "                                            \"task\": task_name,\n",
        "                                            \"response\": f\"Task error: {task_result}\",\n",
        "                                            \"context_chunks_used\": 0,\n",
        "                                            \"input_tokens\": 0,\n",
        "                                            \"output_tokens\": 0,\n",
        "                                            \"total_tokens\": 0\n",
        "                                        })\n",
        "                         else:\n",
        "                              csv_rows.append({\n",
        "                                    \"filename\": filename,\n",
        "                                    \"task\": \"processing_failed\",\n",
        "                                    \"response\": f\"Document processing failed: {doc_results}\",\n",
        "                                    \"context_chunks_used\": 0,\n",
        "                                    \"input_tokens\": 0,\n",
        "                                    \"output_tokens\": 0,\n",
        "                                    \"total_tokens\": 0\n",
        "                                })\n",
        "\n",
        "\n",
        "                     if csv_rows:\n",
        "                        results_df = pd.DataFrame(csv_rows)\n",
        "                        column_order = [\n",
        "                            \"filename\", \"task\", \"response\",\n",
        "                            \"context_chunks_used\", \"input_tokens\",\n",
        "                            \"output_tokens\", \"total_tokens\"\n",
        "                        ]\n",
        "                        existing_columns = [col for col in column_order if col in results_df.columns]\n",
        "                        results_df = results_df[existing_columns]\n",
        "                        for token_col in ['input_tokens', 'output_tokens', 'total_tokens']:\n",
        "                             if token_col in results_df.columns:\n",
        "                                results_df[token_col] = pd.to_numeric(results_df[token_col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "                  except Exception as csv_e:\n",
        "                       compiled_details += f\"\\nError compiling partial results DataFrame: {csv_e}\"\n",
        "                       results_df = pd.DataFrame({\"Error\": [f\"Could not compile results: {csv_e}\"]}) # Indicate error in DataFrame too\n",
        "\n",
        "\n",
        "             return status, compiled_details, results_df # Return DataFrame even on error\n",
        "\n",
        "\n",
        "        # Compile results for display if no workflow-level error\n",
        "        if task_results or final_state.get(\"processed_filenames\"): # Include ingestion errors even if no task results\n",
        "            compiled_details = \"\"\n",
        "            csv_rows = [] # Prepare data for DataFrame display\n",
        "\n",
        "            # Add ingestion errors first\n",
        "            failed_ingestion_info = [f for f in final_state.get(\"processed_filenames\", []) if isinstance(f, dict) and \"error\" in f]\n",
        "            for fail_info in failed_ingestion_info:\n",
        "                 csv_rows.append({\n",
        "                    \"filename\": fail_info.get(\"filename\", \"unknown\"),\n",
        "                    \"task\": \"ingestion_failed\",\n",
        "                    \"response\": fail_info.get(\"error\", \"Unknown ingestion error\"),\n",
        "                    \"context_chunks_used\": 0,\n",
        "                    \"input_tokens\": 0,\n",
        "                    \"output_tokens\": 0,\n",
        "                    \"total_tokens\": 0\n",
        "                })\n",
        "                 compiled_details += f\"\\n📄 DOCUMENT: {fail_info.get('filename', 'unknown')}\\n\"\n",
        "                 compiled_details += f\"  ❌ Ingestion Failed: {fail_info.get('error', 'Unknown ingestion error')}\\n\"\n",
        "\n",
        "\n",
        "            for filename, results in task_results.items():\n",
        "                 compiled_details += f\"\\n📄 DOCUMENT: {filename}\\n\" + \"─\" * 50 + \"\\n\"\n",
        "                 if isinstance(results, dict) and \"error\" in results:\n",
        "                     compiled_details += f\"❌ Document processing failed: {results['error']}\\n\"\n",
        "                     csv_rows.append({\n",
        "                        \"filename\": filename,\n",
        "                        \"task\": \"processing_failed\",\n",
        "                        \"response\": results[\"error\"],\n",
        "                        \"context_chunks_used\": 0,\n",
        "                        \"input_tokens\": 0,\n",
        "                        \"output_tokens\": 0,\n",
        "                        \"total_tokens\": 0\n",
        "                    })\n",
        "                 elif isinstance(results, dict):\n",
        "                    for task_name, task_result in results.items():\n",
        "                        if isinstance(task_result, dict):\n",
        "                            response = task_result.get(\"response\", \"No response\")\n",
        "                            chunks_used = task_result.get(\"context_chunks_used\", 0)\n",
        "                            token_usage = task_result.get(\"token_usage\", {})\n",
        "\n",
        "                            compiled_details += f\"\\n🎯 {task_name.upper()}:\\n\"\n",
        "                            compiled_details += f\"   📝 Response: {response[:200]}...\\n\" # Limit display length\n",
        "                            compiled_details += f\"   📚 Chunks used: {chunks_used}\\n\"\n",
        "                            if token_usage:\n",
        "                                compiled_details += f\"   🔢 Tokens: {token_usage.get('total_tokens', 0)}\\n\"\n",
        "\n",
        "                            csv_rows.append({\n",
        "                                \"filename\": filename,\n",
        "                                \"task\": task_name,\n",
        "                                \"response\": response,\n",
        "                                \"context_chunks_used\": chunks_used,\n",
        "                                \"input_tokens\": token_usage.get(\"input_tokens\", 0),\n",
        "                                \"output_tokens\": token_usage.get(\"output_tokens\", 0),\n",
        "                                \"total_tokens\": token_usage.get(\"total_tokens\", 0)\n",
        "                            })\n",
        "                        else:\n",
        "                             compiled_details += f\"\\n🎯 {task_name.upper()}:\\n\"\n",
        "                             compiled_details += f\"   ❌ Task Error: {task_result}\\n\"\n",
        "                             csv_rows.append({\n",
        "                                \"filename\": filename,\n",
        "                                \"task\": task_name,\n",
        "                                \"response\": f\"Task error: {task_result}\",\n",
        "                                \"context_chunks_used\": 0,\n",
        "                                \"input_tokens\": 0,\n",
        "                                \"output_tokens\": 0,\n",
        "                                \"total_tokens\": 0\n",
        "                            })\n",
        "                 else:\n",
        "                     compiled_details += f\"❌ Document processing failed with non-dict result: {results}\\n\"\n",
        "                     csv_rows.append({\n",
        "                        \"filename\": filename,\n",
        "                        \"task\": \"processing_failed_unknown\",\n",
        "                        \"response\": f\"Processing failed with unexpected result type: {results}\",\n",
        "                        \"context_chunks_used\": 0,\n",
        "                        \"input_tokens\": 0,\n",
        "                        \"output_tokens\": 0,\n",
        "                        \"total_tokens\": 0\n",
        "                    })\n",
        "\n",
        "\n",
        "            # Create DataFrame for the summary output\n",
        "            df = pd.DataFrame(csv_rows) # Create DataFrame even if empty\n",
        "\n",
        "            if not df.empty:\n",
        "                results_df = df\n",
        "                column_order = [\n",
        "                    \"filename\", \"task\", \"response\",\n",
        "                    \"context_chunks_used\", \"input_tokens\",\n",
        "                    \"output_tokens\", \"total_tokens\"\n",
        "                ]\n",
        "                existing_columns = [col for col in column_order if col in results_df.columns]\n",
        "                results_df = results_df[existing_columns]\n",
        "                for token_col in ['input_tokens', 'output_tokens', 'total_tokens']:\n",
        "                     if token_col in results_df.columns:\n",
        "                        results_df[token_col] = pd.to_numeric(results_df[token_col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "            else:\n",
        "                 results_df = pd.DataFrame({\"Status\": [\"No task results or errors generated.\"]})\n",
        "\n",
        "\n",
        "            # Display token usage summary in the details output as well\n",
        "            compiled_details += \"\\n\\n📊 TOTAL TOKEN USAGE SUMMARY FOR THIS RUN:\\n\"\n",
        "            compiled_details += f\"🔢 Total LLM Calls: {token_tracker.call_count}\\n\"\n",
        "            compiled_details += f\"📥 Total Input Tokens: {token_tracker.total_input_tokens:,}\\n\"\n",
        "            compiled_details += f\"📤 Total Output Tokens: {token_tracker.total_output_tokens:,}\\n\"\n",
        "            compiled_details += f\"🎯 Grand Total Tokens: {token_tracker.total_tokens:,}\\n\"\n",
        "            if token_tracker.call_count > 0:\n",
        "                compiled_details += f\"📊 Average per call: {token_tracker.total_tokens/token_tracker.call_count:.1f} tokens\\n\"\n",
        "\n",
        "\n",
        "            return status, compiled_details, results_df # Return DataFrame for display\n",
        "\n",
        "        else:\n",
        "            # If task_results is empty and no ingestion errors, something went wrong early\n",
        "            return status, \"Workflow completed but generated no results or errors.\", pd.DataFrame()\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"An unexpected workflow execution failed: {e}\"\n",
        "        print(f\"❌ {error_msg}\")\n",
        "        # Attempt to capture state before the crash if possible\n",
        "        debug_details = f\"An unexpected error occurred: {e}\"\n",
        "        if final_state: # Check if final_state object was created before the crash\n",
        "             debug_details += f\"\\nPartial workflow state available. Status: {final_state.get('workflow_status', 'unknown')}\"\n",
        "             if final_state.get('error'):\n",
        "                  debug_details += f\"\\nInternal state error: {final_state['error']}\"\n",
        "             if final_state.get('task_results'):\n",
        "                  debug_details += f\"\\nPartial task results available for {len(final_state['task_results'])} documents.\"\n",
        "             if final_state.get('processed_filenames'):\n",
        "                   debug_details += f\"\\nProcessed filenames state: {final_state['processed_filenames']}\"\n",
        "\n",
        "\n",
        "        # Attempt to create a simple error DataFrame\n",
        "        error_df = pd.DataFrame({\"Workflow Error\": [error_msg], \"Details\": [debug_details]})\n",
        "\n",
        "        return \"Workflow Failed\", debug_details, error_df\n",
        "\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Enhanced RAG-Based Expense Claims Processing\")\n",
        "    gr.Markdown(\"Upload your expense claim document (PDF, JPG, PNG, etc.) to extract details.\")\n",
        "\n",
        "    file_input = gr.File(label=\"Upload Expense Document\", file_types=[\".pdf\", \".jpg\", \".jpeg\", \".png\", \".tiff\"])\n",
        "    process_button = gr.Button(\"Process Document\")\n",
        "\n",
        "    status_output = gr.Textbox(label=\"Processing Status\", max_lines=5)\n",
        "    details_output = gr.Textbox(label=\"Extracted Details (Raw)\", interactive=False, lines=20)\n",
        "    summary_output = gr.Dataframe(label=\"Extracted Details (Structured)\") # Use Dataframe for structured output\n",
        "\n",
        "    process_button.click(\n",
        "        process_document_ui,\n",
        "        inputs=file_input,\n",
        "        outputs=[status_output, details_output, summary_output]\n",
        "    )\n",
        "\n",
        "# Launch the Gradio interface\n",
        "demo.launch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63ecafdc"
      },
      "source": [
        "## Display results\n",
        "\n",
        "### Subtask:\n",
        "Present the extracted expense details and processing status to the user on the web page.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aae149a7"
      },
      "source": [
        "**Reasoning**:\n",
        "Review the `process_document_ui` function and the Gradio interface definition to ensure the extracted data (status, raw text/logs, and DataFrame results) from the workflow's `final_state` is correctly passed to the respective Gradio output components (`status_output`, `details_output`, `summary_output`) and that the interface is launched. This directly addresses steps 1-5 of the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc72412e"
      },
      "source": [
        "# The code from the previous step already implements the logic to extract\n",
        "# processing status, compiled raw text/logs/errors, and the structured\n",
        "# results DataFrame from the workflow's final_state and return them.\n",
        "# It also already maps these return values to the status_output,\n",
        "# details_output, and summary_output components in the Gradio interface\n",
        "# definition. The demo.launch() call is also present.\n",
        "\n",
        "# We just need to ensure the Gradio interface definition correctly\n",
        "# uses these components and the process_button click event maps\n",
        "# the outputs correctly.\n",
        "\n",
        "# Review of the Gradio interface definition:\n",
        "# with gr.Blocks() as demo:\n",
        "#     gr.Markdown(...)\n",
        "#     gr.Markdown(...)\n",
        "#\n",
        "#     file_input = gr.File(...)\n",
        "#     process_button = gr.Button(...)\n",
        "#\n",
        "#     status_output = gr.Textbox(label=\"Processing Status\", max_lines=5)\n",
        "#     details_output = gr.Textbox(label=\"Extracted Details (Raw)\", interactive=False, lines=20)\n",
        "#     summary_output = gr.Dataframe(label=\"Extracted Details (Structured)\")\n",
        "#\n",
        "#     process_button.click(\n",
        "#         process_document_ui,\n",
        "#         inputs=file_input,\n",
        "#         outputs=[status_output, details_output, summary_output]\n",
        "#     )\n",
        "#\n",
        "# demo.launch()\n",
        "\n",
        "# The outputs list [status_output, details_output, summary_output]\n",
        "# correctly aligns with the return order of process_document_ui:\n",
        "# return status, compiled_details, results_df\n",
        "\n",
        "# status_output is a Textbox, suitable for the status string.\n",
        "# details_output is a Textbox with interactive=False and multiple lines,\n",
        "# suitable for displaying the compiled raw text, logs, and errors.\n",
        "# summary_output is a Dataframe, suitable for displaying the pandas DataFrame results.\n",
        "\n",
        "# The demo.launch() call will make the interface accessible.\n",
        "\n",
        "# No changes are needed to the code as the previous step already completed\n",
        "# the requirements of this subtask.\n",
        "\n",
        "print(\"Review complete: The Gradio interface is configured to display results.\")\n",
        "print(\"The process_document_ui function extracts the necessary data and maps it correctly.\")\n",
        "print(\"The Gradio interface includes components for status, raw details, and a structured DataFrame.\")\n",
        "print(\"The interface is launched using demo.launch().\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5685ebf"
      },
      "source": [
        "## Add error handling\n",
        "\n",
        "### Subtask:\n",
        "Implement error handling to provide feedback to the user if something goes wrong during upload or processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2591a4e"
      },
      "source": [
        "**Reasoning**:\n",
        "Review the existing code to identify potential error points and ensure they are handled within the workflow nodes and the Gradio UI function to provide user feedback.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04cda2b9"
      },
      "source": [
        "# Review the existing code, focusing on error handling.\n",
        "\n",
        "# In enhanced_ingestion_node:\n",
        "# - Catches exceptions during processor.ingest_document and stores in state[\"error\"].\n",
        "# - Adds {\"error\": ..., \"filename\": ...} to processed_filenames list for failed ingests.\n",
        "# - Sets workflow_status to \"ingestion_failed\" if no files are successfully processed.\n",
        "# - Returns the updated state.\n",
        "\n",
        "# In enhanced_task_processing_node:\n",
        "# - Filters out ingestion errors from processed_filenames.\n",
        "# - Catches exceptions during processor.process_all_tasks_for_document and stores in state[\"error\"].\n",
        "# - Adds {\"error\": ...} for a document that failed task processing after successful ingestion.\n",
        "# - Sets workflow_status to \"processing_skipped\" if no processable files exist.\n",
        "# - Returns the updated state.\n",
        "\n",
        "# In enhanced_results_compilation_node:\n",
        "# - Catches exceptions during JSON and CSV saving attempts and stores in state[\"error\"].\n",
        "# - Includes ingestion and task processing errors in the generated CSV and summary CSV data.\n",
        "# - Returns the updated state.\n",
        "\n",
        "# In process_document_ui:\n",
        "# - Checks for file is None.\n",
        "# - Catches exceptions during EnhancedRAGExpenseProcessor initialization.\n",
        "# - Catches exceptions during enhanced_rag_workflow.invoke().\n",
        "# - Checks final_state.get(\"error\") after workflow invocation.\n",
        "# - If final_state.get(\"error\") is present, it constructs a detailed error message including partial results.\n",
        "# - It attempts to create a DataFrame from partial results or indicates an error in the DataFrame.\n",
        "# - It returns appropriate status, details, and DataFrame outputs in case of errors at various stages.\n",
        "\n",
        "# The existing code already incorporates significant error handling as requested in the subtask instructions.\n",
        "# The workflow nodes capture and propagate errors via the state[\"error\"] field and by marking individual document results with errors.\n",
        "# The process_document_ui function checks for the state[\"error\"] after the workflow run and provides user-friendly feedback in the Gradio outputs.\n",
        "# Try-except blocks are present around critical initialization and workflow invocation calls.\n",
        "\n",
        "print(\"Review complete: Error handling mechanisms are present in the workflow nodes and the UI function.\")\n",
        "print(\"The code attempts to capture and display errors during ingestion, task processing, compilation, initialization, and workflow execution.\")\n",
        "print(\"Partial results and error messages are included in the Gradio outputs when errors occur.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4b7422d"
      },
      "source": [
        "## Containerize the application (optional but recommended)\n",
        "\n",
        "### Subtask:\n",
        "Containerize the application using Docker for easier deployment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7647570"
      },
      "source": [
        "**Reasoning**:\n",
        "Create the Dockerfile to containerize the application.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad3ed9b1"
      },
      "source": [
        "# Create Dockerfile\n",
        "dockerfile_content = \"\"\"\n",
        "# Use an official Python runtime as a parent image\n",
        "FROM python:3.10-slim\n",
        "\n",
        "# Set the working directory in the container\n",
        "WORKDIR /app\n",
        "\n",
        "# Install system dependencies needed for UnstructuredIO (like libmagic-dev, poppler-utils, tesseract-ocr)\n",
        "# These might vary based on the exact needs of UnstructuredIO and its dependencies.\n",
        "# Based on UnstructuredIO documentation and common requirements for PDF/image processing:\n",
        "RUN apt-get update && apt-get install -y \\\\\n",
        "    libpq-dev \\\\\n",
        "    build-essential \\\\\n",
        "    poppler-utils \\\\\n",
        "    tesseract-ocr \\\\\n",
        "    libtesseract-dev \\\\\n",
        "    libleptonica-dev \\\\\n",
        "    libmagic-dev \\\\\n",
        "    # Additional dependencies for specific file types if needed, e.g., libxml2-dev for HTML\n",
        "    # libxml2-dev \\\\\n",
        "    # libxslt1-dev \\\\\n",
        "    # libjpeg-dev \\\\\n",
        "    # libpng-dev \\\\\n",
        "    # zlib1g-dev \\\\\n",
        "    # swig \\\\\n",
        "    # libcurl4-openssl-dev \\\\\n",
        "    # libssl-dev \\\\\n",
        "    pkg-config \\\\\n",
        "    curl \\\\\n",
        "    gnupg \\\\\n",
        "    # Ensure ollama is available (download script)\n",
        "    # This curl command is from ollama.com/install.sh\n",
        "    # It downloads the install script, which we then execute.\n",
        "    # Note: Running install scripts in Dockerfile can be less transparent.\n",
        "    # An alternative is to download the binary directly if possible.\n",
        "    # For simplicity and following the original install method, we use the script.\n",
        "    # We need to ensure the script installs ollama in a location accessible by the app.\n",
        "    # The default install location is /usr/local, which is fine.\n",
        "    && curl -fsSL https://ollama.com/install.sh | sh \\\\\n",
        "    && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "# Copy the current directory contents into the container at /app\n",
        "COPY . /app\n",
        "\n",
        "# Install any needed dependencies specified in requirements.txt\n",
        "# Create a requirements.txt file based on the installed packages\n",
        "# In a real scenario, you would generate this from your environment (e.g., pip freeze > requirements.txt)\n",
        "# For this example, we'll list the main packages explicitly.\n",
        "RUN echo \"ollama\" >> requirements.txt && \\\\\n",
        "    echo \"langchain-ollama\" >> requirements.txt && \\\\\n",
        "    echo \"langchain-core\" >> requirements.txt && \\\\\n",
        "    echo \"langchain-community\" >> requirements.txt && \\\\\n",
        "    echo \"chromadb>=0.4.0\" >> requirements.txt && \\\\\n",
        "    echo \"unstructured[pdf]>=0.10.0\" >> requirements.txt && \\\\\n",
        "    echo \"sentence-transformers\" >> requirements.txt && \\\\\n",
        "    echo \"pandas\" >> requirements.txt && \\\\\n",
        "    echo \"pillow\" >> requirements.txt && \\\\\n",
        "    echo \"python-dateutil\" >> requirements.txt && \\\\\n",
        "    echo \"pydantic\" >> requirements.txt && \\\\\n",
        "    echo \"langgraph\" >> requirements.txt && \\\\\n",
        "    echo \"gradio\" >> requirements.txt && \\\\\n",
        "    echo \"aiofiles\" >> requirements.txt && \\\\\n",
        "    echo \"fsspec\" >> requirements.txt && \\\\\n",
        "    echo \"protobuf>=3.20.2,<5.0.0dev\" >> requirements.txt && \\\\\n",
        "    echo \"google-auth>=2.38.0\" >> requirements.txt && \\\\\n",
        "    echo \"notebook\" >> requirements.txt && \\\\\n",
        "    echo \"requests\" >> requirements.txt && \\\\\n",
        "    echo \"tornado\" >> requirements.txt && \\\\\n",
        "    echo \"google-api-core>=1.14.0,<2.0.0dev\" >> requirements.txt && \\\\\n",
        "    echo \"google-cloud-automl\" >> requirements.txt && \\\\\n",
        "    echo \"google-cloud-bigquery\" >> requirements.txt && \\\\\n",
        "    echo \"google-cloud-bigquery-storage\" >> requirements.txt && \\\\\n",
        "    echo \"google-cloud-translate\" >> requirements.txt && \\\\\n",
        "    echo \"google-cloud-aiplatform\" >> requirements.txt && \\\\\n",
        "    echo \"google-cloud-language\" >> requirements.txt && \\\\\n",
        "    echo \"google-cloud-storage\" >> requirements.txt && \\\\\n",
        "    echo \"google-cloud-vision\" >> requirements.txt && \\\\\n",
        "    echo \"grpcio\" >> requirements.txt && \\\\\n",
        "    echo \"grpcio-status\" >> requirements.txt && \\\\\n",
        "    echo \"numpy\" >> requirements.txt && \\\\\n",
        "    echo \"scipy\" >> requirements.txt && \\\\\n",
        "    echo \"tqdm\" >> requirements.txt && \\\\\n",
        "    echo \"layoutparser[ocr,layout]>=0.3.4\" >> requirements.txt && \\\\\n",
        "    echo \"fastjsonschema\" >> requirements.txt && \\\\\n",
        "    echo \"python-multipart\" >> requirements.txt && \\\\\n",
        "    echo \"onnxruntime\" >> requirements.txt && \\\\\n",
        "    echo \"onnx\" >> requirements.txt && \\\\\n",
        "    echo \"fastapi\" >> requirements.txt && \\\\\n",
        "    echo \"uvicorn\" >> requirements.txt && \\\\\n",
        "    echo \"pydantic-settings\" >> requirements.txt && \\\\\n",
        "    echo \"typing-extensions\" >> requirements.txt && \\\\\n",
        "    echo \"regex\" >> requirements.txt && \\\\\n",
        "    echo \"html2text\" >> requirements.txt && \\\\\n",
        "    echo \"bs4\" >> requirements.txt && \\\\\n",
        "    echo \"html-text\" >> requirements.txt && \\\\\n",
        "    echo \"ftfy\" >> requirements.txt && \\\\\n",
        "    echo \"spacy\" >> requirements.txt && \\\\\n",
        "    echo \"pandas-stubs\" >> requirements.txt && \\\\\n",
        "    echo \"lxml\" >> requirements.txt && \\\\\n",
        "    echo \"tabulate\" >> requirements.txt && \\\\\n",
        "    echo \"requests-toolbelt\" >> requirements.txt && \\\\\n",
        "    echo \"more-itertools\" >> requirements.txt && \\\\\n",
        "    echo \"dataclasses-json\" >> requirements.txt && \\\\\n",
        "    echo \"jsonpath-ng\" >> requirements.txt && \\\\\n",
        "    echo \"tenacity\" >> requirements.txt && \\\\\n",
        "    echo \"typing-inspect\" >> requirements.txt && \\\\\n",
        "    echo \"jsonpatch\" >> requirements.txt && \\\\\n",
        "    echo \"jsonschema\" >> requirements.txt && \\\\\n",
        "    echo \"markdown-it-py\" >> requirements.txt && \\\\\n",
        "    echo \"mdurl\" >> requirements.txt && \\\\\n",
        "    echo \"Pygments\" >> requirements.txt && \\\\\n",
        "    echo \"rich\" >> requirements.txt && \\\\\n",
        "    echo \"comm\" >> requirements.txt && \\\\\n",
        "    echo \"jupyter_client\" >> requirements.txt && \\\\\n",
        "    echo \"jupyter_core\" >> requirements.txt && \\\\\n",
        "    echo \"nest_asyncio\" >> requirements.txt && \\\\\n",
        "    echo \"packaging\" >> requirements.txt && \\\\\n",
        "    echo \"platformdirs\" >> requirements.txt && \\\\\n",
        "    echo \"psutil\" >> requirements.txt && \\\\\n",
        "    echo \"pyzmq\" >> requirements.txt && \\\\\n",
        "    echo \"send2trash\" >> requirements.txt && \\\\\n",
        "    echo \"terminado\" >> requirements.txt && \\\\\n",
        "    echo \"textdistance\" >> requirements.txt && \\\\\n",
        "    echo \"traitlets\" >> requirements.txt && \\\\\n",
        "    echo \"websockets\" >> requirements.txt && \\\\\n",
        "    echo \"anyio\" >> requirements.txt && \\\\\n",
        "    echo \"babel\" >> requirements.txt && \\\\\n",
        "    echo \"comm\" >> requirements.txt && \\\\\n",
        "    echo \"debugpy\" >> requirements.txt && \\\\\n",
        "    echo \"jedi\" >> requirements.txt && \\\\\n",
        "    echo \"jupyter_events\" >> requirements.txt && \\\\\n",
        "    echo \"jupyter_server\" >> requirements.txt && \\\\\n",
        "    echo \"jupyterlab_server\" >> requirements.txt && \\\\\n",
        "    echo \"matplotlib-inline\" >> requirements.txt && \\\\\n",
        "    echo \"mistune\" >> requirements.txt && \\\\\n",
        "    echo \"nbformat\" >> requirements.txt && \\\\\n",
        "    echo \"packaging\" >> requirements.txt && \\\\\n",
        "    echo \"prometheus_client\" >> requirements.txt && \\\\\n",
        "    echo \"prompt_toolkit\" >> requirements.txt && \\\\\n",
        "    echo \"pygments\" >> requirements.txt && \\\\\n",
        "    echo \"rpds-py\" >> requirements.txt && \\\\\n",
        "    echo \"Send2Trash\" >> requirements.txt && \\\\\n",
        "    echo \"stack_data\" >> requirements.txt && \\\\\n",
        "    echo \"tinycss2\" >> requirements.txt && \\\\\n",
        "    echo \"uri-template\" >> requirements.txt && \\\\\n",
        "    echo \"wcwidth\" >> requirements.txt && \\\\\n",
        "    echo \"webcolors\" >> requirements.txt && \\\\\n",
        "    echo \"yarl\" >> requirements.txt && \\\\\n",
        "    echo \"zipp\" >> requirements.txt \\\\\n",
        "    && pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Ensure spacy models are downloaded if used by unstructured or other deps\n",
        "RUN python -m spacy download en_core_web_sm || echo \"Spacy model download failed, proceeding...\"\n",
        "\n",
        "# Expose the port that Gradio will run on\n",
        "EXPOSE 7860\n",
        "\n",
        "# Set the OLLAMA_HOST environment variable\n",
        "ENV OLLAMA_HOST=127.0.0.1:11434\n",
        "\n",
        "# Command to run the application\n",
        "# Start Ollama server in the background and then run the Python app\n",
        "# This requires ollama to be installed and in the PATH within the container.\n",
        "# Use a script to handle startup order\n",
        "COPY start.sh /app/start.sh\n",
        "RUN chmod +x /app/start.sh\n",
        "\n",
        "CMD [\"/app/start.sh\"]\n",
        "\"\"\"\n",
        "\n",
        "with open(\"Dockerfile\", \"w\") as f:\n",
        "    f.write(dockerfile_content)\n",
        "\n",
        "# Create a start.sh script\n",
        "start_script_content = \"\"\"\n",
        "#!/bin/bash\n",
        "\n",
        "# Start Ollama server in the background\n",
        "echo \"Starting Ollama server...\"\n",
        "ollama serve &\n",
        "\n",
        "# Wait for Ollama to be ready (optional, but good practice)\n",
        "# You might need a more robust health check depending on your needs\n",
        "sleep 10\n",
        "\n",
        "# Download required Ollama models (if not already present)\n",
        "echo \"Downloading Ollama models...\"\n",
        "ollama pull nomic-embed-text || echo \"nomic-embed-text pull failed, attempting to continue...\"\n",
        "ollama pull gemma3:1b || echo \"gemma3:1b pull failed, attempting to continue...\"\n",
        "\n",
        "# Pre-download UnstructuredIO models by running a small python script\n",
        "# This is needed because the python -m spacy download doesn't handle all unstructured deps.\n",
        "echo \"Pre-downloading UnstructuredIO models...\"\n",
        "python -c \"from unstructured.partition.auto import partition; from unstructured.partition.pdf import partition_pdf; from unstructured.partition.utils.constants import Source; print('UnstructuredIO imports successful, models might be downloaded.'); import os; from PIL import Image; import io; img = Image.new('RGB', (100, 100), color='white'); img_bytes = io.BytesIO(); img.save(img_bytes, format='PNG'); img_bytes.seek(0); temp_path = '/tmp/dummy_image.png'; with open(temp_path, 'wb') as f: f.write(img_bytes.getvalue()); try: elements = partition(filename=temp_path); print('UnstructuredIO dummy partition successful, models should be downloaded.'); except Exception as e: print(f'UnstructuredIO dummy partition failed: {e}'); if os.path.exists(temp_path): os.remove(temp_path);\" || echo \"UnstructuredIO pre-download script failed, attempting to continue...\"\n",
        "\n",
        "\n",
        "# Run the Gradio application\n",
        "echo \"Starting Gradio application...\"\n",
        "python your_app_file_name.py # Replace 'your_app_file_name.py' with the actual name of your Python script\n",
        "\"\"\"\n",
        "\n",
        "with open(\"start.sh\", \"w\") as f:\n",
        "    f.write(start_script_content)\n",
        "\n",
        "print(\"Dockerfile and start.sh created.\")\n",
        "print(\"Remember to replace 'your_app_file_name.py' in start.sh with the actual name of your main Python script.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b0401b9"
      },
      "source": [
        "## Deploy the application\n",
        "\n",
        "### Subtask:\n",
        "Deploy the web application to a cloud platform or server.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "525d7e52"
      },
      "source": [
        "**Reasoning**:\n",
        "This subtask involves deploying the containerized application. The next logical step is to build the Docker image locally using the created Dockerfile and then tag it appropriately for pushing to a container registry. This is a preparatory step before pushing and deploying.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a88f8cf2",
        "outputId": "0c5188bb-a973-4345-fc3a-078509c3f26b"
      },
      "source": [
        "# Assume the Dockerfile and start.sh created in the previous step exist in the current directory.\n",
        "\n",
        "# Define the image name and tag\n",
        "image_name = \"expense-rag-app\"\n",
        "image_tag = \"latest\"\n",
        "full_image_name = f\"{image_name}:{image_tag}\"\n",
        "\n",
        "print(f\"🛠️ Building Docker image: {full_image_name}\")\n",
        "\n",
        "# Build the Docker image\n",
        "# This command executes the Dockerfile in the current directory (.)\n",
        "try:\n",
        "    subprocess.run([\"docker\", \"build\", \"-t\", full_image_name, \".\"], check=True)\n",
        "    print(f\"✅ Docker image built successfully: {full_image_name}\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"❌ Error building Docker image: {e}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"❌ Docker command not found. Please ensure Docker is installed and in your PATH.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ An unexpected error occurred during Docker build: {e}\")\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🛠️ Building Docker image: expense-rag-app:latest\n",
            "❌ Docker command not found. Please ensure Docker is installed and in your PATH.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e128231"
      },
      "source": [
        "## Deploy the application\n",
        "\n",
        "### Subtask:\n",
        "Deploy the web application to a cloud platform or server.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4939c87b"
      },
      "source": [
        "## Deploy the application\n",
        "\n",
        "### Subtask:\n",
        "Deploy the web application to a cloud platform or server.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62ebfcfb"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Gradio was chosen as the web framework for the front end due to its suitability for file uploads and ease of use for rapid prototyping.\n",
        "*   The Gradio interface was successfully designed and implemented with components for file upload, processing, and displaying results (status, raw details, and a structured DataFrame).\n",
        "*   The Gradio interface correctly receives uploaded files and provides temporary file paths, which are then used as input for the RAG workflow.\n",
        "*   The existing RAG workflow was modified to accept a list of file paths from the front end.\n",
        "*   Error handling was integrated into the workflow nodes and the Gradio UI function to capture and display errors during ingestion, task processing, compilation, and workflow execution.\n",
        "*   Comprehensive logging and status updates were added to track the workflow progress and results.\n",
        "*   Detailed and summary results, including token usage, are compiled and intended to be displayed in the Gradio interface and saved as JSON and CSV files.\n",
        "*   Attempts were made to containerize the application using Docker by creating a `Dockerfile` and `start.sh` script, but the process failed because the Docker environment was not available.\n",
        "*   Deployment of the application to a cloud platform could not be completed due to the lack of a functional Docker environment.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Ensure a functional Docker environment is available to successfully containerize and deploy the application.\n",
        "*   Verify that all necessary dependent Python classes and functions (like `EnhancedRAGExpenseProcessor`, `TokenUsageTracker`, etc.) are correctly defined and accessible in the execution environment, especially when running outside the notebook or integrated development environment where they were initially developed.\n"
      ]
    }
  ]
}